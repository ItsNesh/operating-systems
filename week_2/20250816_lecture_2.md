<!-- 
Generated by LectureQ (Enhanced Comprehensive Version)
Date: 2025-08-16 02:18:38
Model: qwen3:30b
Output Mode: Comprehensive
-->



# Week 2: Lecture 2 - Operating Systems Scheduling

## Overview
This lecture explores process scheduling in operating systems, covering fundamental algorithms, metrics for evaluation, and advanced techniques to balance system performance with user experience. We'll examine how operating systems manage CPU time among competing processes while optimizing key performance indicators like response time and throughput.

---

## Core Concepts: Process States & Scheduling Fundamentals

### Scheduler vs Dispatcher
Operating systems use two distinct but related components:

- **Scheduler**: The *conceptual* component that determines *when* to run a process (the "what" of scheduling)
  
  > *"The scheduler is the logic for job choice. It decides which process should be dispatched next."*

- **Dispatcher**: The *mechanical* component that handles the actual switching between processes (the "how")
  
  > *"The dispatcher determines how to switch between processes, handling user mode/kernel mode transitions and saving Process Control Blocks (PCBs)."*

### Process States
Processes move through three primary states during execution:

| State      | Description                                                                 |
|------------|-----------------------------------------------------------------------------|
| **Running** | The process is currently executing on the CPU                                |
| **Ready**   | The process has all resources needed but isn't currently using the CPU       |
| **Blocked** | The process is waiting for an event (typically I/O completion)               |

> *Instructor Insight*: "Imagine a hospital operating room. When you're in surgery, you're 'running.' When you've finished your procedure and are waiting to be moved out of the OR, you're 'ready.' If you need additional tests before proceeding with another operation, you're 'blocked.'"

### Key Definitions
- **Workload**: The collection of jobs (tasks) that need processing
- **Job**: A single CPU burst of a process (the unit we schedule)
- **Burst Time**: How long a job needs to run on the CPU
- **Arrival Time**: When a job arrives in the system

> *Instructor Emphasis*: "If you don't know what 'metric' means at this point in your university careers, I feel very bad for you. A metric is something we use to determine if scheduling is good or bad."

---

## Key Scheduling Metrics: What We're Trying To Optimize

### Hospital Analogy (Simplified)
Imagine managing an operating room with multiple patients needing surgery:

| Metric                | Definition                                                                 | Real-World Example                          |
|-----------------------|----------------------------------------------------------------------------|---------------------------------------------|
| **Turnaround Time**   | Total time from arrival to completion                                      | How long a patient waits for their surgery  |
| **Response Time**     | Time between arrival and when the process starts running                     | How quickly you get called into the OR      |
| **Wait Time**         | Time spent waiting in queue (not including I/O)                            | Waiting time before being prepared for surgery |
| **Throughput**        | Number of jobs completed per unit time                                     | Patients who successfully complete surgery  |
| **Resource Utilization** | Percentage of CPU time used by processes                                   | How often the OR is actively performing surgery |
| **Overhead (Switches)** | Cost of context switching between processes                                | Time wasted changing surgical teams         |
| **Fairness**          | Equal distribution of resources among all jobs                             | Balanced assignment of surgeries to patients |

> *Instructor Insight*: "The goal isn't just to get things done quickly, but to do it in a way that makes users happy. If you're waiting 3 seconds for your computer to respond after clicking something, you'll be frustrated—unlike waiting 2 minutes before a long computation finishes."

---

## Scheduling Algorithms: Basic Approaches

### Gantt Charts & Tables
We visualize scheduling using **Gantt charts** (time-based diagrams) and **tables** showing arrival times, burst times, completion times, and metrics.

#### Example Table Structure:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| A   | 0            | 5          | 5               | 5              |

> *Instructor Insight*: "Gantt charts are your life once you have a job. They're the visual representation of how we schedule tasks over time."

---

### First Come, First Serve (FCFS)
**Concept**: Processes run in the order they arrive.

#### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| A   | 0            | 5          | 5               | 5              |
| B   | 0            | 5          | 10              | 10             |
| C   | 0            | 5          | 15              | 15             |

**Average Turnaround Time**: (5 + 10 + 15) / 3 = **10**

#### Problem: The Convoy Effect
When a long job arrives first, it "blocks" shorter jobs that arrive later.

> *Instructor Insight*: "This is like doing your large assignment before your tutorial. It's not how university students actually operate—nobody does this because it leads to the convoy effect."

**Worse Example:**
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| A   | 0            | 20         | 20              | 20             |
| B   | 0            | 4          | 24              | 24             |
| C   | 0            | 4          | 28              | 28             |

**Average Turnaround Time**: (20 + 24 + 28) / 3 = **24**

> *Instructor Insight*: "This is why FCFS isn't good for interactive systems—it's simple to implement but dependent on job order, and big tasks can starve small ones."

---

### Shortest Job First (SJF)
**Concept**: Run the shortest job first.

#### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| B   | 0            | 4          | 4               | 4              |
| C   | 0            | 4          | 8               | 8              |
| A   | 0            | 20         | 28              | 28             |

**Average Turnaround Time**: (4 + 8 + 28) / 3 = **13.3**

> *Instructor Insight*: "SJF reduces average turnaround time, but it only works well if all jobs arrive simultaneously. If they don't, we need a more sophisticated approach."

---

### Shortest Time to Completion First (STCF)
**Concept**: Preemptive version of SJF—interrupt the current process if a shorter job arrives.

#### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|-----------------|---------------|
| A   | 0            | 10         | 10              | 10              | 0             |
| B   | 2            | 10         | 18              | 16              | 8             |
| C   | 4            | 10         | 26              | 22              | 16            |

**Average Response Time**: (0 + 8 + 16) / 3 = **8**

> *Instructor Insight*: "STCF is preemptive, meaning it can interrupt a running process to run a shorter one. This gives better response times but requires more context switching."

---

### Round Robin (RR)
**Concept**: Each job gets a fixed time slice before being preempted.

#### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|-----------------|---------------|
| A   | 0            | 10         | 26              | 26              | 0             |
| B   | 2            | 10         | 26              | 24              | 0             |
| C   | 4            | 10         | 26              | 22              | 0             |

**Average Response Time**: (0 + 0 + 0) / 3 = **0**

> *Instructor Insight*: "Round Robin gives everyone a fair chance, but it can lead to long turnaround times if the time slice is too short. The key challenge is determining how often to switch between processes."

---

## Advanced Concepts: Beyond Basic Scheduling

### Preemptive vs Non-Preemptive
| Type                | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| **Non-preemptive**  | Once a process starts running, it continues until completion (FCFS, SJF)     |
| **Preemptive**      | A running process can be interrupted to run another process (STCF, RR)       |

> *Instructor Insight*: "If you're doing an assignment and get distracted by a tutorial notification, that's preemptive scheduling. If you finish your large assignment before touching the tutorial, that's non-preemptive."

---

### I/O Blocking: Why It Matters
Processes often alternate between CPU time and waiting for I/O (input/output).

#### Example:
- Process runs 2 seconds on CPU → waits 4 seconds for I/O → runs 2 more seconds

> *Instructor Insight*: "Don't waste the CPU! If a process is blocked waiting for I/O, we should run another process instead of idling."

**Visual Representation:**
```
Time:   0    1    2    3    4    5    6    7
CPU:     Task A  Task B  Task C
I/O:              Task A      Task B
```

> *Instructor Insight*: "Interactive jobs (like keyboard input) need quick response times, while batch jobs (like simulations) care more about total completion time."

---

### Priority Systems

#### Fixed Priority Scheduling
- High-priority processes run first
- Low-priority processes may never get CPU time ("starvation")

> *Instructor Insight*: "Imagine a hospital where all the urgent cases are treated immediately, but non-urgent patients wait forever. This is why fixed priority scheduling isn't ideal."

#### Dynamic Priority Scheduling (MLFQ)
The key innovation: **priority decreases over time** as processes use more CPU.

##### MLFQ Rules:
1. Run high-priority jobs first
2. Start all new jobs at the highest priority
3. If a job runs for too long, drop its priority
4. Periodically reset priorities (prevent starvation)

> *Instructor Insight*: "The problem with dynamic priority is that if you only lose priority, eventually your process will be ignored forever."

---

### The "Evil" Strategy: How to Exploit MLFQ
**Question**: How can a malicious program stay at the highest priority?

**Answer**: Run for just under the allotted time before being preempted.

> *Instructor Insight*: "If you know how long your time slice is (say 10ms), run for 9.9ms, then sleep briefly to reset your timer and get back to high priority."

**Visual Representation:**
```
Time:   0    1    ...    9    10    11
Priority: High → Medium → High (after reset)
```

> *Instructor Insight*: "This is why MLFQ systems need a way to 'refresh' priorities periodically—otherwise, long-running jobs get stuck in lower priority queues."

---

### Multi-Level Feedback Queue (MLFQ) Implementation

#### Design Decisions:
- How many priority queues should exist?
- What's the time allotment for each queue?
- Are allotments equal across all queues?
- How often to refresh priorities?

> *Instructor Insight*: "These aren't mathematical questions with perfect answers. You experiment, test on real systems, and adjust based on observed performance."

**Visual Representation:**
```
Queue 0 (High Priority) → Runs first
Queue 1 (Medium Priority) → Runs if Queue 0 empty
Queue 2 (Low Priority)    → Runs if Queues 0 & 1 empty
```

---

### Lottery Scheduling

**Concept**: Assign processes tickets proportional to priority. Randomly select a ticket for execution.

> *Instructor Insight*: "It sounds crazy, but lottery scheduling can be quite fair and effective. You could even have 'ticket trading' where processes exchange tickets."

#### Key Questions:
- How many tickets should high-priority jobs get?
- Can you trade tickets between processes?
- What happens if ticket values change over time?

> *Instructor Insight*: "This is a simple idea that works surprisingly well, though it's not used as commonly in modern systems."

---

### Completely Fair Scheduler (CFS)

**Core Idea**: Each process gets an equal share of CPU time based on its weight.

#### Key Concepts:
- **vruntime**: Virtual runtime—how long the process has been running
- **Target Latency**: How often to switch processes (typically 6ms)
- **Weighting**: Adjusts how much CPU a process receives

> *Instructor Insight*: "CFS is used in Linux since 2007. It's called 'completely fair' because every process gets a share of the CPU based on its weight."

#### How CFS Works:
1. Tracks vruntime for each process
2. Runs processes with smallest vruntime first (highest priority)
3. Each process runs for about target latency period
4. Uses Red-Black tree to efficiently find next process

> *Instructor Insight*: "The magic of CFS is that it guarantees every process gets a minimum amount of CPU time, preventing starvation."

#### Example:
| Process        | Type                | vruntime (Low = High Priority) |
|----------------|---------------------|-------------------------------|
| Video Rendering| Long-running batch  | High                          |
| Word Processor | Interactive         | Low                           |

> *Instructor Insight*: "The word processor accumulates little vruntime. When it wakes up after user input, it gets run immediately."

---

### The `nice` Command in Linux

**Concept**: Adjusts a process's priority (weight) for CFS.

| Nice Value | Priority Level | CPU Share |
|------------|----------------|-----------|
| -20        | Highest        | Most      |
| 0          | Default        | Medium    |
| +19        | Lowest         | Least     |

> *Instructor Insight*: "Changing a program's 'nice' value changes its weighting for CFS. Lower nice = more CPU time."

---

### Multi-core Considerations

**Challenge**: How to fairly distribute processes across multiple cores.

> *Instructor Insight*: "Imagine waiting in line at an airport with 6 queues, but you pick the wrong one and have to wait while others get served quickly. That's the challenge of multi-core scheduling."

#### Current Approach:
- One Red-Black tree per core
- Processes can migrate between cores based on load

> *Instructor Insight*: "This is a complex problem with no perfect solution, but modern systems use sophisticated algorithms to balance workloads across cores."

---

## Summary & Key Takeaways

### Scheduling Algorithms Comparison
| Algorithm          | Preemptive? | Best For                          | Major Drawback               |
|--------------------|-------------|-----------------------------------|------------------------------|
| **FCFS**           | No          | Simple batch processing           | Convoy effect, poor response time |
| **SJF**            | No (usually)| Batch jobs with known run times   | Doesn't work well if jobs arrive at different times |
| **STCF**           | Yes         | Interactive systems               | High context switching overhead |
| **Round Robin**    | Yes         | General-purpose, fair distribution | Long turnaround for long jobs  |
| **MLFQ**           | Yes (dynamic)| Mixed interactive/batch workloads | Complex implementation        |
| **CFS**            | Yes         | Linux systems                     | Requires sophisticated data structures |

### Key Principles to Remember
1. **Scheduling is about trade-offs**: You can't optimize for all metrics simultaneously.
2. **Interactive vs Batch Jobs**: Need different scheduling strategies (response time vs throughput).
3. **Starvation Prevention**: Always design algorithms so no process gets ignored indefinitely.
4. **Real-world Complexity**: Actual systems don't follow the simple assumptions we use in examples.

> *Instructor Insight*: "The most important thing about scheduling isn't knowing all the algorithms—it's understanding why they exist and when to use them."

---

## Study Questions

1. What is the difference between a scheduler and a dispatcher?
2. Calculate turnaround time for FCFS with jobs A(0,5), B(0,3), C(0,7).
3. Explain the convoy effect using an example from university life.
4. Why does SJF require all jobs to arrive simultaneously in its basic form?
5. How would you modify STCF if a job arrives while another is running?
6. What are the advantages and disadvantages of Round Robin scheduling?
7. Describe how I/O blocking affects process scheduling decisions.
8. Explain why fixed priority scheduling can lead to starvation.
9. How does MLFQ prevent long-running jobs from being starved?
10. What is vruntime in CFS, and how does it affect scheduling decisions?
11. Why might a malicious program try to "sleep" just before its time slice ends?
12. Explain the difference between response time and turnaround time.
13. How do Red-Black trees improve performance in CFS compared to other data structures?
14. What is the purpose of the `nice` command in Linux scheduling?
15. Describe a situation where FCFS would be preferable to SJF.

---

## Practical Exercise

**Scenario**: You're designing a scheduler for a hospital's operating room system with these requirements:
- 3 surgeons (A, B, C) available at different times
- Each surgery has an estimated duration
- Some surgeries require special equipment that takes time to prepare

Create a Gantt chart showing how you would schedule the following operations:

| Surgery | Arrival Time | Duration |
|---------|--------------|----------|
| A       | 0            | 120 min  |
| B       | 30           | 60 min   |
| C       | 45           | 90 min   |

Calculate:
- Turnaround time for each surgery
- Average turnaround time
- Response time for each surgery

*Hint: Consider how I/O blocking (equipment preparation) would affect scheduling.*
<!-- 
Combined Study Guide
Generated by LectureQ
Date: 2025-08-16 03:08:12
Week/Module: Week 2
Files Combined: 6
Content Type: Mixed
-->

# Week 2 - Complete Study Guide

> **Auto-generated combined study guide**  
> Generated from 6 individual notes files  
> Date: August 16, 2025

## 📚 Table of Contents

1. [Week 2: Lecture 2 - Operating Systems Scheduling](#week-2-lecture-2---operating-systems-scheduling)
2. [Comprehensive Study Notes: CPU Scheduling](#comprehensive-study-notes-cpu-scheduling)
3. [Comprehensive Study Notes: Scheduling - Proportional Share](#comprehensive-study-notes-scheduling---proportional-share)
4. [Comprehensive Study Notes: Multi-Level Feedback Queue (MLFQ) Scheduling](#comprehensive-study-notes-multi-level-feedback-queue-mlfq-scheduling)
5. [Comprehensive Study Notes: The Abstraction – Address Spaces](#comprehensive-study-notes-the-abstraction--address-spaces)
6. [Comprehensive Study Notes: Virtual Memory Mechanism - Address Translation](#comprehensive-study-notes-virtual-memory-mechanism---address-translation)


---

# Week 2: Lecture 2 - Operating Systems Scheduling

### Overview
This lecture explores process scheduling in operating systems, covering fundamental algorithms, metrics for evaluation, and advanced techniques to balance system performance with user experience. We'll examine how operating systems manage CPU time among competing processes while optimizing key performance indicators like response time and throughput.

---

### Core Concepts: Process States & Scheduling Fundamentals

#### Scheduler vs Dispatcher
Operating systems use two distinct but related components:

- **Scheduler**: The *conceptual* component that determines *when* to run a process (the "what" of scheduling)
  
  > *"The scheduler is the logic for job choice. It decides which process should be dispatched next."*

- **Dispatcher**: The *mechanical* component that handles the actual switching between processes (the "how")
  
  > *"The dispatcher determines how to switch between processes, handling user mode/kernel mode transitions and saving Process Control Blocks (PCBs)."*

#### Process States
Processes move through three primary states during execution:

| State      | Description                                                                 |
|------------|-----------------------------------------------------------------------------|
| **Running** | The process is currently executing on the CPU                                |
| **Ready**   | The process has all resources needed but isn't currently using the CPU       |
| **Blocked** | The process is waiting for an event (typically I/O completion)               |

> *Instructor Insight*: "Imagine a hospital operating room. When you're in surgery, you're 'running.' When you've finished your procedure and are waiting to be moved out of the OR, you're 'ready.' If you need additional tests before proceeding with another operation, you're 'blocked.'"

#### Key Definitions
- **Workload**: The collection of jobs (tasks) that need processing
- **Job**: A single CPU burst of a process (the unit we schedule)
- **Burst Time**: How long a job needs to run on the CPU
- **Arrival Time**: When a job arrives in the system

> *Instructor Emphasis*: "If you don't know what 'metric' means at this point in your university careers, I feel very bad for you. A metric is something we use to determine if scheduling is good or bad."

---

### Key Scheduling Metrics: What We're Trying To Optimize

#### Hospital Analogy (Simplified)
Imagine managing an operating room with multiple patients needing surgery:

| Metric                | Definition                                                                 | Real-World Example                          |
|-----------------------|----------------------------------------------------------------------------|---------------------------------------------|
| **Turnaround Time**   | Total time from arrival to completion                                      | How long a patient waits for their surgery  |
| **Response Time**     | Time between arrival and when the process starts running                     | How quickly you get called into the OR      |
| **Wait Time**         | Time spent waiting in queue (not including I/O)                            | Waiting time before being prepared for surgery |
| **Throughput**        | Number of jobs completed per unit time                                     | Patients who successfully complete surgery  |
| **Resource Utilization** | Percentage of CPU time used by processes                                   | How often the OR is actively performing surgery |
| **Overhead (Switches)** | Cost of context switching between processes                                | Time wasted changing surgical teams         |
| **Fairness**          | Equal distribution of resources among all jobs                             | Balanced assignment of surgeries to patients |

> *Instructor Insight*: "The goal isn't just to get things done quickly, but to do it in a way that makes users happy. If you're waiting 3 seconds for your computer to respond after clicking something, you'll be frustrated—unlike waiting 2 minutes before a long computation finishes."

---

### Scheduling Algorithms: Basic Approaches

#### Gantt Charts & Tables
We visualize scheduling using **Gantt charts** (time-based diagrams) and **tables** showing arrival times, burst times, completion times, and metrics.

##### Example Table Structure:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| A   | 0            | 5          | 5               | 5              |

> *Instructor Insight*: "Gantt charts are your life once you have a job. They're the visual representation of how we schedule tasks over time."

---

#### First Come, First Serve (FCFS)
**Concept**: Processes run in the order they arrive.

##### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| A   | 0            | 5          | 5               | 5              |
| B   | 0            | 5          | 10              | 10             |
| C   | 0            | 5          | 15              | 15             |

**Average Turnaround Time**: (5 + 10 + 15) / 3 = **10**

##### Problem: The Convoy Effect
When a long job arrives first, it "blocks" shorter jobs that arrive later.

> *Instructor Insight*: "This is like doing your large assignment before your tutorial. It's not how university students actually operate—nobody does this because it leads to the convoy effect."

**Worse Example:**
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| A   | 0            | 20         | 20              | 20             |
| B   | 0            | 4          | 24              | 24             |
| C   | 0            | 4          | 28              | 28             |

**Average Turnaround Time**: (20 + 24 + 28) / 3 = **24**

> *Instructor Insight*: "This is why FCFS isn't good for interactive systems—it's simple to implement but dependent on job order, and big tasks can starve small ones."

---

#### Shortest Job First (SJF)
**Concept**: Run the shortest job first.

##### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| B   | 0            | 4          | 4               | 4              |
| C   | 0            | 4          | 8               | 8              |
| A   | 0            | 20         | 28              | 28             |

**Average Turnaround Time**: (4 + 8 + 28) / 3 = **13.3**

> *Instructor Insight*: "SJF reduces average turnaround time, but it only works well if all jobs arrive simultaneously. If they don't, we need a more sophisticated approach."

---

#### Shortest Time to Completion First (STCF)
**Concept**: Preemptive version of SJF—interrupt the current process if a shorter job arrives.

##### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|-----------------|---------------|
| A   | 0            | 10         | 10              | 10              | 0             |
| B   | 2            | 10         | 18              | 16              | 8             |
| C   | 4            | 10         | 26              | 22              | 16            |

**Average Response Time**: (0 + 8 + 16) / 3 = **8**

> *Instructor Insight*: "STCF is preemptive, meaning it can interrupt a running process to run a shorter one. This gives better response times but requires more context switching."

---

#### Round Robin (RR)
**Concept**: Each job gets a fixed time slice before being preempted.

##### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|-----------------|---------------|
| A   | 0            | 10         | 26              | 26              | 0             |
| B   | 2            | 10         | 26              | 24              | 0             |
| C   | 4            | 10         | 26              | 22              | 0             |

**Average Response Time**: (0 + 0 + 0) / 3 = **0**

> *Instructor Insight*: "Round Robin gives everyone a fair chance, but it can lead to long turnaround times if the time slice is too short. The key challenge is determining how often to switch between processes."

---

### Advanced Concepts: Beyond Basic Scheduling

#### Preemptive vs Non-Preemptive
| Type                | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| **Non-preemptive**  | Once a process starts running, it continues until completion (FCFS, SJF)     |
| **Preemptive**      | A running process can be interrupted to run another process (STCF, RR)       |

> *Instructor Insight*: "If you're doing an assignment and get distracted by a tutorial notification, that's preemptive scheduling. If you finish your large assignment before touching the tutorial, that's non-preemptive."

---

#### I/O Blocking: Why It Matters
Processes often alternate between CPU time and waiting for I/O (input/output).

##### Example:
- Process runs 2 seconds on CPU → waits 4 seconds for I/O → runs 2 more seconds

> *Instructor Insight*: "Don't waste the CPU! If a process is blocked waiting for I/O, we should run another process instead of idling."

**Visual Representation:**
```
Time:   0    1    2    3    4    5    6    7
CPU:     Task A  Task B  Task C
I/O:              Task A      Task B
```

> *Instructor Insight*: "Interactive jobs (like keyboard input) need quick response times, while batch jobs (like simulations) care more about total completion time."

---

#### Priority Systems

##### Fixed Priority Scheduling
- High-priority processes run first
- Low-priority processes may never get CPU time ("starvation")

> *Instructor Insight*: "Imagine a hospital where all the urgent cases are treated immediately, but non-urgent patients wait forever. This is why fixed priority scheduling isn't ideal."

##### Dynamic Priority Scheduling (MLFQ)
The key innovation: **priority decreases over time** as processes use more CPU.

###### MLFQ Rules:
1. Run high-priority jobs first
2. Start all new jobs at the highest priority
3. If a job runs for too long, drop its priority
4. Periodically reset priorities (prevent starvation)

> *Instructor Insight*: "The problem with dynamic priority is that if you only lose priority, eventually your process will be ignored forever."

---

#### The "Evil" Strategy: How to Exploit MLFQ
**Question**: How can a malicious program stay at the highest priority?

**Answer**: Run for just under the allotted time before being preempted.

> *Instructor Insight*: "If you know how long your time slice is (say 10ms), run for 9.9ms, then sleep briefly to reset your timer and get back to high priority."

**Visual Representation:**
```
Time:   0    1    ...    9    10    11
Priority: High → Medium → High (after reset)
```

> *Instructor Insight*: "This is why MLFQ systems need a way to 'refresh' priorities periodically—otherwise, long-running jobs get stuck in lower priority queues."

---

#### Multi-Level Feedback Queue (MLFQ) Implementation

##### Design Decisions:
- How many priority queues should exist?
- What's the time allotment for each queue?
- Are allotments equal across all queues?
- How often to refresh priorities?

> *Instructor Insight*: "These aren't mathematical questions with perfect answers. You experiment, test on real systems, and adjust based on observed performance."

**Visual Representation:**
```
Queue 0 (High Priority) → Runs first
Queue 1 (Medium Priority) → Runs if Queue 0 empty
Queue 2 (Low Priority)    → Runs if Queues 0 & 1 empty
```

---

#### Lottery Scheduling

**Concept**: Assign processes tickets proportional to priority. Randomly select a ticket for execution.

> *Instructor Insight*: "It sounds crazy, but lottery scheduling can be quite fair and effective. You could even have 'ticket trading' where processes exchange tickets."

##### Key Questions:
- How many tickets should high-priority jobs get?
- Can you trade tickets between processes?
- What happens if ticket values change over time?

> *Instructor Insight*: "This is a simple idea that works surprisingly well, though it's not used as commonly in modern systems."

---

#### Completely Fair Scheduler (CFS)

**Core Idea**: Each process gets an equal share of CPU time based on its weight.

##### Key Concepts:
- **vruntime**: Virtual runtime—how long the process has been running
- **Target Latency**: How often to switch processes (typically 6ms)
- **Weighting**: Adjusts how much CPU a process receives

> *Instructor Insight*: "CFS is used in Linux since 2007. It's called 'completely fair' because every process gets a share of the CPU based on its weight."

##### How CFS Works:
1. Tracks vruntime for each process
2. Runs processes with smallest vruntime first (highest priority)
3. Each process runs for about target latency period
4. Uses Red-Black tree to efficiently find next process

> *Instructor Insight*: "The magic of CFS is that it guarantees every process gets a minimum amount of CPU time, preventing starvation."

##### Example:
| Process        | Type                | vruntime (Low = High Priority) |
|----------------|---------------------|-------------------------------|
| Video Rendering| Long-running batch  | High                          |
| Word Processor | Interactive         | Low                           |

> *Instructor Insight*: "The word processor accumulates little vruntime. When it wakes up after user input, it gets run immediately."

---

#### The `nice` Command in Linux

**Concept**: Adjusts a process's priority (weight) for CFS.

| Nice Value | Priority Level | CPU Share |
|------------|----------------|-----------|
| -20        | Highest        | Most      |
| 0          | Default        | Medium    |
| +19        | Lowest         | Least     |

> *Instructor Insight*: "Changing a program's 'nice' value changes its weighting for CFS. Lower nice = more CPU time."

---

#### Multi-core Considerations

**Challenge**: How to fairly distribute processes across multiple cores.

> *Instructor Insight*: "Imagine waiting in line at an airport with 6 queues, but you pick the wrong one and have to wait while others get served quickly. That's the challenge of multi-core scheduling."

##### Current Approach:
- One Red-Black tree per core
- Processes can migrate between cores based on load

> *Instructor Insight*: "This is a complex problem with no perfect solution, but modern systems use sophisticated algorithms to balance workloads across cores."

---

### Summary & Key Takeaways

#### Scheduling Algorithms Comparison
| Algorithm          | Preemptive? | Best For                          | Major Drawback               |
|--------------------|-------------|-----------------------------------|------------------------------|
| **FCFS**           | No          | Simple batch processing           | Convoy effect, poor response time |
| **SJF**            | No (usually)| Batch jobs with known run times   | Doesn't work well if jobs arrive at different times |
| **STCF**           | Yes         | Interactive systems               | High context switching overhead |
| **Round Robin**    | Yes         | General-purpose, fair distribution | Long turnaround for long jobs  |
| **MLFQ**           | Yes (dynamic)| Mixed interactive/batch workloads | Complex implementation        |
| **CFS**            | Yes         | Linux systems                     | Requires sophisticated data structures |

#### Key Principles to Remember
1. **Scheduling is about trade-offs**: You can't optimize for all metrics simultaneously.
2. **Interactive vs Batch Jobs**: Need different scheduling strategies (response time vs throughput).
3. **Starvation Prevention**: Always design algorithms so no process gets ignored indefinitely.
4. **Real-world Complexity**: Actual systems don't follow the simple assumptions we use in examples.

> *Instructor Insight*: "The most important thing about scheduling isn't knowing all the algorithms—it's understanding why they exist and when to use them."

---

### Study Questions

1. What is the difference between a scheduler and a dispatcher?
2. Calculate turnaround time for FCFS with jobs A(0,5), B(0,3), C(0,7).
3. Explain the convoy effect using an example from university life.
4. Why does SJF require all jobs to arrive simultaneously in its basic form?
5. How would you modify STCF if a job arrives while another is running?
6. What are the advantages and disadvantages of Round Robin scheduling?
7. Describe how I/O blocking affects process scheduling decisions.
8. Explain why fixed priority scheduling can lead to starvation.
9. How does MLFQ prevent long-running jobs from being starved?
10. What is vruntime in CFS, and how does it affect scheduling decisions?
11. Why might a malicious program try to "sleep" just before its time slice ends?
12. Explain the difference between response time and turnaround time.
13. How do Red-Black trees improve performance in CFS compared to other data structures?
14. What is the purpose of the `nice` command in Linux scheduling?
15. Describe a situation where FCFS would be preferable to SJF.

---

### Practical Exercise

**Scenario**: You're designing a scheduler for a hospital's operating room system with these requirements:
- 3 surgeons (A, B, C) available at different times
- Each surgery has an estimated duration
- Some surgeries require special equipment that takes time to prepare

Create a Gantt chart showing how you would schedule the following operations:

| Surgery | Arrival Time | Duration |
|---------|--------------|----------|
| A       | 0            | 120 min  |
| B       | 30           | 60 min   |
| C       | 45           | 90 min   |

Calculate:
- Turnaround time for each surgery
- Average turnaround time
- Response time for each surgery

*Hint: Consider how I/O blocking (equipment preparation) would affect scheduling.*


---

# Comprehensive Study Notes: CPU Scheduling
*Based on Textbook Chapter "Week 2: Cpu Sched"*

---

### I. Introduction to Scheduling
#### A. Core Problem Statement
> **THE CRUX**: *How should we develop a basic framework for thinking about scheduling policies? What are the key assumptions? What metrics are important? What fundamental approaches have been used in early computer systems?*  

- **Origins**: Scheduling principles originated from operations management (e.g., assembly lines), emphasizing efficiency.
- **Key Insight**: Human endeavors like manufacturing and computing share core concerns: *resource utilization*, *efficiency*, and *fairness*.  
  > *"Assembl[y] lines... require scheduling, and many of the same concerns exist therein."*

---

### II. Workload Assumptions (Simplifying Conditions)
*All assumptions are unrealistic but serve as a starting point for analysis.*

| **Assumption** | **Description**                                                                 | **Realism Check** |
|----------------|---------------------------------------------------------------------------------|-------------------|
| 1              | Each job runs for the same amount of time.                                      | ❌ Highly Unrealistic |
| 2              | All jobs arrive at the same time.                                               | ❌ Unlikely in practice |
| 3              | Once started, each job runs to completion (non-preemptive).                     | ⚠️ Partially Realistic (batch systems) |
| 4              | All jobs only use CPU (no I/O operations).                                      | ❌ Highly Unrealistic |
| 5              | Run-time of each job is known *a priori* ("oracle" assumption).                 | ❌ **Worst Assumption** |

> 💡 **Critical Note**:  
> *"The run-time of each job is known: this would make the scheduler omniscient... not likely to happen anytime soon."*

---

### III. Scheduling Metrics
#### A. Turnaround Time (Primary Metric)
- **Definition**:  
  \[
  T_{\text{turnaround}} = T_{\text{completion}} - T_{\text{arrival}}
  \]
- *Simplified under Assumption 2*: \(T_{\text{arrival}} = 0\) → \(T_{\text{turnaround}} = T_{\text{completion}}\).
- **Goal**: Minimize average turnaround time (e.g., for batch processing).

#### B. Response Time
- **Definition**:  
  \[
  T_{\text{response}} = T_{\text{first\_run}} - T_{\text{arrival}}
  \]
- *Best-case assumption*: Job produces response instantly upon first run.
- **Goal**: Minimize average response time (e.g., for interactive systems).

#### C. Fairness
- Measured by **Jain’s Fairness Index** [J91].
- **Trade-off with Performance**:  
  > *"Performance and fairness are often at odds... a scheduler may optimize performance but prevent some jobs from running, decreasing fairness."*

---

### IV. Scheduling Algorithms & Analysis

#### A. First In, First Out (FIFO) / FCFS
##### *Description*
- Simplest algorithm: Run jobs in arrival order.
- **Non-preemptive**: Runs job to completion before switching.

##### *Example 1: Equal Job Lengths*  
| Job | Arrival Time | Runtime | Completion Time |
|-----|--------------|---------|----------------|
| A   | 0            | 10      | 10             |
| B   | 0            | 10      | 20             |
| C   | 0            | 10      | 30             |

- **Average Turnaround Time**: \(\frac{10 + 20 + 30}{3} = 20\) seconds.

##### *Example 2: Unequal Job Lengths (Convoy Effect)*  
| Job | Arrival Time | Runtime |
|-----|--------------|---------|
| A   | 0            | 100     |
| B   | 0            | 10      |
| C   | 0            | 10      |

- **FIFO Schedule**:  
  - A runs (0–100), then B (100–110), then C (110–120).  
  - *Turnaround*: \(A=10


---

# Comprehensive Study Notes: Scheduling - Proportional Share

---

### **Core Concept & CRUX**  
**Proportional-Share Scheduling**: A scheduler that guarantees each process a *specific percentage* of CPU time (e.g., 75% for Process A, 25% for Process B), rather than optimizing for turnaround/response time.  
> **CRUX QUESTION**: How to design a scheduler to share the CPU proportionally? What are key mechanisms and their effectiveness?

---

### **Key Mechanisms & Theories**  

#### 🔑 **1. Ticket System: Foundation of Proportional Sharing**  
- **Tickets**: Numerical representation of a process’s *share* of resources (CPU, memory).  
  - `Process Share % = (Process Tickets / Total Tickets) × 100`  
  - Example: Process A has 75 tickets, B has 25 → A gets 75% CPU, B gets 25%.  

- **Lottery Scheduling**: Uses randomness to allocate CPU based on ticket count.  
  - *Mechanism*: At each time slice, pick a random number `0 ≤ winner < Total Tickets`. Process owning the winning ticket runs next.  
    > Example: A (tickets 0–74), B (75–99). Winning numbers `[63,85,70,...]` → Schedule: **A repeated 15x**, then **B 4x** (20% for B vs. desired 25%).  
    > *Why probabilistic?* Long-term fairness improves with more time slices; short-term deviations occur due to randomness.

- **Advantages of Randomness**:  
  | Advantage                | Explanation                                                                 |
  |--------------------------|-----------------------------------------------------------------------------|
  | Avoids corner cases      | Unlike LRU (worst-case for cyclic workloads), random avoids deterministic failures. |
  | Lightweight              | Minimal state: Only track *number of tickets per process* (no per-process CPU accounting). |
  | Fast                     | Random number generation is O(1); decision speed matches hardware limits. |

---

#### 🔑 **2. Ticket Mechanisms**  
##### a) **Ticket Currency**  
- Allows users to allocate tickets in their own "currency," converted globally by the system.  
  > *Example*:  
  > - User A: 100 global tickets → Allocates 500 each to jobs A1, A2 (in A’s currency).  
  >   Global conversion: `500 / 1000 × 100 = 50` per job.  
  > - User B: 100 global tickets → Allocates 10 to job B1 (B’s currency).  
  >   Global conversion: `10 / 10 × 100 = 100`.  
  > **Total Tickets**: A1=50, A2=50, B1=100 → Lottery runs over global tickets.  

##### b) **Ticket Transfer**  
- Process temporarily hands off its tickets to another process (e.g., client→server for faster work).  
  - *Use Case*: Client sends message to server; transfers tickets to prioritize the request. Server returns tickets after completion.  

##### c) **Ticket Inflation**  
- Temporarily increases/decreases a process’s ticket count in trusted environments.  
  > *Example*: A group of processes trusts each other → If Process X needs more CPU, it inflates its tickets (no communication needed).  
  > ⚠️ *Not for competitive settings* (greedy processes could monopolize resources).

---

#### 🔑 **3. Implementation: Lottery Scheduling**  
##### Pseudocode & Workflow (`Figure 9.1`):  
```c
int counter = 0;
int winner = getrandom(0, total_tickets); // Random number in [0, total-1]
node_t*current = head; 
while (current) {
    counter += current->tickets;
    if (counter > winner) break; // Found winner!
    current = current->next;
}
schedule(current); // Run the winning process
```

##### Efficiency Optimization:  
- **Sort processes by tickets descending** to minimize list iterations.  
  > *Example*: Processes A(100), B(50), C(250) → Sorted as [C, A, B]. For winner=300:  
  > - Counter += C (250) → 250 < 300? Yes → Next.  
  > - Counter += A (100) → 350 > 300 → **Winner = A** (only 2 iterations vs. 3 unsorted).  

---

#### 🔑 **4. Fairness Analysis: Lottery Scheduling**  
##### Fairness Metric (`F`):  
> `F = Time_First_Job_Completes / Time_Second_Job_Completes`  
- Perfect fairness → `F ≈ 1`.  
- *Example*: Jobs A/B with equal tickets (R=10) finish at times 10 and 20 → `F = 10/20 = 0.5`.  

##### Key Insight (`Figure 9.2`):  
| Job Length (R) | Average Fairness (F) |
|----------------|----------------------|
| Short (e.g., R=1) | Low (~0.4–0.6)       |
| Long (e.g., R=1000)| High (>0.95)         |

> **Conclusion**: Lottery scheduling achieves fairness *only over long time scales*; short jobs exhibit significant variance.

---

#### 🔑 **5. Stride Scheduling: Deterministic Alternative**  
- **Goal**: Achieve exact proportional sharing without randomness (Waldspurger [W95]).  

##### Core Mechanism:  
1. Each process has a `stride = Large_Number / Tickets` (e.g., 10,000/tickets).  
2. Track `pass_value` for each process (initially 0).  
3. **Scheduler picks the process with lowest `pass_value`** to run next.  
4. After running: `pass_value += stride`.  

> *Example*: A(100 tickets), B(50), C(250) → Strides: A=100, B=200, C=40.  
> **Trace (`Figure 9.3`)**:
| Pass(A)=100 | Pass(B)=200 | Pass(C)=40 | Who Runs? |
|-------------|-------------|------------|-----------|
| 0           | 0           | 0          | A         |
| 100         | 0           | 0          | B         |
| 100         | 200         | 0          | C         |
| ...         |             |            | **C runs 5x**, A 2x, B 1x → Matches ticket ratio (250:100:50 = 5:2:1). |

##### Lottery vs. Stride Comparison:  
| Feature                | Lottery Scheduling       | Stride Scheduling        |
|------------------------|--------------------------|--------------------------|
| **Fairness**           | Probabilistic (long-term) | Exact at cycle end       |
| **State per Process**  | Minimal (`tickets` only) | Requires `pass_value`    |
| **Adding New Processes**| Easy: Add with tickets   | Hard: Must set initial `pass_value` correctly |

> 💡 *Why use lottery?* Simpler for dynamic systems (e.g., new processes join mid-schedule).

---

#### 🔑 **6. Linux Completely Fair Scheduler (CFS)**  
##### Core Idea:  
- Replace fixed time slices with a **virtual runtime (`vruntime`)** that scales by process priority.  

##### Key Parameters & Formulas:  
| Parameter               | Description                                                                 |
|-------------------------|-----------------------------------------------------------------------------|
| `sched_latency`         | Target fairness period (default = 48 ms). Time slice per job = `sched_latency / n`. |
| `min_granularity`       | Minimum time slice (default = 6 ms) to avoid excessive context switches.    |

##### **Time Slice Calculation** (`Equation 9.1`):  
> `timeslice_k = weight_k / Σ(weight_i) × sched_latency`  

- *Example*: Process A (nice=-5 → weight=3121), B (default, nice=0 → weight=1024).  
  > Total weight = 3121 + 1024 = 4145.  
  > `timeslice_A ≈ (3121/4145) × 48 ms ≈ 36 ms`, `timeslice_B ≈ 12 ms`.  

##### **Virtual Runtime (`vruntime`) Update** (`Equation 9.2`):  
> `vruntime_i = vruntime_i + (weight0 / weight_i) × runtime_i`  
- Where `weight0 = 1024` (default weight).  
- *Example*: A runs for 1 ms → `vruntime_A += (1024/3121) × 1 ≈ 0.33 ms`. B runs same time → `vruntime_B += (1024/1024)×1 = 1 ms`.  
- **Result**: A accumulates vruntime slower than B due to higher priority.

##### Data Structure: Red-Black Tree  
| Why?                     | Benefit                                                                 |
|--------------------------|-------------------------------------------------------------------------|
| Avoids O(n) list searches | Modern systems have thousands of processes → Linear search wastes CPU.   |
| Logarithmic operations   | Insert/delete/find in **O(log n)** vs. linear for lists (e.g., 10,000 processes: log₂(10k)=~14 ops). |

> *Example*: Processes with `vruntime` values [1,5,9,...24] stored in red-black tree → Lowest vruntime found in O(log n) time.

##### Handling Sleeping Processes  
- **Problem**: A process waking after sleep (e.g., 10s) has much lower vruntime than others.  
- **CFS Fix**: Set `vruntime` to *minimum value* in the tree when it wakes up → Prevents starvation but causes short-sleeping jobs to miss fair share.

---

### **Summary of Scheduling Approaches**  

| Approach          | Fairness Type     | Key Mechanism                     | Pros                                      | Cons                                       |
|-------------------|-------------------|-----------------------------------|-------------------------------------------|--------------------------------------------|
| **Lottery**       | Probabilistic     | Random ticket selection           | Simple, no global state                   | Short-term unfairness                      |
| **Stride**        | Deterministic     | `stride = Large_Number / Tickets` | Exact fairness at cycle end               | Hard to add new processes                  |
| **CFS (Linux)**   | Weighted Fairness | `vruntime` + red-black tree       | Efficient, scales well, handles priorities | Complex; I/O jobs may starve               |

---

### **Critical Problems & Limitations**  
1. **I/O and Starvation**:  
   - Processes doing frequent I/O (e.g., waiting for disk) wake up with low vruntime → May monopolize CPU until they catch up (`CFS` mitigates this but not perfectly).  
2. **Ticket/Priority Assignment**:  
   - *Hard open problem*: How many tickets should a browser get? What `nice` value for a text editor?  
   - *Contrast with MLFQ*: General-purpose schedulers (e.g., Linux’s MLFQ) auto-assign priorities; proportional-share requires manual tuning.  
3. **When to Use Proportional Sharing**:  
   - Ideal in virtualized environments (e.g., assign 25% CPU to Windows VM, rest to Linux).  
   - *Not ideal* for I/O-heavy workloads or systems where priority assignment is non-trivial.

---

### **Key Formulas**  

| Formula                                      | Description                                  |
|----------------------------------------------|---------------------------------------------|
| `Share % = (Tickets / Total Tickets) × 100`  | Ticket-based CPU allocation                 |
| `stride = Large_Number / Tickets`            | Stride scheduling base value                |
| `timeslice_k = weight_k / Σ(weight_i) × sched_latency` | CFS time slice calculation |
| `vruntime_i += (weight0 / weight_i) × runtime_i` | CFS virtual runtime update |

---

### **15-20 Study Questions**  
*(For exam mastery and deep learning)*  

1. How does lottery scheduling achieve proportional sharing? Why is it probabilistic, not deterministic?  
2. Explain the *ticket currency* mechanism with a numerical example (include global conversion).  
3. What are three advantages of using randomness in scheduler design? Provide concrete examples for each.  
4. In stride scheduling, why must `stride = Large_Number / Tickets` instead of just `1/Tickets`?  
5. Calculate the exact run order for processes A(20 tickets), B(30 tickets), C(50 tickets) using stride scheduling (use large number=100).  
6. How does CFS handle a process waking up after 1 second of sleep? What is the trade-off?  
7. Derive `timeslice_A` for two processes: A (nice=-2, weight=8876), B (default, nice=0, weight=1024). Assume `sched_latency`=48ms.  
8. Why does CFS use a red-black tree instead of a sorted list? Quantify the performance difference for 10,000 processes.  
9. In lottery scheduling with two jobs (A:50 tickets, B:50 tickets), what is the probability job A finishes before job B if both have length R=2 time slices?  
10. How does ticket inflation differ from ticket transfer in practice? Give a use case for each.  
11. Why can’t stride scheduling easily handle new processes joining mid-schedule? What happens if you set `pass_value` to 0?  
12. Explain why CFS’s fairness metric (`F`) approaches 1 as job length increases (based on Figure 9.2).  
13. Describe the *crux* of proportional-share scheduling: Why is it harder than optimizing for response time?  
14. How does Linux CFS map `nice` values to weights in practice? What happens if a process has nice=+5 vs. nice=0?  
15. In what scenario would you choose lottery over stride scheduling, and vice versa? Justify with system properties.  
16. Why is the *minimum granularity* parameter (`min_granularity`) necessary in CFS? Provide an example where it prevents excessive context switches.  
17. How does ticket transfer improve performance in a client/server architecture? What happens if the server crashes after receiving tickets?  
18. Explain why proportional-share schedulers struggle with I/O-bound processes (e.g., web servers waiting for network packets).  
19. Calculate `vruntime` accumulation for two jobs: A (nice=-5, weight=3121) and B (default, weight=1024), each running 1ms in a CFS system with `sched_latency`=48ms.  
20. Compare the *state overhead* of lottery vs. stride scheduling per process. Which requires more memory? Why?

---

> **Final Insight**: Proportional-share scheduling is not "better" than other schedulers—it’s designed for specific use cases (e.g., cloud resource allocation). The choice depends on whether you prioritize simplicity (`lottery`), exact fairness (`stride`), or scalability/efficiency in modern systems (`CFS`). Mastery requires understanding *when* to apply each approach.


---

# Comprehensive Study Notes: Multi-Level Feedback Queue (MLFQ) Scheduling
*Based on OSTEP Chapter "Scheduling: The Multi-Level Feedback Queue"*

---

### **I. Introduction & Historical Context**  

#### **Core Problem Addressed by MLFQ**
- **Dual Optimization Challenge**:  
  - Optimize *turnaround time* (short jobs first, like SJF/STCF) → Requires knowing job length upfront.
  - Minimize *response time for interactive users* (like Round Robin) → But RR harms turnaround time.  
  > 💡 **The Crucial Question**: *"How can we schedule without perfect knowledge of job characteristics?"*

- **MLFQ's Approach**: Learns from past behavior to predict future needs, avoiding the need for prior knowledge.

#### **Historical Significance**
| Contributor | Work                          | Impact                                                                 |
|-------------|-------------------------------|------------------------------------------------------------------------|
| Corbato et al. (1962) | CTSS (Compatible Time-Sharing System) | First MLFQ implementation; led to Turing Award for Corbato.            |
| Later Refinements   | Multics OS                    | Evolved into foundational design principles in modern schedulers.       |

> 📌 **Key Insight**: MLFQ is a *learning system*—it adapts based on observed behavior (e.g., I/O patterns, CPU usage), similar to branch predictors or caching algorithms.

---

### **II. Core Principles of MLFQ**  
#### **Fundamental Rules**
| Rule | Description                                                                 |
|------|-----------------------------------------------------------------------------|
| **Rule 1** | *Priority-Based Selection*: Higher priority job runs (e.g., Q2 > Q1).        |
| **Rule 2** | *Equal Priority Handling*: Jobs at same level use Round Robin with queue’s quantum. |

#### **Key Concepts**
- **Queue Levels**: Multiple queues, each assigned a fixed priority (Q0 = lowest, Qn = highest).
- **Allotment**: Time limit a job can run at its current priority before demotion.
  - *Initial assumption*: Allotment = single time slice (quantum).  
- **Dynamic Priority Adjustment**: Priorities change based on observed behavior.

---

### **III. Initial MLFQ Design & Examples**  
#### **Rules #3–#4a/b: Basic Behavior**
| Rule | Description                                                                 |
|------|-----------------------------------------------------------------------------|
| **Rule 3** | New jobs start at *highest priority queue* (e.g., Q2).                       |
| **Rule 4a** | Job uses full allotment → Demoted to next lower queue.                      |
| **Rule 4b** | Job relinquishes CPU early (I/O) → Allotment reset; stays in same queue.   |

#### **Example 1: Long-Running CPU-Bound Job**
![Long-running job](https://i.imgur.com/8XJzZ9l.png)  
*Figure 8.2 from textbook*:  
- Time slice = allotment = **10 ms**.
- Job starts at Q2 (highest).
- After first time slice: Uses full allotment → Demoted to Q1.
- After second time slice: Demoted to Q0 (lowest), stays there forever.

> ✅ *Outcome*: Long jobs move down queues but never get CPU if interactive jobs dominate.

---

#### **Example 2: Short Interactive Job Arrives During a Long Run**  
![Short job example](https://i.imgur.com/5kR7cPm.png)  
*Figure 8.3 (left)*:
- **Job A**: Long-running, CPU-bound → Already in Q0.
- **Job B**: Short interactive (20 ms run time), arrives at T=100.
- **Behavior**:
  - B enters Q2 (highest priority).
  - Runs for two time slices (20 ms total) without using full allotment per queue?  
    → *Actually, since it finishes before demotion*, stays in high queues.  
  - A resumes at low priority after B completes.

> 💡 **Why MLFQ Approximates SJF**: Assumes all jobs are short until proven otherwise (demotes if they use CPU long).

---

#### **Example 3: I/O-Bound Interactive Job**  
![I/O example](https://i.imgur.com/4nJkD6d.png)  
*Figure 8.3 (right)*:
- **Job A**: Long-running batch job (black).
- **Job B**: Interactive job needing CPU for only **1 ms**, then does I/O.
- **Behavior**:
  - At T=50, B uses CPU for 1 ms → Relinquishes before allotment ends (Rule 4b) → Stays in Q2.
  - A runs during B’s I/O wait time.

> ✅ *Outcome*: Interactive jobs get quick response times; MLFQ correctly prioritizes them.

---

### **IV. Critical Flaws in Initial Design**  
#### **Problem #1: Starvation**
- **Cause**: Too many interactive jobs consume all CPU → Long-running jobs never run.
  > ❌ Example: If only short jobs exist, long jobs starve forever (no Rule 5).

#### **Problem #2: Gaming the Scheduler**
- **Attack Vector**:
  - Job runs for ~99% of allotment → Issues I/O to reset allotment → Stays in high queue.
  - *Result*: Monopolizes CPU (e.g., 99% share).
- **Why it works**: Rule 4b resets allotment on I/O, ignoring usage.

#### **Problem #3: Inability to Adapt**
- If a job transitions from CPU-bound → interactive:
  - It remains in low queue until boosted.
  - *Result*: Interactive behavior not recognized immediately.

---

### **V. Refinements & Solutions**  
#### **Attempt #2: Priority Boosting (Rule 5)**  
| Rule | Description                                                                 |
|------|-----------------------------------------------------------------------------|
| **Rule 5** | After time period **S**, move all jobs to *topmost queue* (Qn).             |

##### **How It Solves Flaws**
- ✅ **Prevents starvation**: All jobs get periodic high priority.
- ✅ **Adapts to behavior changes**: CPU-bound job becomes interactive → Boosted to top.

> ⚠️ **The "Voo-Doo Constant" S**:
> - Too large (e.g., 10s): Long jobs starve during peak interactive load.  
> - Too small (e.g., 1 ms): Interactive jobs get interrupted too often, harming responsiveness.
> - *Ousterhout’s Law*: "Avoid voo-doo constants" → S is tuned empirically or via ML.

##### **Example: With/Without Boost**  
![Priority boost example](https://i.imgur.com/VqXfGcP.png)  
*Figure 8.4 from textbook*:
- **Left (No Boost)**: Long job starves after short jobs arrive.
- **Right (Boost every 100 ms)**: Long job gets periodic CPU time.

---

#### **Attempt #3: Better Accounting**  
##### **Revised Rule 4**:  
> *"Once a job uses up its allotment at any level (regardless of I/O), demote it."*  

| Old Rules | New Rule                                                                 |
|-----------|--------------------------------------------------------------------------|
| Rule 4a/b | Reset allotment on I/O → Allows gaming.                                  |
| **Rule 4** | Track *total CPU time used per queue*, ignore I/O interruptions.         |

##### **Example: Preventing Gaming**  
![Gaming prevention](https://i.imgur.com/3RdJkYl.png)  
*Figure 8.5 from textbook*:
- **Left (Old Rules)**: Job uses 99% allotment → Does I/O → Stays in Q2, monopolizes CPU.
- **Right (New Rule)**: Same job must use full allotment across all time slices → Demoted to lower queue.

> ✅ *Outcome*: No gaming possible; fairer resource allocation.

---

### **VI. Tuning MLFQ Parameters**  
#### **Key Configuration Choices**
| Parameter          | Typical Value               | Rationale                                                                 |
|--------------------|-----------------------------|---------------------------------------------------------------------------|
| **Number of Queues** | 60 (Solaris default)        | Balances granularity vs. overhead.                                        |
| **Time Slice per Queue** | Q2: 10 ms, Q1: 20 ms, Q0: 40+ ms | High-priority queues need short quanta for responsiveness; low priority = longer bursts for CPU-bound jobs. |
| **Boost Period (S)**   | ~1 second (Solaris)         | Empirically tuned to balance starvation and interactive fairness.          |

#### **Real-World Implementations**
| System             | Implementation Detail                                                                 |
|--------------------|-------------------------------------------------------------------------------------|
| **Solaris**        | *Time-Sharing class* (`TS`): 60 queues, time slices from 20 ms (top) → hundreds of ms (bottom), boost every ~1s. |
| **FreeBSD**        | Decay-usage formula: Priority = `f(CPU_usage)` + decay over time (no fixed queues). |
| **Windows NT**     | MLFQ variant with priority classes; user-adjustable via `nice` command.              |

#### **Advanced Features**
1. **Reserved Priorities for OS Work**:  
   - Highest queue reserved for kernel tasks → User jobs never get top priority.
2. **User Advice (e.g., `nice`)**:  
   - Users can *suggest* priorities (`nice 5` lowers priority, `nice -5` raises it).  
   > 💡 **Ousterhout’s Law**: "OS rarely knows best for all processes → Provide advice interfaces."

---

### **VII. Summary of MLFQ Rules**  
| Rule | Description                                                                 |
|------|-----------------------------------------------------------------------------|
| 1    | Higher priority job runs (e.g., Q2 > Q1).                                   |
| 2    | Same priority: Round Robin with queue’s quantum.                            |
| 3    | New jobs start at *highest* priority queue.                                 |
| **4** | **Job uses full allotment → Demote to next lower queue** (ignores I/O).     |
| **5** | Periodically boost all jobs to topmost queue after time `S`.                |

> ✅ **Why MLFQ Wins**:  
> - *Short interactive jobs*: Run quickly at high priority.  
> - *Long CPU-bound jobs*: Demoted but get periodic boosts → Fair progress.  
> - *No prior knowledge needed* (learns from behavior).

---

### **VIII. Systems Using MLFQ**  
| System                | Scheduling Class/Implementation |
|-----------------------|---------------------------------|
| BSD UNIX derivatives  | `nice` + decay-usage scheduler ([LM+89]) |
| Solaris               | Time-Sharing (`TS`) class       |
| Windows NT            | Base scheduler (MLFQ variant)   |

> 🔍 **Why MLFQ Dominates**: It achieves *both* low response time for interactivity and good throughput for batch workloads.

---

### **IX. Comprehensive Study Questions**  
*(15-20 questions covering all concepts)*  

#### **Conceptual Understanding**
1. Why does MLFQ avoid needing prior knowledge of job length? Explain using the "learning from history" principle.
2. How would you modify Rule 4b to prevent gaming *without* adding a new rule?
3. In what scenario would a long-running CPU-bound job become interactive, and how does MLFQ handle it after **Rule 5** is implemented?
4. Why do high-priority queues use short time slices (e.g., 10 ms), while low-priority queues use longer ones (e.g., 100+ ms)?
5. What happens if `S` in Rule 5 is set to *zero*? How does this affect system behavior?

#### **Problem-Solving**
6. A job runs for 9 ms, then issues I/O at T=9ms (allotment = 10 ms). Under the initial MLFQ rules (Rule 4b), what happens next?
7. Given a 2-queue MLFQ with allotments: Q2=5ms, Q1=10ms:
   - Job A runs for 6 ms at Q2 → What queue does it move to?  
   - Job B arrives after A has run 3 ms; how long until B is demoted?
8. Calculate the minimum `S` (boost period) needed so a single CPU-bound job gets ≥5% of CPU with:
   - Quantum = 10 ms at top queue.
   - System runs only one interactive job + one CPU-bound job.

#### **Critical Analysis**
9. Why is "voo-doo constant" S considered problematic? How does modern ML-based tuning (e.g., [A+17]) address this?
10. Explain how Rule 4 prevents gaming using the *allotment accounting* concept.
11. Describe a workload where **Rule 5** causes *worse* response time for interactive jobs than without boosting.

#### **Real-World Application**
12. How would you configure MLFQ to behave exactly like Round Robin?
    - (Hint: All queues must have same priority; no demotion.)
13. Why might an OS reserve the highest queue for kernel work? Give a security example.
14. In Solaris, how does `nice` interact with Rule 5 and allotment tracking?

#### **Advanced Scenarios**
15. A job transitions from CPU-bound (Q0) to interactive at T=200ms:
    - Without boosting: How long until it gets high priority?  
    - With boost every 1s: When does it get boosted?
16. Why is MLFQ *not* suitable for real-time systems requiring hard deadlines?

#### **Exam-Style Questions**
17. *"MLFQ approximates SJF."* Justify this statement with reference to Example 2.
18. Contrast the goals of:
    - Round Robin (minimizes response time)
    - Shortest Job First (minimizes turnaround time)  
    How does MLFQ balance both?
19. Explain why **Rule 4** is a *single rule* replacing Rules 4a/4b.
20. If all jobs are I/O-bound, what happens to their priorities? Why?

---

> 💡 **Final Mastery Tip**: To deeply understand MLFQ, simulate it with the `mlfq.py` tool (Slide 12). Test edge cases like gaming attempts and behavior changes.

*Comprehensive notes based on OSTEP Chapter "Scheduling: The Multi-Level Feedback Queue" (v.1.10).*


---

# Comprehensive Study Notes: The Abstraction – Address Spaces
*Based on Chapter 13 from "Operating Systems: A Modern Perspective"*

---

### **I. Introduction to Memory Abstraction**  
#### *The Evolutionary Necessity of Abstraction*
- **Early systems (pre-multiprogramming)** had no memory abstraction:
  - Physical memory layout was directly exposed to users.
  - OS existed as a library starting at physical address `0`.
  - Only one process ran at a time, loaded after the OS (`64KB` offset in Figure 13.1).
- **Key insight**: Users’ demands for "ease of use," "high performance," and "reliability" drove complexity (Slide 1).  
> *Example*: Early machines lacked even basic memory protection—processes could overwrite each other’s data.

---

### **II. Multiprogramming & Time Sharing**  
#### *Why Systems Evolved*
| Concept | Purpose | Historical Context |
|---------|---------|--------------------|
| **Multiprogramming** (DV66) | Run multiple processes concurrently to improve CPU utilization when machines cost \$100k–\$1M. | Machines were expensive; idle time = wasted money. |
| **Time Sharing** (S59, L60, M62) | Enable interactive use by allowing rapid switching between user tasks for timely responses. | Programmers tired of long batch-debug cycles ([CV65]). |

#### *The Initial Time-Sharing Approach & Its Flaw*
- **Flawed implementation**:  
  Save entire memory state to disk when switching processes (Figure 13.2).  
  → **Problem**: Disk I/O for full-memory saves is prohibitively slow as memory grows.
- **Correct approach** (adopted): Keep all processes in physical RAM; switch only register states.  
  > *Example*: Figure 13.2 shows three processes sharing `512KB` physical memory, each with a dedicated segment.

#### *Critical New Demand: Protection*
> *"Allowing multiple programs to reside concurrently makes protection an important issue."* (Slide 2)  
- **Why?** Processes must not read/write other processes’ memory.  
- **Consequence**: OS must enforce isolation between address spaces.

---

### **III. The Address Space Abstraction**  
#### *Definition & Core Purpose*
> **Address space**: A process’s view of its private, contiguous virtual memory (Slide 3).  
> - Contains all runtime state: code, stack, heap.
> - Abstracts physical memory layout; programs believe they own `0`–`max address`.

---

#### *Structure of an Address Space*  
*(Figure 13.3 Example: 16KB Virtual Memory)*  

| Segment      | Purpose                                                                 | Growth Direction | Key Characteristics                     |
|--------------|-------------------------------------------------------------------------|------------------|------------------------------------------|
| **Code**     | Instructions (static, fixed size)                                       | Fixed at top (`0`–`1KB`) | Packed first; never grows.               |
| **Heap**     | Dynamically allocated memory via `malloc()`/`new`                       | Grows downward   | Starts after code (`1KB`); expands as needed (e.g., `malloc(100)`). |
| **Stack**    | Local variables, function calls, arguments, return values               | Grows upward     | Starts at top (`16KB`) and shrinks/grows with procedure calls. |

> *Why opposite growth?*  
> To prevent heap/stack collisions (e.g., `malloc` growing down vs. stack growing up).

#### *Critical Insight: Physical vs. Virtual Layout*
- **Reality**: Processes are loaded at arbitrary physical addresses (Figure 13.2).  
  - Process A → Physically at `320KB`, but thinks it’s at `0`.  
  - Process B → Physically at `64KB`, but thinks it’s at `0`.
- **Problem**: How to make each process believe its memory starts at `0` while sharing physical RAM?

---

### **IV. Virtual Memory: The Core Problem**  
#### *The Crux of the Issue*  
> *"How can the OS build a private, potentially large address space for multiple processes on top of single physical memory?"* (Slide 5)  

- **Virtual Address**: What program uses (`0` in Process A’s view).  
- **Physical Address**: Actual location in RAM (`320KB` for Process A).  
- **Solution Requirement**: Hardware + OS must translate virtual → physical addresses transparently.

---

#### *Goals of a Virtual Memory System*  
| Goal | Explanation | Why It Matters |
|------|-------------|----------------|
| **Transparency** (Slide 5) | Program unaware memory is virtualized; behaves as if own private RAM. | Enables legacy software to run without modification. |
| **Efficiency** | Minimize slowdown from translation and minimize overhead for metadata storage. | Avoids performance penalty of full-memory swaps. |
| **Protection** (Slide 5) | Prevent processes from accessing other processes’ memory or OS code. | Ensures isolation; stops malicious or faulty programs from crashing the system. |
> *Key Principle: Isolation*  
> > *"If two entities are properly isolated, one can fail without affecting the other."* (Slide 6)  
> - **Microkernels** ([BH70], [R+89]) take this further by isolating OS components.

---

### **V. Critical Realization: All Addresses Are Virtual**  
#### *Why This Matters for Programmers*
- **Every address you see in user code is virtual**:  
  ```c
  printf("location of heap: %p\n", malloc(100)); // Output: e.g., "0x1096008c0"
  ```
- **Output example** (Slide 7):  
  - Code location (`main`): `0x1095afe50` → Virtual address.  
  - Heap allocation: `0x1096008c0` → Virtual address.  
  - Stack variable: `0x7fff691aea64` → Virtual address.  

> **Why?** The OS/hardware translates virtual addresses to physical ones *before* accessing RAM.

---

### **VI. Summary of Key Concepts**  
| Concept | Definition | Significance |
|---------|------------|-------------|
| **Address Space** | Process’s private view of memory (code, stack, heap). | Foundation for virtualization; enables isolation. |
| **Virtual Address** | Logical address used by program (`0`–`max`). | Abstracts physical layout; all user-visible addresses are virtual. |
| **Physical Address** | Actual RAM location where data resides (e.g., `320KB`). | Managed entirely by OS/hardware translation. |
| **Virtualization** | Mapping of virtual → physical memory via hardware + OS support. | Enables multiple processes to share RAM safely. |
| **Isolation** | Processes cannot access each other’s address spaces or the OS kernel. | Core security/reliability principle (via protection). |
---

### **VII. Historical Context & References**  
*(Key papers cited in Slide 8)*  

| Reference | Contribution | Significance |
|-----------|--------------|--------------|
| [DV66] | First formal paper on multiprogramming semantics. | Established foundational theory for modern OS scheduling. |
| [S59] | Earliest reference to time sharing ("Time Sharing in Large Fast Computers"). | Pioneered interactive computing concepts. |
| [M83] (McCarthy) | Historical account of time-sharing origins; disputes Strachey’s priority. | Highlights iterative development of ideas. |
| [BH70], [R+89] | Microkernel design philosophy ("Nucleus" and "Mach"). | Enabled modern isolation techniques in OS reliability. |
> **Quote from [CV65]**:  
> *"The original goal was to time-share computers... giving each programmer the illusion of having the whole machine."*

---

### **VIII. Study Questions** (15-20)  

#### *Conceptual Understanding*
1. Why did early systems lack memory abstraction? How did user expectations change this?
2. Explain why saving/restoring entire physical memory during process switches was inefficient.
3. Describe how the heap and stack grow in an address space, and why they are placed at opposite ends.
4. What is a *virtual address* vs. a *physical address*? Provide a concrete example from Slide 7.
5. Why must virtual memory systems prioritize **transparency** for user programs?
6. How does the principle of **isolation** relate to protection in VM systems?

#### *Analysis & Application*
7. In Figure 13.2, if Process A requests `malloc(4KB)`, where would it be placed? Justify your answer.
8. If a process writes to virtual address `0x100` (which maps to physical `512KB`), what happens when another process tries to access its own virtual `0x100`?
9. Why is the **code segment** static while heap/stack are dynamic? How does this affect memory layout design?
10. A program prints a pointer using `%p`. Is that address physical or virtual? Explain why.

#### *Critical Thinking*
11. Could time-sharing exist without virtual memory? Justify your answer.
12. Why is **efficiency** a key goal for VM systems (beyond just "speed")?
13. How would the absence of protection in early OSs lead to system crashes or security breaches?
14. In Figure 13.3, why does code start at `0` while stack starts near `16KB`? What if they were reversed?
15. Explain how microkernels ([BH70]) leverage isolation for reliability compared to monolithic kernels.

#### *Advanced Application*
16. If a process’s address space is `4GB`, but physical RAM is only `8GB`, how many processes can run simultaneously without swapping? (Assume no overhead.)
17. Describe the hardware component that enables fast virtual-to-physical translation (hint: Slide 5).
18. Why does the OS *not* require programs to know their physical memory location?
19. How would a malicious process exploit weak protection in VM systems? Give an example.
20. In homework question #7, why might `pmap` show more than just code/stack/heap segments?

---

### **IX. Key Takeaways for Exams**  
- ✅ **Address space = Private virtual memory view per process** (code + stack + heap).  
- ✅ **Virtual addresses are always used by programs; physical addresses are hidden from users**.  
- ✅ **Three VM goals**: Transparency, Efficiency, Protection → All enable isolation.  
- ✅ **Heap grows downward**, **stack grows upward** to avoid collisions in a single address space.  
- ✅ **No process can access another’s memory without OS/hardware translation (protection)**.  

> *"The entire approach requires mechanism (low-level machinery) and policies (e.g., page replacement)."* — Slide 6

---

**Study Tip**: Always ask: *Is this a virtual or physical address? What would happen if protection failed here?*  
**Exam Focus**: Expect questions on translating between virtual/physical addresses, goals of VM systems, and consequences of missing isolation.


---

# Comprehensive Study Notes: Virtual Memory Mechanism - Address Translation




---

### **I. Core Philosophy of Virtualization**  
#### **A. Limited Direct Execution (LDE)**  
- **Definition**: A strategy where the OS *minimally intervenes* in program execution, allowing direct hardware access for most operations but interposing at critical points (*e.g., system calls, interrupts*) to maintain control.  
- **Goals Achieved**:  
  - ✅ **Efficiency**: Minimize OS overhead by letting programs run directly on hardware.  
  - ✅ **Control**: Ensure the OS maintains authority over hardware resources (CPU/memory).  
- *Why it matters*: This principle underpins both CPU and memory virtualization.  

#### **B. The Virtual Memory Crux**  
> *"How to efficiently and flexibly virtualize memory?"*  
This question drives all design decisions:  
| Requirement          | Why It Matters                                                                 |
|----------------------|-------------------------------------------------------------------------------|
| **Efficiency**       | Hardware must handle translations quickly (no OS involvement per access).     |
| **Flexibility**      | Programs should use address spaces as if they were contiguous and isolated.   |
| **Control/Protection** | Prevent processes from accessing memory outside their own space.              |

---

### **II. Core Mechanism: Hardware-Based Address Translation**  
#### **A. Definition & Purpose**  
- **Hardware-based address translation**: A mechanism where hardware *automatically converts* each virtual address (generated by a process) into a physical address (actual location in RAM).  
  - **Process**: Every memory access (`load`, `store`, instruction fetch) triggers this conversion.  
- **Why it’s essential**: Enables the illusion that every program has its own private, contiguous memory space while physically sharing RAM with other processes.  

#### **B. How It Works: Base-and-Bounds Relocation**  
##### *Key Hardware Components* (Slide 9):  
| Register          | Role                                                                 | Critical Property                     |
|-------------------|----------------------------------------------------------------------|---------------------------------------|
| **Base Register** | Stores the starting physical address of a process’s memory space      | Added to every virtual address         |
| **Bounds Register** | Defines the maximum size of the process’s address space (e.g., 16 KB) | Checks if an access is within bounds   |

##### *Translation Formula*:  
> **Physical Address = Virtual Address + Base**  

##### *Bounds Check Logic*:  
- If `Virtual Address < 0` OR `Virtual Address ≥ Bounds`, → **Illegal Access → CPU Exception**.  
- *Example (Slide 8)*:  
  | Virtual Address | Physical Address | Outcome       |
  |----------------|------------------|---------------|
  | 0              | 16 KB            | Valid         |
  | 1 KB           | 17 KB            | Valid         |
  | 3000 (decimal) | 19,384 (hex: 4BC8)| Valid       |
  | 4400           | —                | **Fault**     |

##### *Why Bounds Check is Critical*:  
- Prevents processes from accessing other processes’ memory or OS kernel space.  
- Without it, a malicious process could overwrite the trap table (Slide 12), hijacking system control.  

---

### **III. Detailed Example: Process Execution with Base-and-Bounds**  
#### **A. Setup (Figure 15.1 & Slide 4)**  
| Component          | Address Space Location | Value/Description                     |
|--------------------|------------------------|---------------------------------------|
| Code Section       | Starts at virtual address `0x80` (128) | Instruction sequence: `movl`, `addl`, `movl` |
| Variable `x`       | Virtual address `0x3C80` (15 KB)      | Initial value = 3000                  |

#### **B. Execution Flow**  
*(Tracing the assembly code from Slide 3)*:  
```assembly
128: movl 0x0(%ebx), %eax   ; Load x into eax (virtual address 15 KB)
132: addl $0x03, %eax       ; Increment by 3
135: movl %eax, 0x0(%ebx)  ; Store back to x
```
##### *Hardware Translation During Execution*:  
| Step                          | Virtual Address | Base (32 KB = 32768) | Physical Address |
|-------------------------------|-----------------|----------------------|------------------|
| Fetch instruction at `128`    | 128             | +32,768              | **32,896**       |
| Load from virtual address `15 KB` (15,360) | 15,360      | +32,768              | **48,128** (≈47 KB) |
| Store back to same location   | 15,360          | +32,768              | **48,128**       |

> 💡 *Key Insight*: The process thinks it’s using addresses `0`–`16KB`, but hardware maps these to physical memory starting at `32 KB`.  

---

### **IV. Hardware Requirements for Base-and-Bounds (Slide 9)**  
| Requirement                          | Purpose                                                                 |
|--------------------------------------|-------------------------------------------------------------------------|
| **Privileged Mode**                  | OS runs in kernel mode; user processes run in user mode (prevents tampering with base/bounds) |
| **Base/Bound Registers per CPU**     | Hardware registers for address translation and bounds checks            |
| **Address Translation Circuitry**    | Adds `base` to virtual addresses on-the-fly                             |
| **Privileged Instructions**          | OS-only instructions to modify base/bounds (e.g., `set_base()`)         |
| **Exception Handling Mechanism**     | Triggers when illegal access occurs → terminates process                |

#### *Why Privileged Mode is Non-Negotiable*  
- If a user program could change the base register, it would:  
  - Access other processes’ memory.  
  - Bypass bounds checks entirely.  
  > *"Imagine the havoc..."* (Slide 7) — This is why hardware enforces privilege separation.

---

### **V. Operating System Responsibilities**  
#### *Critical Tasks for OS Implementation (Slide 10)*:  

| Task                          | How It’s Implemented                                                                 |
|-------------------------------|------------------------------------------------------------------------------------|
| **Memory Allocation**         | Uses a `free list` (list of unused memory blocks) to find space for new processes.   |
| **Process Termination**       | Reclaims all allocated memory → adds it back to the free list.                      |
| **Context Switching**         | Saves/Restores base/bounds registers in process control block (PCB).                |
| **Exception Handling**        | Installs handlers for illegal accesses (e.g., terminates offending process).         |

#### *Example: Context Switch Workflow*  
1. OS stops Process A → saves its `base` and `bounds` values to PCB(A).  
2. OS loads base/bounds from PCB(B) into CPU registers.  
3. Resumes Process B with new memory mappings (transparent to the process!).  

> ⚠️ *Critical Insight*: The process never knows it was relocated — this is **transparency** in action.

---

### **VI. Limitations of Base-and-Bounds & Path Forward**  
#### **A. Internal Fragmentation Problem**  
- **Cause**: Fixed-size memory slots (e.g., 16 KB) waste space between allocated processes.  
- *Example*: Process uses only `8 KB` in a `16 KB` slot → wasted `8 KB`.  
  > *(Figure 15.2: Physical memory shows unused gaps at 16–32 KB and 48–64 KB)*  

#### **B. Why This Matters**  
- Limits system scalability (fewer processes can run concurrently).  
- Motivates the need for more advanced techniques like **segmentation** (next chapter) to avoid fragmentation.

---

### **VII. Key Concepts & Definitions**  
| Term                          | Definition                                                                 |
|-------------------------------|----------------------------------------------------------------------------|
| **Virtual Address Space**     | The memory layout a process *thinks* it has (addresses `0`–`size`).         |
| **Physical Memory**           | Actual RAM locations where data resides (`32 KB`, `48 KB`, etc.).          |
| **Dynamic Relocation**        | Hardware-based address translation that relocates processes at runtime.     |
| **Interposition**             | Hardware/software inserting itself between a process and memory access (e.g., MMU). |
| **Free List**                 | Data structure tracking unused physical memory blocks (`[16KB-32KB]`, `[48KB-64KB]`). |

---

### **VIII. Summary of Core Principles**  
> ✅ **Efficiency**: Hardware handles translation per access (no OS overhead).  
> ✅ **Control**: Bounds register + hardware checks enforce memory isolation.  
> ✅ **Flexibility**: Processes use contiguous virtual addresses; physical layout is hidden.  
> ❌ **Limitation**: Internal fragmentation wastes memory → requires next-level techniques.  

---

### **IX. Study Questions (Exam-Prep Focus)**  
1. What are the *two* key goals of limited direct execution? How does address translation support them?  
2. Explain why a process cannot modify base/bounds registers in user mode. What would happen if it could?  
3. Given virtual address `0x400` (1 KB), base register = 64 KB, bounds = 16 KB: Is this access valid? If yes, what is the physical address?  
4. How does hardware-based translation differ from *static relocation* (software-only)? Why was static relocation abandoned?  
5. In Figure 15.2, why can’t Process A use memory at `0–16KB` even though it thinks its space starts there?  
6. What happens during a **bounds violation** exception? Describe the OS’s response step-by-step.  
7. Why is *internal fragmentation* specific to fixed-size allocation (like base-and-bounds)? Contrast with external fragmentation.  
8. How does the `free list` data structure help in memory management? Provide an example of its contents from Figure 15.2.  
9. Trace the translation for a load instruction at virtual address `0x3C80` when base = 32 KB, bounds = 16 KB. Is it valid? Why or why not?  
10. What is *transparency* in memory virtualization? How does base-and-bounds achieve it?  
11. Describe the role of privileged mode during a context switch (Slide 12).  
12. If bounds = `8 KB`, what range of virtual addresses would be valid for a process with no offset?  
13. Why is *dynamic relocation* called "dynamic"? How does this differ from static relocation?  
14. What hardware component performs address translation in base-and-bounds? Where is it located?  
15. In the homework simulation (`relocation.py`), what happens when you set `-l 0` (bounds = 0)? Why?  
16. How does internal fragmentation affect system performance? Propose one way to mitigate it *without* changing base-and-bounds.  
17. Explain why a process cannot access memory beyond its bounds using the formula `physical address = virtual + base`.  
18. What would happen if the OS forgot to save/restore base/bounds during context switching?  
19. Compare and contrast **base register** vs. **bounds register** in terms of their roles in translation and protection.  
20. Why does Slide 7 state that "the bounds register holds the *size* of the address space" (not its end)? How is this logically equivalent to holding the physical endpoint?  

---

### **X. References & Historical Context**  
- **Early Work**: Base-and-bounds was independently developed for:  
  - IBM Stretch computer (multiprogramming),  
  - MIT time-sharing systems,  
  - Burroughs B5000 (hardware-interpreted descriptors).  
- *Key Paper*: Saltzer and Schroeder’s *"Protection of Information in Computer Systems"* (CACM 1974) [SS74].  
- **Modern Relevance**: Base-and-bounds is the foundation for modern MMUs, though today’s systems use paging instead to avoid fragmentation.  

> 💡 *Final Insight*: This chapter shows how OS design balances efficiency (`hardware translation`), control (`bounds checks`), and flexibility (`transparent relocation`) — a blueprint still used in all modern operating systems.

---

## 📝 Study Guide Summary

This combined study guide contains all notes for **Week 2**, automatically generated from 6 individual files.

### Individual Files Included:

1. `20250816_lecture_2.md`
2. `20250816_textbook_cpu_sched.md`
3. `20250816_textbook_cpu_sched_lottery.md`
4. `20250816_textbook_cpu_sched_mlfq.md`
5. `20250816_textbook_vm_intro.md`
6. `20250816_textbook_vm_mechanism.md`

### Usage Tips:
- Use the table of contents above to navigate quickly
- Each section is separated by horizontal rules (`---`)
- Individual files are still available for focused study
- This combined file is perfect for comprehensive review

---
*Generated by [LectureQ](https://github.com/yourusername/lectureq) - Your AI-powered study companion* 🤖

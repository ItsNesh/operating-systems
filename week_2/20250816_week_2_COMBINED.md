<!-- 
Combined Study Guide
Generated by LectureQ
Date: 2025-08-16 03:08:12
Week/Module: Week 2
Files Combined: 6
Content Type: Mixed
-->

# Week 2 - Complete Study Guide

> **Auto-generated combined study guide**  
> Generated from 6 individual notes files  
> Date: August 16, 2025

## 📚 Table of Contents

1. [Week 2: Lecture 2 - Operating Systems Scheduling](#week-2-lecture-2---operating-systems-scheduling)
2. [Comprehensive Study Notes: CPU Scheduling (Week 2)](#comprehensive-study-notes-cpu-scheduling-(week-2))
3. [Hierarchical Headers](#hierarchical-headers)
4. [Comprehensive Study Notes: Multi-Level Feedback Queue (MLFQ) Scheduling](#comprehensive-study-notes-multi-level-feedback-queue-(mlfq)-scheduling)
5. [Heading for main section](#heading-for-main-section)
6. [Comprehensive Study Notes: Virtual Memory Mechanism - Address Translation](#comprehensive-study-notes-virtual-memory-mechanism---address-translation)


---

# Week 2: Lecture 2 - Operating Systems Scheduling

<think>
I need to create comprehensive lecture notes for a student who missed class on Week 2, Lecture 2 about Operating Systems Scheduling.

Let me analyze the requirements carefully:

- I must synthesize all information from both slides and transcript
- Write natural, flowing notes that include EVERY concept, example, and explanation
- Rephrase and improve explanations for clarity (don't copy verbatim)
- Make it educational so a student can learn everything from these notes alone
- Include ALL algorithms, examples, calculations, key insights

I'll structure the notes with:
1. Clear H1 title including week/lecture info
2. Overview section immediately after title
3. Hierarchical headers for organization
4. Key insight sections formatted as specified
5. Comprehensive summary at the end

Let me go through all the slides and transcript to extract all information, then synthesize it into flowing educational notes.

First, I'll identify all key concepts from both sources:

From Slides:
- Scheduling overview (FCFS, SJF, STCF, RR, MLFQ)
- Scheduler vs Dispatcher
- Process states (Running, Ready, Blocked)
- Definitions: Workload, Job, Burst Time, Arrival Time
- Metrics: Turnaround Time, Response Time, Wait Time, Throughput, Resource Utilisation, Overhead, Fairness
- Gantt Charts and Tables for scheduling examples
- Scheduling Assumptions
- Convoy Effect with FCFS
- Preemptive vs Non-preemptive
- I/O Blocking concepts
- Priority systems (Fixed vs Dynamic)
- Multi-level Feedback Queue (MLFQ) rules
- Lottery Scheduling
- Completely Fair Scheduler (CFS)

From Transcript:
- Instructor's explanations of scheduling concepts
- Hospital analogy for understanding metrics
- Gantt chart explanation with examples
- Discussion of assumptions in scheduling algorithms
- Convoy effect explained through assignment example
- Preemptive vs Non-preemptive discussion
- I/O Blocking and why it matters
- Priority systems and how they work
- MLFQ details including the "evil" strategy question
- Lottery Scheduling explanation
- Completely Fair Scheduler (CFS) with vruntime concept
- Red-black tree explanation for CFS implementation

Now, I'll organize all this information into a logical flow that makes sense as educational notes.

I need to make sure I include:
1. All algorithms and their metrics calculations
2. Every example from the slides with step-by-step explanations
3. All instructor insights and clarifications
4. All student questions and responses

Let me create an outline for my notes:


### Overview
[Concise summary of what this lecture covers]

#### Process States & Basic Concepts
- Scheduler vs Dispatcher
- Workload, Job, Burst Time, Arrival Time definitions
- Process states (Running, Ready, Blocked)

#### Key Metrics for Scheduling
- Turnaround Time
- Response Time
- Wait Time
- Throughput
- Resource Utilisation
- Overhead
- Fairness

#### Basic Scheduling Algorithms
- First Come, First Serve (FCFS)
- Shortest Job First (SJF)
- Shortest Time to Completion First (STCF)
- Round Robin (RR)

#### Advanced Concepts
- Preemptive vs Non-preemptive scheduling
- I/O Blocking considerations
- Priority systems (Fixed vs Dynamic)
- Multi-level Feedback Queue (MLFQ) rules and implementation details
- Lottery Scheduling
- Completely Fair Scheduler (CFS) with vruntime concept

### Summary & Key Takeaways

Now, let me craft the actual notes following all requirements.

I'll make sure to:
1. Rephrase everything in clear educational language
2. Include every example with calculations shown step-by-step
3. Add instructor insights as specified
4. Format key concepts properly (bold for important terms)
5. Create tables where needed for clarity

Let me start writing the notes.
</think>

# Week 2: Lecture 2 - Operating Systems Scheduling

### Overview
This lecture explores process scheduling in operating systems, covering fundamental algorithms, metrics for evaluation, and advanced techniques to balance system performance with user experience. We'll examine how operating systems manage CPU time among competing processes while optimizing key performance indicators like response time and throughput.

---

### Core Concepts: Process States & Scheduling Fundamentals

#### Scheduler vs Dispatcher
Operating systems use two distinct but related components:

- **Scheduler**: The *conceptual* component that determines *when* to run a process (the "what" of scheduling)
  
  > *"The scheduler is the logic for job choice. It decides which process should be dispatched next."*

- **Dispatcher**: The *mechanical* component that handles the actual switching between processes (the "how")
  
  > *"The dispatcher determines how to switch between processes, handling user mode/kernel mode transitions and saving Process Control Blocks (PCBs)."*

#### Process States
Processes move through three primary states during execution:

| State      | Description                                                                 |
|------------|-----------------------------------------------------------------------------|
| **Running** | The process is currently executing on the CPU                                |
| **Ready**   | The process has all resources needed but isn't currently using the CPU       |
| **Blocked** | The process is waiting for an event (typically I/O completion)               |

> *Instructor Insight*: "Imagine a hospital operating room. When you're in surgery, you're 'running.' When you've finished your procedure and are waiting to be moved out of the OR, you're 'ready.' If you need additional tests before proceeding with another operation, you're 'blocked.'"

#### Key Definitions
- **Workload**: The collection of jobs (tasks) that need processing
- **Job**: A single CPU burst of a process (the unit we schedule)
- **Burst Time**: How long a job needs to run on the CPU
- **Arrival Time**: When a job arrives in the system

> *Instructor Emphasis*: "If you don't know what 'metric' means at this point in your university careers, I feel very bad for you. A metric is something we use to determine if scheduling is good or bad."

---

### Key Scheduling Metrics: What We're Trying To Optimize

#### Hospital Analogy (Simplified)
Imagine managing an operating room with multiple patients needing surgery:

| Metric                | Definition                                                                 | Real-World Example                          |
|-----------------------|----------------------------------------------------------------------------|---------------------------------------------|
| **Turnaround Time**   | Total time from arrival to completion                                      | How long a patient waits for their surgery  |
| **Response Time**     | Time between arrival and when the process starts running                     | How quickly you get called into the OR      |
| **Wait Time**         | Time spent waiting in queue (not including I/O)                            | Waiting time before being prepared for surgery |
| **Throughput**        | Number of jobs completed per unit time                                     | Patients who successfully complete surgery  |
| **Resource Utilization** | Percentage of CPU time used by processes                                   | How often the OR is actively performing surgery |
| **Overhead (Switches)** | Cost of context switching between processes                                | Time wasted changing surgical teams         |
| **Fairness**          | Equal distribution of resources among all jobs                             | Balanced assignment of surgeries to patients |

> *Instructor Insight*: "The goal isn't just to get things done quickly, but to do it in a way that makes users happy. If you're waiting 3 seconds for your computer to respond after clicking something, you'll be frustrated—unlike waiting 2 minutes before a long computation finishes."

---

### Scheduling Algorithms: Basic Approaches

#### Gantt Charts & Tables
We visualize scheduling using **Gantt charts** (time-based diagrams) and **tables** showing arrival times, burst times, completion times, and metrics.

##### Example Table Structure:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| A   | 0            | 5          | 5               | 5              |

> *Instructor Insight*: "Gantt charts are your life once you have a job. They're the visual representation of how we schedule tasks over time."

---

#### First Come, First Serve (FCFS)
**Concept**: Processes run in the order they arrive.

##### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| A   | 0            | 5          | 5               | 5              |
| B   | 0            | 5          | 10              | 10             |
| C   | 0            | 5          | 15              | 15             |

**Average Turnaround Time**: (5 + 10 + 15) / 3 = **10**

##### Problem: The Convoy Effect
When a long job arrives first, it "blocks" shorter jobs that arrive later.

> *Instructor Insight*: "This is like doing your large assignment before your tutorial. It's not how university students actually operate—nobody does this because it leads to the convoy effect."

**Worse Example:**
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| A   | 0            | 20         | 20              | 20             |
| B   | 0            | 4          | 24              | 24             |
| C   | 0            | 4          | 28              | 28             |

**Average Turnaround Time**: (20 + 24 + 28) / 3 = **24**

> *Instructor Insight*: "This is why FCFS isn't good for interactive systems—it's simple to implement but dependent on job order, and big tasks can starve small ones."

---

#### Shortest Job First (SJF)
**Concept**: Run the shortest job first.

##### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time |
|-----|--------------|------------|-----------------|----------------|
| B   | 0            | 4          | 4               | 4              |
| C   | 0            | 4          | 8               | 8              |
| A   | 0            | 20         | 28              | 28             |

**Average Turnaround Time**: (4 + 8 + 28) / 3 = **13.3**

> *Instructor Insight*: "SJF reduces average turnaround time, but it only works well if all jobs arrive simultaneously. If they don't, we need a more sophisticated approach."

---

#### Shortest Time to Completion First (STCF)
**Concept**: Preemptive version of SJF—interrupt the current process if a shorter job arrives.

##### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|-----------------|---------------|
| A   | 0            | 10         | 10              | 10              | 0             |
| B   | 2            | 10         | 18              | 16              | 8             |
| C   | 4            | 10         | 26              | 22              | 16            |

**Average Response Time**: (0 + 8 + 16) / 3 = **8**

> *Instructor Insight*: "STCF is preemptive, meaning it can interrupt a running process to run a shorter one. This gives better response times but requires more context switching."

---

#### Round Robin (RR)
**Concept**: Each job gets a fixed time slice before being preempted.

##### Example Calculation:
| Job | Arrival Time | Burst Time | Completion Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|-----------------|---------------|
| A   | 0            | 10         | 26              | 26              | 0             |
| B   | 2            | 10         | 26              | 24              | 0             |
| C   | 4            | 10         | 26              | 22              | 0             |

**Average Response Time**: (0 + 0 + 0) / 3 = **0**

> *Instructor Insight*: "Round Robin gives everyone a fair chance, but it can lead to long turnaround times if the time slice is too short. The key challenge is determining how often to switch between processes."

---

### Advanced Concepts: Beyond Basic Scheduling

#### Preemptive vs Non-Preemptive
| Type                | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| **Non-preemptive**  | Once a process starts running, it continues until completion (FCFS, SJF)     |
| **Preemptive**      | A running process can be interrupted to run another process (STCF, RR)       |

> *Instructor Insight*: "If you're doing an assignment and get distracted by a tutorial notification, that's preemptive scheduling. If you finish your large assignment before touching the tutorial, that's non-preemptive."

---

#### I/O Blocking: Why It Matters
Processes often alternate between CPU time and waiting for I/O (input/output).

##### Example:
- Process runs 2 seconds on CPU → waits 4 seconds for I/O → runs 2 more seconds

> *Instructor Insight*: "Don't waste the CPU! If a process is blocked waiting for I/O, we should run another process instead of idling."

**Visual Representation:**
```
Time:   0    1    2    3    4    5    6    7
CPU:     Task A  Task B  Task C
I/O:              Task A      Task B
```

> *Instructor Insight*: "Interactive jobs (like keyboard input) need quick response times, while batch jobs (like simulations) care more about total completion time."

---

#### Priority Systems

##### Fixed Priority Scheduling
- High-priority processes run first
- Low-priority processes may never get CPU time ("starvation")

> *Instructor Insight*: "Imagine a hospital where all the urgent cases are treated immediately, but non-urgent patients wait forever. This is why fixed priority scheduling isn't ideal."

##### Dynamic Priority Scheduling (MLFQ)
The key innovation: **priority decreases over time** as processes use more CPU.

###### MLFQ Rules:
1. Run high-priority jobs first
2. Start all new jobs at the highest priority
3. If a job runs for too long, drop its priority
4. Periodically reset priorities (prevent starvation)

> *Instructor Insight*: "The problem with dynamic priority is that if you only lose priority, eventually your process will be ignored forever."

---

#### The "Evil" Strategy: How to Exploit MLFQ
**Question**: How can a malicious program stay at the highest priority?

**Answer**: Run for just under the allotted time before being preempted.

> *Instructor Insight*: "If you know how long your time slice is (say 10ms), run for 9.9ms, then sleep briefly to reset your timer and get back to high priority."

**Visual Representation:**
```
Time:   0    1    ...    9    10    11
Priority: High → Medium → High (after reset)
```

> *Instructor Insight*: "This is why MLFQ systems need a way to 'refresh' priorities periodically—otherwise, long-running jobs get stuck in lower priority queues."

---

#### Multi-Level Feedback Queue (MLFQ) Implementation

##### Design Decisions:
- How many priority queues should exist?
- What's the time allotment for each queue?
- Are allotments equal across all queues?
- How often to refresh priorities?

> *Instructor Insight*: "These aren't mathematical questions with perfect answers. You experiment, test on real systems, and adjust based on observed performance."

**Visual Representation:**
```
Queue 0 (High Priority) → Runs first
Queue 1 (Medium Priority) → Runs if Queue 0 empty
Queue 2 (Low Priority)    → Runs if Queues 0 & 1 empty
```

---

#### Lottery Scheduling

**Concept**: Assign processes tickets proportional to priority. Randomly select a ticket for execution.

> *Instructor Insight*: "It sounds crazy, but lottery scheduling can be quite fair and effective. You could even have 'ticket trading' where processes exchange tickets."

##### Key Questions:
- How many tickets should high-priority jobs get?
- Can you trade tickets between processes?
- What happens if ticket values change over time?

> *Instructor Insight*: "This is a simple idea that works surprisingly well, though it's not used as commonly in modern systems."

---

#### Completely Fair Scheduler (CFS)

**Core Idea**: Each process gets an equal share of CPU time based on its weight.

##### Key Concepts:
- **vruntime**: Virtual runtime—how long the process has been running
- **Target Latency**: How often to switch processes (typically 6ms)
- **Weighting**: Adjusts how much CPU a process receives

> *Instructor Insight*: "CFS is used in Linux since 2007. It's called 'completely fair' because every process gets a share of the CPU based on its weight."

##### How CFS Works:
1. Tracks vruntime for each process
2. Runs processes with smallest vruntime first (highest priority)
3. Each process runs for about target latency period
4. Uses Red-Black tree to efficiently find next process

> *Instructor Insight*: "The magic of CFS is that it guarantees every process gets a minimum amount of CPU time, preventing starvation."

##### Example:
| Process        | Type                | vruntime (Low = High Priority) |
|----------------|---------------------|-------------------------------|
| Video Rendering| Long-running batch  | High                          |
| Word Processor | Interactive         | Low                           |

> *Instructor Insight*: "The word processor accumulates little vruntime. When it wakes up after user input, it gets run immediately."

---

#### The `nice` Command in Linux

**Concept**: Adjusts a process's priority (weight) for CFS.

| Nice Value | Priority Level | CPU Share |
|------------|----------------|-----------|
| -20        | Highest        | Most      |
| 0          | Default        | Medium    |
| +19        | Lowest         | Least     |

> *Instructor Insight*: "Changing a program's 'nice' value changes its weighting for CFS. Lower nice = more CPU time."

---

#### Multi-core Considerations

**Challenge**: How to fairly distribute processes across multiple cores.

> *Instructor Insight*: "Imagine waiting in line at an airport with 6 queues, but you pick the wrong one and have to wait while others get served quickly. That's the challenge of multi-core scheduling."

##### Current Approach:
- One Red-Black tree per core
- Processes can migrate between cores based on load

> *Instructor Insight*: "This is a complex problem with no perfect solution, but modern systems use sophisticated algorithms to balance workloads across cores."

---

### Summary & Key Takeaways

#### Scheduling Algorithms Comparison
| Algorithm          | Preemptive? | Best For                          | Major Drawback               |
|--------------------|-------------|-----------------------------------|------------------------------|
| **FCFS**           | No          | Simple batch processing           | Convoy effect, poor response time |
| **SJF**            | No (usually)| Batch jobs with known run times   | Doesn't work well if jobs arrive at different times |
| **STCF**           | Yes         | Interactive systems               | High context switching overhead |
| **Round Robin**    | Yes         | General-purpose, fair distribution | Long turnaround for long jobs  |
| **MLFQ**           | Yes (dynamic)| Mixed interactive/batch workloads | Complex implementation        |
| **CFS**            | Yes         | Linux systems                     | Requires sophisticated data structures |

#### Key Principles to Remember
1. **Scheduling is about trade-offs**: You can't optimize for all metrics simultaneously.
2. **Interactive vs Batch Jobs**: Need different scheduling strategies (response time vs throughput).
3. **Starvation Prevention**: Always design algorithms so no process gets ignored indefinitely.
4. **Real-world Complexity**: Actual systems don't follow the simple assumptions we use in examples.

> *Instructor Insight*: "The most important thing about scheduling isn't knowing all the algorithms—it's understanding why they exist and when to use them."

---

### Study Questions

1. What is the difference between a scheduler and a dispatcher?
2. Calculate turnaround time for FCFS with jobs A(0,5), B(0,3), C(0,7).
3. Explain the convoy effect using an example from university life.
4. Why does SJF require all jobs to arrive simultaneously in its basic form?
5. How would you modify STCF if a job arrives while another is running?
6. What are the advantages and disadvantages of Round Robin scheduling?
7. Describe how I/O blocking affects process scheduling decisions.
8. Explain why fixed priority scheduling can lead to starvation.
9. How does MLFQ prevent long-running jobs from being starved?
10. What is vruntime in CFS, and how does it affect scheduling decisions?
11. Why might a malicious program try to "sleep" just before its time slice ends?
12. Explain the difference between response time and turnaround time.
13. How do Red-Black trees improve performance in CFS compared to other data structures?
14. What is the purpose of the `nice` command in Linux scheduling?
15. Describe a situation where FCFS would be preferable to SJF.

---

### Practical Exercise

**Scenario**: You're designing a scheduler for a hospital's operating room system with these requirements:
- 3 surgeons (A, B, C) available at different times
- Each surgery has an estimated duration
- Some surgeries require special equipment that takes time to prepare

Create a Gantt chart showing how you would schedule the following operations:

| Surgery | Arrival Time | Duration |
|---------|--------------|----------|
| A       | 0            | 120 min  |
| B       | 30           | 60 min   |
| C       | 45           | 90 min   |

Calculate:
- Turnaround time for each surgery
- Average turnaround time
- Response time for each surgery

*Hint: Consider how I/O blocking (equipment preparation) would affect scheduling.*


---

# Comprehensive Study Notes: CPU Scheduling (Week 2)

<think>
We are creating comprehensive study notes based on the provided textbook slides (Week 2: Textbook CPU Sched). We must cover ALL content from Slide 1 to Slide 13.

Let's break down each slide and extract all concepts, theories, principles, examples, case studies, definitions, formulas, etc.

Slide 1:
- Introduction to scheduling
- Origins of scheduling (from operations management)
- The problem: How to develop a basic framework for thinking about scheduling policies?
- Key questions: key assumptions? metrics? approaches?

Slide 2:
- Workload Assumptions (simplifying assumptions):
  1. Each job runs for the same amount of time.
  2. All jobs arrive at the same time.
  3. Once started, each job runs to completion.
  4. All jobs only use the CPU (no I/O).
  5. The run-time of each job is known.

- Scheduling Metrics:
  - Turnaround Time: T_turnaround = T_completion - T_arrival
    Since all arrive at same time (T_arrival=0), then T_turnaround = T_completion.
  - Fairness (measured by Jain's Fairness Index) and the trade-off with performance.

- FIFO (First In, First Out / First Come, First Served):
  - Simple to implement.
  - Example: Three jobs A,B,C each run for 10 seconds -> average turnaround = 20s.
  - Problem when job lengths differ: convoy effect (long job blocks short ones).

Slide 3:
- Convoy Effect example: 
  - Jobs: A=100s, B=10s, C=10s
  - FIFO: A runs first -> completes at 100, then B at 110, C at 120 -> average = (100+110+120)/3 = 110s.
- Shortest Job First (SJF) principle:
  - Run the shortest job first.

Slide 4:
- SJF example with same jobs: 
  - B and C run before A -> completes at 10, then 20, then 120? Actually, let's recalc: 
    If we have three jobs: A=100s, B=10s, C=10s.
    SJF runs B (10), then C (10), then A (100) -> completion times: 10,20,120 -> average = (10+20+120)/3 = 50s.

- Note: The text says "more than a factor of two improvement" from 110 to 50.

Slide 5:
- Preemptive vs Non-preemptive schedulers.
  - Non-preemptive: run job to completion (like SJF).
  - Modern schedulers are preemptive: can stop one process and start another via context switch.
- Problem with non-preemptive SJF when jobs arrive at different times.

Slide 6:
- STCF (Shortest Time-to-Completion First) / Preemptive Shortest Job First (PSJF):
  - When a new job arrives, the scheduler checks which remaining job has the least time left and runs it.
  - Example: 
      A arrives at t=0 (100s), B and C arrive at t=10 (each 10s).
      Without preemption: STCF would run A until completion? But actually, with non-preemptive SJF we have the convoy problem. With preemptive:
        At t=0: only A -> runs.
        At t=10: B and C arrive. Now remaining time of A is 90s (if it ran for 10 seconds), but B and C are 10s each. So STCF preempts A to run B or C.
      Figure 7.5 shows the schedule.

- Turnaround times in example: 
    For jobs arriving at t=0, with B,C arriving at t=10:
        Job A: arrives at 0, completes at 120 -> turnaround = 120
        Job B: arrives at 10, first run at 10? Actually, the figure shows that from time 10 to 20 is job B (so it runs for 10 seconds) and then C. 
        So:
          A starts at 0, but gets preempted at t=10 -> so it doesn't complete until after B and C finish? Actually, the figure shows that from time 10 to 20: job B (so first run of B is at 10), then 20-30: job C. Then A runs from 30 to 130? But wait, the text says "only when they are finished would A’s remaining time be scheduled". 
        The figure caption says: TimeABC A[B,C arrive] -> meaning at t=10 (when B and C arrive), we switch.

      Actually, let's read the example again:
          Figure 7.5: STCF Simple Example
          It shows: from 0-10: A; then 10-20: B; 20-30: C; then 30-130: A? But that would be a long time for A.

      The text says: "STCF would preempt A and run B and C to completion; only when they are finished would A’s remaining time be scheduled."

      So:
        t=0 to 10: A runs (for 10 seconds) -> then at t=10, B and C arrive. Now the scheduler sees that B and C have less time left than A's remaining (90s vs 10s each). So it switches to run B for 10 seconds (t=10-20), then runs C from 20-30.
        Then at t=30, only A remains. It has been preempted and had 90 seconds of work left? Actually, we ran A for 10s so it has 90s remaining -> runs until 120 (t=30 to 120).

      Therefore:
        Completion times: 
          B completes at 20
          C completes at 30
          A completes at 120

      Turnaround for each:
          A: 120 - 0 = 120
          B: 20 - 10 = 10
          C: 30 - 10 = 20
        Average turnaround time = (120+10+20)/3 = 150/3 = 50.

      The text says: "average turnaround time: 50 seconds ((120−0)+(20−10)+(30 −10))/3" -> which is the same as above (but note: they wrote it as (120-0) for A, but actually we have to subtract arrival times).

      However, in the text: "average turnaround time: 50 seconds ((120−0)+(20−10)+(30 −10))/3" -> that's correct.

Slide 7:
- Response Time (new metric):
  - Defined as T_response = T_first_run - T_arrival.
  - Example from Figure 7.5: 
        A arrives at 0, first run at 0 -> response time=0
        B arrives at 10, first run at 10 -> response time=0? But the text says "for job A:0, for B:0, and for C:10" (average:3.33).
      Why is C's response time 10?
          Because in Figure 7.5, we see that from t=0 to 10: A runs; then at t=10, the scheduler switches to B? But wait, when does C get its first run?

      Actually, the example says "B and C arrive at time 10", so they are both in the ready queue. The STCF will choose one of them (say B) for the next slot. So:
          Job A: arrives at 0 -> runs immediately? But note: it's preempted by the arrival of B and C, but actually we run A until t=10 when they arrive.
          Then from t=10 to 20: job B (so first run for B is at 10) -> response time = 10-10=0? 
          But what about C? It arrives at 10, but the scheduler runs B next. So when does C get its turn?
          The figure shows that from t=20 to 30: job C is running (so first run of C starts at 20). Therefore:
            Response time for C = 20 - 10 = 10.

      Hence, the text says: "response time of each job: A:0, B:0, C:10" -> average= (0+0+10)/3 ≈ 3.33.

- STCF is bad for response time because a new short job that arrives late might have to wait until the current long job finishes? Actually, in this case it didn't wait too much but note: if there are many jobs arriving at different times, the last one might get delayed by multiple preemptions.

Slide 8:
- Round Robin (RR):
  - Runs each job for a fixed time slice (quantum) and then switches to next.
  - Example: three jobs A,B,C each run for 5 seconds. Time slice=1 second.
      SJF would run one after another -> completion times: A at 5, B at 10, C at 15? But wait, no: in the example they say:
          Figure 7.6 (SJF): 
            A runs to completion (at 5), then B (to 10), then C (to 15) -> so response times: A=0, B=5, C=10.
          RR with time slice=1s:
            Time 0-1: A
            Time 1-2: B
            Time 2-3: C
            Then repeats until done? Actually, each job needs 5 seconds -> so it will take 15 units (each cycle of all three takes 3 seconds).
            So completion times:
                A completes at time 1+4*3 = 13? 
                Let's break down:
                  Cycle 0: [A,B,C] from t=0 to 3
                  Cycle 1: [A,B,C] from t=3 to 6
                  Cycle 2: [A,B,C] from t=6 to 9
                  Cycle 3: [A,B,C] from t=9 to 12 -> but at the end of cycle 3, A has run for 4 seconds (in cycles 0-3) and needs one more second? Actually, each job runs in a time slice.

            The figure shows "ABCABCABC" meaning:
                Time: 0-1: A; 1-2:B; 2-3:C; 
                3-4:A; 4-5:B; 5-6:C;
                ... until the last one.
                Since each job needs 5 seconds, it will take 5 cycles (each cycle has three jobs) but note: at the end of a time slice for a job that completes early? Actually, we run exactly 1 second per slot.

            So:
                A runs at t=0-1, then 3-4, then 6-7, then 9-10 -> so it finishes at 10 (because after the last run from 9 to 10, it has completed its 5 seconds? Actually, no: each time slice is one second and we need five slices for A. So:
                  t=0-1: first slice of A
                  t=3-4: second slice
                  t=6-7: third
                  t=9-10: fourth -> but wait, it needs 5? Actually, the example says each job runs for 5 seconds and time slice is 1s. So:
                    A will run at times [0,3,6,9] (each one second) and then completes at 10.

                Similarly, B: 
                  t=1-2,4-5,7-8 -> but wait, it needs 5 seconds? Actually, the figure shows "ABC" repeated until done. So:
                    A runs for 5 slices: so from time 0 to 1 (first), then next at 3,6,9 -> that's four slices and we need one more? 
                The text says in Figure 7.7: ABCABCABCABCABC -> meaning it cycles until all jobs are done.

            Actually, the figure shows:
                Time A B C
                So for job A: runs at time intervals [0-1], [3-4], [6-7], [9-10] -> that's 4 seconds? But we need 5. 
                The text says "each with running times of 5 seconds", so they must have run the last slice for each job in a way that completes.

            Let me recast: 
                Total time = (number of jobs * quantum) but actually it's more complex because when a job finishes, we don't schedule it again.

            The example says:
                In RR with 1s time slice: A finishes at 13? Actually the text says "A finishes at 13", meaning from t=0 to 13 (so 13 seconds total).

            How?
              Time 0-1: A
              Time 1-2: B
              Time 2-3: C
              Time 3-4: A -> now A has run for 2s, needs 3 more.
              Time 4-5: B -> runs again (now total 2s)
              Time 5-6: C -> runs again (total 2s)
              Time 6-7: A
              Time 7-8: B
              Time 8-9: C
              Time 9-10: A -> now A has run for 4 seconds? Then at time 10, it's the next turn but we have to see if it completes.

            Actually, after 3 cycles (each cycle of 3s) they've each run 3 times (so 3s). Now:
              Time 9-10: A -> now A has completed? No, because it needs 5 seconds. So at time 10, we have to schedule the next job.

            The text says in Figure 7.7: "ABCABCABCABCABC" meaning that after the first cycle (3s) they do another and so on until all jobs are done. Since each job requires 5 slices of 1 second, it will take 5 cycles? But note: when a job completes early, we don't schedule it again.

            Actually, for A:
              It runs at time intervals [0-1], [3-4], [6-7], [9-10] -> that's 4 seconds. Then the next slot is B (at t=10-11), then C (t=11-12). Now it's A again? But wait, at time 10, after running for one second in the last slice, does A complete? The text says "each with running times of 5 seconds", so we assume that when a job has run its full required time, it is done.

            How do they get completion at 13?
              Let me count:
                t=0-1: A (A needs 4 more)
                t=1-2: B
                t=2-3: C
                t=3-4: A (now A has run for 2s, needs 3 more) -> but wait, we have to count the time slices.

            Actually, a better way: 
              Each job requires 5 units of CPU. The scheduler runs each in turn for one unit until they complete.
              So:
                Job A completes after its 5th slice (which is at the end of the 14th second?).

            But note: the figure shows "ABC" repeated and then says "A finishes at 13". How?

            The text states: 
                In Figure 7.7, it's shown as a long string of ABCs.

            And in Slide 8: "A finishes at 13, B at 14, C at 15"

            So:
              A runs for the first time from t=0 to 1 -> then again at t=3-4 (so second run), t=6-7 (third), t=9-10 (fourth). Then it needs one more slice. The next available slot is after B and C have been scheduled? 
              After the fourth run of A, we are at time 10. Now:
                Time 10-11: B
                Time 11-12: C
                Time 12-13: A -> completes.

            So completion times:
                A: 13 (from t=12 to 13)
                B: after its fifth slice? 
                  B runs at time 1-2,4-5,7-8,10-11 -> that's four slices. Then the next slot for B would be at time 13-14? But wait, A is running from 12 to 13 and then completes. So after A finishes (at t=13), we run B: 
                  Time 13-14: B -> so B completes at 14.
                C runs at time 2-3,5-6,8-9,11-12 -> four slices? Then next slot is after A and then B? Actually:
                  After t=10 (when we ran B from 10-11), then C from 11-12. Then at time 12: A runs to completion (so ends at 13). 
                  Now the ready queue has only B and C, but note that after running for four slices each, they still need one more slice? Actually:
                    Each job needs five slices.

                So for C: it ran at t=2-3,5-6,8-9,11-12 -> that's four. Then the next slot is A (which we did from 12-13). Now after A finishes, we run B? But wait, the scheduler cycles through all ready jobs.

                Actually, at time 10: 
                  Ready queue: [A,B,C] but note that when a job runs for one slice and hasn't completed, it goes back to the end of the queue.
                So after t=9-10 (which was A), we have:
                  At t=10: B is next -> so time 10-11: B
                  Then at t=11: C -> 11-12: C
                  Then at t=12: A again? But wait, after the first run of A (t=0-1), it went back to queue. So when we get to the end of the cycle, we start over.

                Therefore:
                  Time 0-1: A -> then goes to end of queue
                  Queue becomes [B,C,A]
                  Then time 1-2: B -> queue [C,A,B]
                  Time 2-3: C -> queue [A,B,C]
                  ... and so on.

                So the order is cyclic. Therefore, for job A:
                  It runs at times: 
                    t=0 (first run), then after two others: t=3,6,9,12.
                  At time 12-13: it gets its fifth slice -> completes at 13.

                Similarly, B:
                  Runs at t=1,4,7,10,13? But wait, at time 13 we are running A until completion (so from 12 to 13) and then the next job is B. So for B: 
                    First run: t=1-2
                    Second: t=4-5
                    Third: t=7-8
                    Fourth: t=10-11
                    Fifth: t=13-14 -> completes at 14.

                C:
                  Runs at t=2,5,8,11, and then next would be at time 14? But note after B runs from 13 to 14, we run C for the fifth slice (t=14-15) -> completes at 15.

            Hence: A finishes at 13, B at 14, C at 15. Average turnaround = (13+14+15)/3 = 14.

- Trade-off between response time and turnaround:
    - RR is good for response time because jobs get a chance to run quickly.
        Response times: 
          A: first run at t=0 -> response time=0
          B: first run at t=1 -> response time=1 (if arrival at 0) but note the example says they all arrive at same time? So T_arrival=0 for all.
          C: first run at t=2 -> response time=2.
        Average = (0+1+2)/3 = 1.

    - But bad for turnaround because jobs are stretched out. 
        Turnaround times: A=13, B=14, C=15 -> average=14.

- The text says: "RR is one of the worst policies if turnaround time is our metric."

Slide 9:
- Incorporating I/O (relaxing assumption 4):
  - When a job issues an I/O request, it blocks and waits. Scheduler should run another job during that wait.
  - Example with two jobs A and B:
        Job A: runs for 10ms then does I/O (takes 10ms), repeats until done? Actually the example says:
          "A runs for 10 ms and then issues an I/O request (assume here that I/Os each take 10 ms)"
          So it has two CPU bursts of 10ms, separated by 10ms I/O.
        Job B: uses CPU for 50ms without interruption.

      Without considering I/O:
          Figure 7.8: 
            A runs first (for the first 10ms) -> then does I/O while B is running? But in this figure, it shows:
              Time 0-10: A on CPU
              Then from time 10 to 20: A doing I/O and B using CPU for 10ms? Actually, the figure has two rows: 
                CPU row: [A,A] (meaning at times 0-10 and then what?) but actually it's a timeline.

          The text says:
            Figure 7.8: Poor Use Of Resources
              Time line: A runs for first burst (0-10), then B starts running from 10 to 60? But wait, the figure shows "A A" on CPU and "B BBBB" meaning that during the I/O of A (which is at time 10-20) we run B. However, note:
                At t=0: start A -> runs until t=10.
                Then from t=10 to 20: A does I/O and B uses CPU? But then why would the figure show "B BBBB" meaning four blocks of 10ms for B (so 40ms) but we only have two bursts?

          Actually, let's read:
            Job A needs total 50ms CPU time -> so it has five 10ms bursts? But the example says: 
                "A runs for 10 ms and then issues an I/O request (assume here that I/Os each take 10 ms), whereas B simply uses the CPU for 50 ms"

            So A will have:
              Burst 1: t=0-10 -> then I/O from 10-20
              Then burst 2: t=20-30 -> I/O 30-40
              ... until five bursts.

          But the figure shows only two bursts for A? Actually, it says "A runs for 10 ms and then issues an I/O request" meaning one cycle. However, we have to assume that after the first burst, there are more.

          The text states: 
            Figure 7.8: Poor Use Of Resources -> shows:
              CPU: A (for two blocks? but it says "A A", so two bursts) and then B for four blocks.
              Disk: I/O for A during its waiting time? But the disk row has "A" at times when A is doing I/O.

          Actually, Figure 7.8 shows:
            Time line from 0 to 140 (in steps of 20).
            CPU: 
                [0-20]: two blocks for A -> meaning it ran twice? But actually the first burst was at 0-10 and then we see a second block at 20-30? The figure has "A" in time intervals that are not contiguous.

          Let me interpret Figure 7.8:
            It says: 
              Time A A A A A B BBBB
              But the timeline is labeled from 0 to 140, and each block represents a unit of time (say 20ms?).

          Actually, it's better to read as:

            CPU row: 
                From 0-20: "A" meaning that during this period A was running for two units? But the text says we have blocks.

          The figure caption says:
            Figure 7.8: Poor Use Of Resources
              Time line with events at multiples of 20ms, and each block is a time unit (so one block = 20ms).

          So in Figure 7.8:
            CPU row: 
                First two blocks (0-40) are for A? But that would be 40ms -> but we know A only runs 10ms at a time.

          Actually, the figure is simplified to show the pattern:

            Without I/O consideration (STCF without treating bursts as independent):
              The scheduler sees two jobs: 
                Job A has total CPU demand of 50ms? But actually it's broken into five 10ms bursts. However, if we don't break it up, STCF would see:
                  At time 0: choose between a job that needs 50ms (B) and one that is "a single burst" but the scheduler doesn't know about I/O -> so it might treat A as having to run for 50ms? But actually we don't have that information.

          The text says:
            Without considering I/O, STCF would choose between a job with total CPU time of 50ms (B) and one that is "a single burst" but the scheduler doesn't know about the breaks. So it might run B first? 

          Actually, in Figure 7.8: 
            It shows A running for two blocks (so 2*20=40ms?) then B runs for four blocks (80ms). But that's not matching.

          Let me read the text again:
            "Assume we are trying to build a STCF scheduler. How should such a scheduler account for the fact that A is broken up into 5 10-ms sub-jobs, whereas B is just a single 50-ms CPU demand?"

          Then it says: 
            Figure 7.8 shows poor use (A runs first then B) and Figure 7.9 shows overlap.

          In Figure 7.8:
            The timeline has for A: two blocks of "A" in the CPU row? Actually, the figure is drawn with time intervals as bars.

          We'll assume that each bar represents a fixed unit (say 10ms) and we have:

            Time [0-10]: A runs
            Then I/O for A from 10-20.
            But in Figure 7.8, the CPU row shows "A" at time intervals: 
                First block: 0-20? (but that's two units) -> no.

          The figure has a timeline with marks every 20ms and each bar is labeled by which job was running on CPU during that interval.

          So:
            Time 0-20: A runs for the first burst? But actually, it should run only 10ms. However, in this poor example, they are not breaking up the bursts so the scheduler doesn't know to switch after 10ms.

          The text says: "Clearly, just running one job and then the other without considering how to take I/O into account makes little sense."

          So Figure 7.8 is a bad schedule because:
            A runs for its first burst (say from t=0-10) but then instead of switching during I/O, it waits until after I/O? Actually, no: the scheduler should switch when A blocks.

          How it's drawn in Figure 7.8:
            CPU row: 
                [0-20]: "A" -> meaning that for two time units (say 20ms) A is running on CPU? But actually we know A only runs 10ms and then does I/O.
            Then from t=20 to 60: B runs continuously.

          This means the scheduler did not switch during A's I/O, so it wasted time when A was blocked (waiting for disk) but the CPU was idle? Actually no: in Figure 7.8, while A is doing I/O (which should be from t=10 to 20), we see that B runs on the CPU from t=10 to 60? But wait, the figure has a row for disk too.

          The text says:
            "Figure 7.8: Poor Use Of Resources
             Time A A A A A B BBBB
             CPU
             Disk"

          And Figure 7.9 shows better overlap.

          So in Figure 7.8:
            For the first time unit (0-20): 
                On CPU, job A is running -> but actually it should run only for 10ms and then block? But they are not accounting for that.
            Then from t=20 to 60: B runs on CPU.

          This means:
            From t=0 to 10: A runs (on CPU) 
            At t=10, A issues I/O -> but the scheduler doesn't switch because it's still in a "run" state? Actually, when an I/O request is issued, the process blocks and moves to blocked queue. Then the scheduler would run B.

          However, Figure 7.8 shows that during the time when A was doing I/O (which should be from t=10 to 20), we are not using the CPU for anything? But wait, in reality, while A is waiting for disk, the CPU could run other jobs. So why isn't B running?

          The figure has a row "Disk" that shows:
            Disk: 
                [0-10]: ? (but it's labeled with an 'A' meaning A was using disk? but actually at t=0-10 A is on CPU, so not doing I/O)
                Then from 10-20: the disk row has "A", meaning that during this time A is waiting for its I/O to complete.

          And in Figure 7.8, the CPU row shows no job running? But it says "B BBBB" starting at t=20? So actually:
            From t=10-20: disk busy with A's I/O -> so CPU could run B but they didn't (because of poor scheduling) and then from 20 to 60, B runs.

          This is inefficient because the CPU was idle during [10-20]? But wait, no: in reality, when a process blocks on I/O, the scheduler immediately switches. So Figure 7.8 represents a bad schedule where we didn't switch and let the CPU sit idle? Or it's just that they are not showing B running during A's I/O.

          The text says "Poor Use", so likely they mean: 
            They ran A for its first burst (0-10), then instead of switching to run another job while waiting for disk, they let the CPU sit idle until t=20? Then started B at 20. But that's not what happens in reality.

          Actually, Figure 7.9 shows:
            Time A A A A A B B B B B
            CPU: 
                [0-10]: A (first burst)
                [10-20]: ? -> but the figure has "A" for two blocks? 

          Let me read the text about Figure 7.9:
            "Figure 7.9: Overlap Allows Better Use Of Resources"

          And it says: 
            "Doing so allows for overlap, with the CPU being used by one process while waiting for the I/O of another process to complete; the system is thus better utilized."

          So in Figure 7.9:
            During A's first burst (0-10), then from 10-20: B runs on CPU? But wait, at t=10, A issues an I/O and blocks -> so scheduler switches to run B for the next time slice.

          Therefore, in Figure 7.9:
            Time 0-10: A (CPU)
            Time 10-20: B (CPU)   [while A is waiting on disk]
            Then at t=20, A's I/O completes -> so it moves to ready queue.
            Now the scheduler can run either A or B? But note that A has a new burst of CPU time.

          The figure shows:
            CPU: 
                0-10: A
                10-20: B   [but then why is there an "A" at 20?] 

          Actually, the timeline in Figure 7.9 says for CPU: "A A A A A B B B B B", meaning that each letter represents a time unit (say 10ms). So:
            Time 0-10: A
            Time 10-20: A? -> but wait, no: the text says they are showing two jobs.

          I think we should trust the description and not get bogged down in the figure. The key point is that by treating each CPU burst as a separate job (so when an I/O completes, it becomes available for scheduling), the scheduler can run other processes during the waiting time of one process.

Slide 10:
- No More Oracle: 
  - We cannot assume we know the length of jobs in advance.
  - How to build a scheduler that behaves like SJF/STCF without knowing job lengths?
    -> Use recent past (history) to predict future behavior.
  - Also, incorporate RR-like fairness for response time.

Slide 11:
- Summary: 
  Two families:
    Family 1: Run the shortest remaining job first (SJF, STCF) -> optimizes turnaround time but bad for response time.
    Family 2: Round Robin (RR) -> alternates between jobs to optimize response time but bad for turnaround.

  Trade-off: fairness vs. performance.

Slide 12:
- References

Slide 13:
- Homework questions and simulation program.

Now, we must create comprehensive study notes covering all of this.

We'll structure the notes as follows:


### I. Introduction to Scheduling
#### A. The Problem Statement
[Explain why scheduling is important]

#### B. Origins and Context
[How it relates to operations management, assembly lines, etc.]

### II. Workload Assumptions
#### A. List of Simplifying Assumptions (with explanations)
1. Each job runs for the same amount of time.
2. All jobs arrive at the same time.
3. Once started, each job runs to completion.
4. All jobs only use the CPU (no I/O).
5. The run-time of each job is known.

#### B. Critique of Assumptions
[Why they are unrealistic and how we relax them]

### III. Scheduling Metrics
#### A. Turnaround Time
- Definition: T_turnaround = T_completion - T_arrival
  (with the assumption that all jobs arrive at time 0, then T_turnaround = T_completion)
- Example calculation

#### B. Response Time
- Definition: T_response = T_first_run - T_arrival
- Why it matters for interactive systems.

#### C. Fairness and Trade-offs
- Jain's Fairness Index (briefly mentioned as a metric).
- The trade-off between performance (turnaround time) and fairness/response time.

### IV. Scheduling Algorithms

#### A. First In, First Out (FIFO)
##### 1. Description
[Simplest algorithm: run jobs in the order they arrive]

##### 2. Example with equal job lengths
- Three jobs of 10s each -> average turnaround = 20s.

##### 3. Problem when job lengths differ: Convoy Effect
- Definition and example (A=100, B=10, C=10) leading to high average turnaround time (110s).

#### B. Shortest Job First (SJF)
##### 1. Description
[Run the shortest job first]

##### 2. Example with different lengths
- Jobs: A=100, B=10, C=10 -> SJF runs B then C then A.
- Turnaround times: B=10, C=20, A=120 -> average = (10+20+120)/3 = 50s.

##### 3. Optimality
[Under the given assumptions, SJF minimizes average turnaround time]

#### C. Shortest Time-to-Completion First (STCF) / Preemptive SJF
##### 1. Description
[Preempt a running job if a shorter one arrives; always run the remaining shortest job.]

##### 2. Example with late arrivals
- A arrives at t=0 (100s), B and C arrive at t=10 (each 10s).
- Without preemption: FIFO would have convoy effect -> average turnaround = 103.33.
- With STCF:
    - Runs A until t=10, then switches to run B (t=10-20) and C (t=20-30), then runs A from t=30-130? 
      Actually: completion times: A at 120, B at 20, C at 30 -> average = (120+10+20)/3 = 50s.

##### 3. Optimality
[STCF is optimal for minimizing average turnaround time when jobs arrive at arbitrary times and can be preempted.]

#### D. Round Robin (RR)
##### 1. Description
- Run each job for a fixed time slice (quantum), then switch to next.
- Time slice must be multiple of timer interrupt period.

##### 2. Example with equal job lengths
- Three jobs, each 5s, quantum=1s:
    - Completion times: A at 13, B at 14, C at 15 -> average turnaround = 14.
    - Response times (first run): A=0, B=1, C=2 -> average response time = 1.

##### 3. Trade-offs
- Shorter quantum improves response time but increases context switch overhead.
- Longer quantum reduces context switches but hurts responsiveness.

#### E. The Role of Context Switching Cost
[How the cost of switching (context-switch) affects performance, especially for RR]

### V. Incorporating I/O Operations

#### A. Why I/O Matters
- When a job issues an I/O request, it blocks and waits.
- Scheduler should run another job during this time.

#### B. Treating CPU Bursts as Independent Jobs
##### 1. Example with two jobs:
   - Job A: alternates between 10ms CPU bursts and 10ms disk wait (so total for one cycle: 20ms).
   - Job B: runs continuously for 50ms.

##### 2. Poor scheduling without I/O awareness:
   - Figure 7.8: Runs A until it blocks, then waits idle? Actually, the figure shows inefficient use of CPU during disk wait (B starts late).

##### 3. Good scheduling with I/O awareness:
   - When A issues an I/O at t=10, scheduler runs B for the next time slice.
   - Then when A's I/O completes (at t=20), it gets a chance to run again.

### VI. The Oracle Problem: Unknown Job Lengths

#### A. Why Knowing Job Length is Unrealistic
- In general-purpose OSes, we don't know job lengths in advance.

#### B. Solution Approach
- Use recent history to predict future behavior.
- Build on RR for good response time and use prediction (like multi-level feedback queue) for turnaround.

### VII. Summary of Scheduling Families

| Algorithm Family | Goal          | Metric Optimized | Response Time | Turnaround Time |
|------------------|---------------|------------------|---------------|----------------|
| SJF/STCF         | Shortest job  | Minimize average turnaround time | Poor (long waits for short jobs arriving late) | Excellent      |
| RR               | Round-robin   | Minimize response time           | Good          | Poor           |

### VIII. Key Trade-offs and Principles

#### A. The Fundamental Trade-off
- **Fairness vs. Performance**: 
  - Being fair (evenly dividing CPU among active processes on a small scale) leads to poor turnaround time.
  - Optimizing for short jobs (unfair) improves turnaround but hurts response time.

#### B. Overlap Principle
- When possible, overlap operations (e.g., while one job is waiting for I/O, run another) to maximize utilization.

#### C. Amortization of Costs
- Context switch cost can be amortized by increasing the quantum: 
  - Example: context-switch cost = 1ms per switch; with time slice=10ms -> 10% overhead.
  - With time slice=100ms -> ~1% overhead.

### IX. Study Questions (15-20)

We'll create a set of questions that cover all the material, including examples and concepts.

Now, let's write out the notes in markdown with the above structure.
</think>

# Comprehensive Study Notes: CPU Scheduling  
*Based on Textbook Chapter "Week 2: Cpu Sched"*

---

### I. Introduction to Scheduling
#### A. Core Problem Statement
> **THE CRUX**: *How should we develop a basic framework for thinking about scheduling policies? What are the key assumptions? What metrics are important? What fundamental approaches have been used in early computer systems?*  

- **Origins**: Scheduling principles originated from operations management (e.g., assembly lines), emphasizing efficiency.
- **Key Insight**: Human endeavors like manufacturing and computing share core concerns: *resource utilization*, *efficiency*, and *fairness*.  
  > *"Assembl[y] lines... require scheduling, and many of the same concerns exist therein."*

---

### II. Workload Assumptions (Simplifying Conditions)
*All assumptions are unrealistic but serve as a starting point for analysis.*

| **Assumption** | **Description**                                                                 | **Realism Check** |
|----------------|---------------------------------------------------------------------------------|-------------------|
| 1              | Each job runs for the same amount of time.                                      | ❌ Highly Unrealistic |
| 2              | All jobs arrive at the same time.                                               | ❌ Unlikely in practice |
| 3              | Once started, each job runs to completion (non-preemptive).                     | ⚠️ Partially Realistic (batch systems) |
| 4              | All jobs only use CPU (no I/O operations).                                      | ❌ Highly Unrealistic |
| 5              | Run-time of each job is known *a priori* ("oracle" assumption).                 | ❌ **Worst Assumption** |

> 💡 **Critical Note**:  
> *"The run-time of each job is known: this would make the scheduler omniscient... not likely to happen anytime soon."*

---

### III. Scheduling Metrics
#### A. Turnaround Time (Primary Metric)
- **Definition**:  
  \[
  T_{\text{turnaround}} = T_{\text{completion}} - T_{\text{arrival}}
  \]
- *Simplified under Assumption 2*: \(T_{\text{arrival}} = 0\) → \(T_{\text{turnaround}} = T_{\text{completion}}\).
- **Goal**: Minimize average turnaround time (e.g., for batch processing).

#### B. Response Time
- **Definition**:  
  \[
  T_{\text{response}} = T_{\text{first\_run}} - T_{\text{arrival}}
  \]
- *Best-case assumption*: Job produces response instantly upon first run.
- **Goal**: Minimize average response time (e.g., for interactive systems).

#### C. Fairness
- Measured by **Jain’s Fairness Index** [J91].
- **Trade-off with Performance**:  
  > *"Performance and fairness are often at odds... a scheduler may optimize performance but prevent some jobs from running, decreasing fairness."*

---

### IV. Scheduling Algorithms & Analysis

#### A. First In, First Out (FIFO) / FCFS
##### *Description*
- Simplest algorithm: Run jobs in arrival order.
- **Non-preemptive**: Runs job to completion before switching.

##### *Example 1: Equal Job Lengths*  
| Job | Arrival Time | Runtime | Completion Time |
|-----|--------------|---------|----------------|
| A   | 0            | 10      | 10             |
| B   | 0            | 10      | 20             |
| C   | 0            | 10      | 30             |

- **Average Turnaround Time**: \(\frac{10 + 20 + 30}{3} = 20\) seconds.

##### *Example 2: Unequal Job Lengths (Convoy Effect)*  
| Job | Arrival Time | Runtime |
|-----|--------------|---------|
| A   | 0            | 100     |
| B   | 0            | 10      |
| C   | 0            | 10      |

- **FIFO Schedule**:  
  - A runs (0–100), then B (100–110), then C (110–120).  
  - *Turnaround*: \(A=10


---

# Hierarchical Headers

<think>
We are creating comprehensive study notes based on the provided textbook slides about "Scheduling: Proportional Share" (Week 2). The goal is to cover every concept, theory, example, and principle from all slides.

Let's break down each slide and extract:

Slide 1:
- Introduction to proportional-share scheduling (also called fair-share scheduler)
- Basic idea: guarantee a certain percentage of CPU time for each job.
- Example: Waldspurger and Weihl [WW94] - lottery scheduling
- CRUX question: How to share the CPU proportionally?

Slide 2:
- Tickets represent share. Percent of tickets = percent of resource.
- Example with A (75 tickets) and B (25 tickets): should get 75% and 25% respectively.
- Lottery: every time slice, pick a random ticket from total (100 in example). If A has tickets 0-74 and B 75-99, then winning number determines which runs.
- Example output of lottery winners: [63,85,...] leading to schedule: A repeated 15 times, then B four times -> 20% for B (desired 25%, but probabilistic).
- TIP on randomness: advantages - avoids corner cases, lightweight (minimal state), fast.

Slide 3:
- Ticket mechanism is powerful and can be applied broadly (e.g., memory in hypervisors [W02]).
- Ticket mechanisms:
   a. Ticket currency: user allocates tickets to their own jobs in their own currency; system converts to global.
      Example: User A gives each of two jobs 500 tickets (in A's currency) -> converted to 50 each globally. User B gives job 10 tickets (B's currency) -> converted to 100 globally. Total global = 200.
   b. Ticket transfer: process can hand off tickets temporarily (e.g., client passing to server for faster work).
   c. Ticket inflation: temporarily raise/lower ticket count (in trusted environments).

Slide 4:
- Implementation of lottery scheduling code (Figure 9.1):
   - Traverse a list of processes, accumulating the number of tickets until exceeding a random winner.
   - Example with A(100), B(50), C(250) -> total=400; if winner=300, then:
        counter starts at 0
        add A: counter=100 (less than 300)
        add B: counter=150 (less than 300)
        add C: counter=400 (>300) -> choose C.
   - Efficiency tip: sort the list by tickets descending to minimize iterations.

Slide 5:
- Example study on fairness (Figure 9.2):
   - Fairness metric F = time_first_job_finish / time_second_job_finish
   - Goal: F=1 for perfect fairness.
   - Graph shows that as job length R increases, average fairness approaches 1 (i.e., more fair over longer runs).

Slide 6:
- How to assign tickets? Open problem. User allocation is not a solution because it doesn't tell you what to do.
- Stride scheduling: deterministic alternative by Waldspurger [W95].
   - Each job has a stride = large number / tickets (e.g., 10,000/tickets).
   - Example: A(100), B(50), C(250) -> strides: A=100, B=200, C=40.
   - Each job has a pass value. When run, increment by stride.
   - Scheduler picks the process with lowest pass value to run next (pseudocode provided).
   - Example trace in Figure 9.3: shows that over time, runs are proportional to tickets.

Slide 7:
- Comparison of lottery vs stride:
   - Lottery: simple implementation, no global state per process -> easy to add new processes.
   - Stride: exact proportions at the end of each cycle (deterministic), but requires setting initial pass for new jobs appropriately.
- Linux CFS introduced as current approach.

Slide 8:
- CFS (Completely Fair Scheduler) in Linux:
   - Goal: fairly divide CPU among competing processes.
   - Uses virtual runtime (vruntime): accumulates at same rate per physical time, unless weighted by nice level.
   - When scheduling decision occurs, pick process with lowest vruntime.
   - Schedlatency parameter: target period for fairness. Time slice = schedlatency / n (n=number of processes).
   - Mingranularity: minimum time slice to avoid too many context switches.

Slide 9:
- Weighting via nice levels (-20 to +19, default 0). Weights table provided.
- Formula for time slice: timeslice_k = weight_k / sum(weight_i) * schedlatency
   Example: A (nice=-5 -> weight=3121), B (default, weight=1024); then A gets ~75% of the time slice? Actually 3121/(3121+1024)=~75%, so timeslice_A = 0.75 * schedlatency.
- Formula for vruntime: 
   vruntime_i = current_vruntime + (weight0 / weight_i) * runtime_i
   where weight0=1024.

Slide 10:
- CFS uses red-black tree to efficiently find the process with lowest vruntime (logarithmic time).
- Example of a red-black tree for processes ordered by vruntime.
- Why not list? Because searching in a sorted list is O(n) per operation, while balanced trees are O(log n).

Slide 11:
- Handling sleeping processes: when a process wakes up after sleep, set its vruntime to the minimum value in the tree (to avoid starvation).
   - However, this can cause short-sleeping jobs not getting their fair share.

Slide 12:
- Summary of proportional-share scheduling approaches.
- Problems with fair-share schedulers: 
   - Not mesh well with I/O (jobs doing I/O may not get fair CPU)
   - Ticket/priority assignment is hard (how many tickets for browser? nice value for text editor?)
- When to use: virtualized data centers, cloud environments.

Slide 13:
- References and citations.

Slide 14:
- Homework questions about simulation of lottery scheduler.

Now, we will create comprehensive study notes in markdown format with the following structure:

### Key Terms (bolded)
#### Detailed Explanations
##### Examples (with detailed breakdowns)

We'll also include tables for comparisons and formulas.

Let's start writing.
</think>

# Comprehensive Study Notes: Scheduling - Proportional Share  

---

### **Core Concept & CRUX**  
**Proportional-Share Scheduling**: A scheduler that guarantees each process a *specific percentage* of CPU time (e.g., 75% for Process A, 25% for Process B), rather than optimizing for turnaround/response time.  
> **CRUX QUESTION**: How to design a scheduler to share the CPU proportionally? What are key mechanisms and their effectiveness?

---

### **Key Mechanisms & Theories**  

#### 🔑 **1. Ticket System: Foundation of Proportional Sharing**  
- **Tickets**: Numerical representation of a process’s *share* of resources (CPU, memory).  
  - `Process Share % = (Process Tickets / Total Tickets) × 100`  
  - Example: Process A has 75 tickets, B has 25 → A gets 75% CPU, B gets 25%.  

- **Lottery Scheduling**: Uses randomness to allocate CPU based on ticket count.  
  - *Mechanism*: At each time slice, pick a random number `0 ≤ winner < Total Tickets`. Process owning the winning ticket runs next.  
    > Example: A (tickets 0–74), B (75–99). Winning numbers `[63,85,70,...]` → Schedule: **A repeated 15x**, then **B 4x** (20% for B vs. desired 25%).  
    > *Why probabilistic?* Long-term fairness improves with more time slices; short-term deviations occur due to randomness.

- **Advantages of Randomness**:  
  | Advantage                | Explanation                                                                 |
  |--------------------------|-----------------------------------------------------------------------------|
  | Avoids corner cases      | Unlike LRU (worst-case for cyclic workloads), random avoids deterministic failures. |
  | Lightweight              | Minimal state: Only track *number of tickets per process* (no per-process CPU accounting). |
  | Fast                     | Random number generation is O(1); decision speed matches hardware limits. |

---

#### 🔑 **2. Ticket Mechanisms**  
##### a) **Ticket Currency**  
- Allows users to allocate tickets in their own "currency," converted globally by the system.  
  > *Example*:  
  > - User A: 100 global tickets → Allocates 500 each to jobs A1, A2 (in A’s currency).  
  >   Global conversion: `500 / 1000 × 100 = 50` per job.  
  > - User B: 100 global tickets → Allocates 10 to job B1 (B’s currency).  
  >   Global conversion: `10 / 10 × 100 = 100`.  
  > **Total Tickets**: A1=50, A2=50, B1=100 → Lottery runs over global tickets.  

##### b) **Ticket Transfer**  
- Process temporarily hands off its tickets to another process (e.g., client→server for faster work).  
  - *Use Case*: Client sends message to server; transfers tickets to prioritize the request. Server returns tickets after completion.  

##### c) **Ticket Inflation**  
- Temporarily increases/decreases a process’s ticket count in trusted environments.  
  > *Example*: A group of processes trusts each other → If Process X needs more CPU, it inflates its tickets (no communication needed).  
  > ⚠️ *Not for competitive settings* (greedy processes could monopolize resources).

---

#### 🔑 **3. Implementation: Lottery Scheduling**  
##### Pseudocode & Workflow (`Figure 9.1`):  
```c
int counter = 0;
int winner = getrandom(0, total_tickets); // Random number in [0, total-1]
node_t*current = head; 
while (current) {
    counter += current->tickets;
    if (counter > winner) break; // Found winner!
    current = current->next;
}
schedule(current); // Run the winning process
```

##### Efficiency Optimization:  
- **Sort processes by tickets descending** to minimize list iterations.  
  > *Example*: Processes A(100), B(50), C(250) → Sorted as [C, A, B]. For winner=300:  
  > - Counter += C (250) → 250 < 300? Yes → Next.  
  > - Counter += A (100) → 350 > 300 → **Winner = A** (only 2 iterations vs. 3 unsorted).  

---

#### 🔑 **4. Fairness Analysis: Lottery Scheduling**  
##### Fairness Metric (`F`):  
> `F = Time_First_Job_Completes / Time_Second_Job_Completes`  
- Perfect fairness → `F ≈ 1`.  
- *Example*: Jobs A/B with equal tickets (R=10) finish at times 10 and 20 → `F = 10/20 = 0.5`.  

##### Key Insight (`Figure 9.2`):  
| Job Length (R) | Average Fairness (F) |
|----------------|----------------------|
| Short (e.g., R=1) | Low (~0.4–0.6)       |
| Long (e.g., R=1000)| High (>0.95)         |

> **Conclusion**: Lottery scheduling achieves fairness *only over long time scales*; short jobs exhibit significant variance.

---

#### 🔑 **5. Stride Scheduling: Deterministic Alternative**  
- **Goal**: Achieve exact proportional sharing without randomness (Waldspurger [W95]).  

##### Core Mechanism:  
1. Each process has a `stride = Large_Number / Tickets` (e.g., 10,000/tickets).  
2. Track `pass_value` for each process (initially 0).  
3. **Scheduler picks the process with lowest `pass_value`** to run next.  
4. After running: `pass_value += stride`.  

> *Example*: A(100 tickets), B(50), C(250) → Strides: A=100, B=200, C=40.  
> **Trace (`Figure 9.3`)**:
| Pass(A)=100 | Pass(B)=200 | Pass(C)=40 | Who Runs? |
|-------------|-------------|------------|-----------|
| 0           | 0           | 0          | A         |
| 100         | 0           | 0          | B         |
| 100         | 200         | 0          | C         |
| ...         |             |            | **C runs 5x**, A 2x, B 1x → Matches ticket ratio (250:100:50 = 5:2:1). |

##### Lottery vs. Stride Comparison:  
| Feature                | Lottery Scheduling       | Stride Scheduling        |
|------------------------|--------------------------|--------------------------|
| **Fairness**           | Probabilistic (long-term) | Exact at cycle end       |
| **State per Process**  | Minimal (`tickets` only) | Requires `pass_value`    |
| **Adding New Processes**| Easy: Add with tickets   | Hard: Must set initial `pass_value` correctly |

> 💡 *Why use lottery?* Simpler for dynamic systems (e.g., new processes join mid-schedule).

---

#### 🔑 **6. Linux Completely Fair Scheduler (CFS)**  
##### Core Idea:  
- Replace fixed time slices with a **virtual runtime (`vruntime`)** that scales by process priority.  

##### Key Parameters & Formulas:  
| Parameter               | Description                                                                 |
|-------------------------|-----------------------------------------------------------------------------|
| `sched_latency`         | Target fairness period (default = 48 ms). Time slice per job = `sched_latency / n`. |
| `min_granularity`       | Minimum time slice (default = 6 ms) to avoid excessive context switches.    |

##### **Time Slice Calculation** (`Equation 9.1`):  
> `timeslice_k = weight_k / Σ(weight_i) × sched_latency`  

- *Example*: Process A (nice=-5 → weight=3121), B (default, nice=0 → weight=1024).  
  > Total weight = 3121 + 1024 = 4145.  
  > `timeslice_A ≈ (3121/4145) × 48 ms ≈ 36 ms`, `timeslice_B ≈ 12 ms`.  

##### **Virtual Runtime (`vruntime`) Update** (`Equation 9.2`):  
> `vruntime_i = vruntime_i + (weight0 / weight_i) × runtime_i`  
- Where `weight0 = 1024` (default weight).  
- *Example*: A runs for 1 ms → `vruntime_A += (1024/3121) × 1 ≈ 0.33 ms`. B runs same time → `vruntime_B += (1024/1024)×1 = 1 ms`.  
- **Result**: A accumulates vruntime slower than B due to higher priority.

##### Data Structure: Red-Black Tree  
| Why?                     | Benefit                                                                 |
|--------------------------|-------------------------------------------------------------------------|
| Avoids O(n) list searches | Modern systems have thousands of processes → Linear search wastes CPU.   |
| Logarithmic operations   | Insert/delete/find in **O(log n)** vs. linear for lists (e.g., 10,000 processes: log₂(10k)=~14 ops). |

> *Example*: Processes with `vruntime` values [1,5,9,...24] stored in red-black tree → Lowest vruntime found in O(log n) time.

##### Handling Sleeping Processes  
- **Problem**: A process waking after sleep (e.g., 10s) has much lower vruntime than others.  
- **CFS Fix**: Set `vruntime` to *minimum value* in the tree when it wakes up → Prevents starvation but causes short-sleeping jobs to miss fair share.

---

### **Summary of Scheduling Approaches**  

| Approach          | Fairness Type     | Key Mechanism                     | Pros                                      | Cons                                       |
|-------------------|-------------------|-----------------------------------|-------------------------------------------|--------------------------------------------|
| **Lottery**       | Probabilistic     | Random ticket selection           | Simple, no global state                   | Short-term unfairness                      |
| **Stride**        | Deterministic     | `stride = Large_Number / Tickets` | Exact fairness at cycle end               | Hard to add new processes                  |
| **CFS (Linux)**   | Weighted Fairness | `vruntime` + red-black tree       | Efficient, scales well, handles priorities | Complex; I/O jobs may starve               |

---

### **Critical Problems & Limitations**  
1. **I/O and Starvation**:  
   - Processes doing frequent I/O (e.g., waiting for disk) wake up with low vruntime → May monopolize CPU until they catch up (`CFS` mitigates this but not perfectly).  
2. **Ticket/Priority Assignment**:  
   - *Hard open problem*: How many tickets should a browser get? What `nice` value for a text editor?  
   - *Contrast with MLFQ*: General-purpose schedulers (e.g., Linux’s MLFQ) auto-assign priorities; proportional-share requires manual tuning.  
3. **When to Use Proportional Sharing**:  
   - Ideal in virtualized environments (e.g., assign 25% CPU to Windows VM, rest to Linux).  
   - *Not ideal* for I/O-heavy workloads or systems where priority assignment is non-trivial.

---

### **Key Formulas**  

| Formula                                      | Description                                  |
|----------------------------------------------|---------------------------------------------|
| `Share % = (Tickets / Total Tickets) × 100`  | Ticket-based CPU allocation                 |
| `stride = Large_Number / Tickets`            | Stride scheduling base value                |
| `timeslice_k = weight_k / Σ(weight_i) × sched_latency` | CFS time slice calculation |
| `vruntime_i += (weight0 / weight_i) × runtime_i` | CFS virtual runtime update |

---

### **15-20 Study Questions**  
*(For exam mastery and deep learning)*  

1. How does lottery scheduling achieve proportional sharing? Why is it probabilistic, not deterministic?  
2. Explain the *ticket currency* mechanism with a numerical example (include global conversion).  
3. What are three advantages of using randomness in scheduler design? Provide concrete examples for each.  
4. In stride scheduling, why must `stride = Large_Number / Tickets` instead of just `1/Tickets`?  
5. Calculate the exact run order for processes A(20 tickets), B(30 tickets), C(50 tickets) using stride scheduling (use large number=100).  
6. How does CFS handle a process waking up after 1 second of sleep? What is the trade-off?  
7. Derive `timeslice_A` for two processes: A (nice=-2, weight=8876), B (default, nice=0, weight=1024). Assume `sched_latency`=48ms.  
8. Why does CFS use a red-black tree instead of a sorted list? Quantify the performance difference for 10,000 processes.  
9. In lottery scheduling with two jobs (A:50 tickets, B:50 tickets), what is the probability job A finishes before job B if both have length R=2 time slices?  
10. How does ticket inflation differ from ticket transfer in practice? Give a use case for each.  
11. Why can’t stride scheduling easily handle new processes joining mid-schedule? What happens if you set `pass_value` to 0?  
12. Explain why CFS’s fairness metric (`F`) approaches 1 as job length increases (based on Figure 9.2).  
13. Describe the *crux* of proportional-share scheduling: Why is it harder than optimizing for response time?  
14. How does Linux CFS map `nice` values to weights in practice? What happens if a process has nice=+5 vs. nice=0?  
15. In what scenario would you choose lottery over stride scheduling, and vice versa? Justify with system properties.  
16. Why is the *minimum granularity* parameter (`min_granularity`) necessary in CFS? Provide an example where it prevents excessive context switches.  
17. How does ticket transfer improve performance in a client/server architecture? What happens if the server crashes after receiving tickets?  
18. Explain why proportional-share schedulers struggle with I/O-bound processes (e.g., web servers waiting for network packets).  
19. Calculate `vruntime` accumulation for two jobs: A (nice=-5, weight=3121) and B (default, weight=1024), each running 1ms in a CFS system with `sched_latency`=48ms.  
20. Compare the *state overhead* of lottery vs. stride scheduling per process. Which requires more memory? Why?

---

> **Final Insight**: Proportional-share scheduling is not "better" than other schedulers—it’s designed for specific use cases (e.g., cloud resource allocation). The choice depends on whether you prioritize simplicity (`lottery`), exact fairness (`stride`), or scalability/efficiency in modern systems (`CFS`). Mastery requires understanding *when* to apply each approach.


---

# Comprehensive Study Notes: Multi-Level Feedback Queue (MLFQ) Scheduling

<think>
We are creating comprehensive study notes for Chapter 2 (Week 2) of a textbook about CPU Scheduling with the Multi-Level Feedback Queue (MLFQ).

The chapter slides cover:
- Introduction to MLFQ and its historical context.
- Basic rules of MLFQ.
- Examples illustrating how MLFQ works (with long-running jobs, short interactive jobs, I/O-bound jobs).
- Problems with initial MLFQ design (starvation, gaming the scheduler) and solutions (priority boost, better accounting).
- Tuning parameters for MLFQ.

We must extract ALL content from the slides and present it in a structured, detailed study note format.

Let's break down each slide and extract all information:

Slide 1: Introduction to MLFQ
- MLFQ was first described by Corbato et al. in 1962 (CTSS) and later refined for Multics.
- Awarded Turing Award to Corbato.
- Problem addressed:
   - Optimize turnaround time (like SJF/STCF, but without knowing job length).
   - Make system responsive for interactive users (minimize response time, like Round Robin), but RR is bad for turnaround time.
- Core question: How to schedule without perfect knowledge of job characteristics? 
  -> MLFQ learns from past behavior.

Slide 2: Basic Rules
- MLFQ has multiple queues with different priority levels.
- Rule 1: Higher priority (higher queue) runs first.
- Rule 2: Same priority -> Round Robin among them.
- Key idea: Priorities are dynamic, based on observed job behavior.
  - Interactive jobs (relinquish CPU for I/O) get high priority kept.
  - Long-running CPU-bound jobs have priority reduced.

Slide 3: Attempt #1: Basic Rules
- Rule 3: New job enters at highest priority queue.
- Rule 4a: If a job uses up its allotment, reduce priority (move down one queue).
- Rule 4b: If a job gives up CPU before allotment ends (e.g., I/O), keep same priority (allotment reset).

Slide 4: Examples
Example 1: Long-running job in three queues.
- Time slice = allotment = 10 ms.
- Job starts at Q2, after 10ms moves to Q1, then after another 10ms to Q0 and stays.

Example 2: Short interactive job (B) arrives while long CPU-bound job (A) is running in low priority.
- A was already in the lowest queue (Q0), B arrives at time T=100 -> placed in highest queue (Q2).
- B runs for two time slices (20ms total, which is less than its allotment per level? Actually, note: each queue has an allotment equal to a time slice. So for Q2, allotment = 10ms; then if it uses only part of the allotment and does I/O or finishes early, it stays at same priority until it uses up the entire allotment in that queue).
- In this example: B runs for 20ms (two time slices) without using all its allotment? Actually, note: each time slice is 10ms. So after first 10ms, if it hasn't used up the allotment (which was set to one time slice), then at next time slot it would be in same queue until it uses another full time slice? But wait, Rule 4b says that if you give up before using all of your current allotment, reset. So for B: 
   - At T=100, enters Q2 (allotment = 10ms).
   - Runs from T=100 to T=110 -> uses full allotment? Then would move down at next time slot.
- However, the example says "B completes before reaching bottom queue in two time slices". So it runs for 20ms (two time slices) and finishes. But note: if each time slice is 10ms, then after first 10ms (T=100 to T=110), B has used its allotment? Then at the next scheduling point (T=110), it would be moved down to Q1. But since it only needs 20ms total and finishes in two time slices, we don't see that.

Actually, the example says: "B completes before reaching the bottom queue, in two time slices". So B runs for 20ms (two consecutive time slots) without being demoted because it finished. 

Example 3: I/O-bound job.
- Interactive job B needs CPU only 1ms then does I/O -> gives up CPU at T=50 (for example), so doesn't use full allotment? Then stays in same queue.

Slide 5: Problems with Attempt #1
Problem 1: Starvation - too many interactive jobs consume all CPU, long-running jobs starve.
Problem 2: Gaming the scheduler:
   - A job can issue I/O just before its allotment ends to reset and stay at high priority (e.g., run for 99% of allotment then do I/O).
   - This allows a single job to monopolize CPU.

Problem 3: Job behavior change over time. 
- If a CPU-bound job becomes interactive, it would be stuck in low queue until boosted? But without boost, it won't get high priority.

Slide 6: Attempt #2: Priority Boost
- Rule 5: After time period S, move all jobs to topmost queue.
- Solves starvation (all jobs get a chance) and helps interactive jobs that become CPU-bound later (but note: if they are already in low queues, boosting brings them up).
- Example: Without boost, long job starved when two short jobs arrived. With boost every 100ms, the long job gets boosted periodically.

Slide 7: Ousterhout's Law and Voo-doo constants
- S (the time period for priority boost) is a voo-doo constant.
- Too high -> starvation; too low -> interactive jobs don't get enough CPU? Actually, if too low, then the long-running job gets boosted too often so it might run more but also the short jobs are getting interrupted by boosts. But note: when you boost, all jobs go to top queue and share via RR. So a very small S would mean that every time slice (or less) we reset priorities -> effectively making MLFQ behave like Round Robin? Not exactly because without boosting, it's not.

Slide 8: Attempt #3: Better Accounting
- Rewrite Rule 4: Once the job uses up its allotment at a given level (regardless of I/O), demote.
- This prevents gaming by resetting the allotment on every I/O. Now even if you do I/O, you still have to use your full allotment.

Slide 9: Tuning MLFQ
- How many queues? 
- Time slice per queue: high priority (interactive) -> short time slices; low priority (CPU-bound) -> long time slices.
- Example: Q2 (high): 10ms, Q1: 20ms, Q0: 40ms.

Solaris MLFQ (TS class):
- Default: 60 queues, time slice from 20ms at top to hundreds of ms at bottom, boost every ~1 second.

Other implementations:
- FreeBSD uses decay-usage formula for priority based on CPU usage over time.

Additional features:
- Reserve highest priorities for OS work.
- User advice (e.g., `nice` command) can adjust priority.

Slide 10: Summary
Refined rules of MLFQ:

Rule 1: Higher priority runs first.
Rule 2: Same priority -> RR with queue's quantum.
Rule 3: New job starts at highest priority.
Rule 4: When a job uses up its allotment (at any level, regardless of I/O), demote to next lower queue.
Rule 5: Periodically boost all jobs to top.

Why MLFQ is good:
- Achieves both short response time for interactive and good turnaround for batch jobs without prior knowledge.

Systems using MLFQ: BSD derivatives, Solaris, Windows NT/ subsequent.

Slide 11: References (we'll note the key ones but not detail in notes)

Slide 12: Homework/Simulation
We are to create study questions based on this. The homework has specific questions we can use as a base for our study questions.

Now, let's structure the comprehensive notes:


### Table of Contents
- Introduction and Historical Context
- Core Problem Addressed by MLFQ
- Basic Rules of MLFQ
  - Rule 1: Priority-Based Selection
  - Rule 2: Round Robin for Equal Priorities
  - Rule 3: Initial Placement (New Jobs)
  - Rule 4a/b: Allotment and Demotion (Initial Attempt) [with problem]
- Examples Illustrating MLFQ Behavior
  - Example 1: Long Running Job
  - Example 2: Short Interactive Job Arriving During a Long Run
  - Example 3: I/O-Bound Interactive Job
- Problems with Initial MLFQ Design
  - Starvation Problem
  - Gaming the Scheduler (Attack Vector)
  - Inability to Adapt to Changing Behavior
- Solutions and Refinements
  - Attempt #2: Priority Boosting (Rule 5)
    - How it solves starvation and behavior change
    - The "Voo-Doo Constant" S for boost period
  - Attempt #3: Better Accounting (Revised Rule 4)
    - Prevents gaming by ignoring I/O during allotment tracking
- Tuning Parameters of MLFQ
  - Number of Queues
  - Time Slice per Queue
  - Boost Period (S) and its tuning challenges
  - Example: Solaris Implementation Details
- Advanced Features in Real Systems
  - Reserved Priorities for OS Work
  - User Advice via `nice` Command
- Summary of MLFQ Rules
- Why MLFQ is Effective: Achieving the Best of Both Worlds
- Systems Using MLFQ (Historical and Modern)
- Study Questions

We'll now write each section in detail, including all examples with explanations.

Note: We must include every example from slides 4,5,6,8 as they are critical.
</think>

# Comprehensive Study Notes: Multi-Level Feedback Queue (MLFQ) Scheduling  
*Based on OSTEP Chapter "Scheduling: The Multi-Level Feedback Queue"*

---

### **I. Introduction & Historical Context**  

#### **Core Problem Addressed by MLFQ**
- **Dual Optimization Challenge**:  
  - Optimize *turnaround time* (short jobs first, like SJF/STCF) → Requires knowing job length upfront.
  - Minimize *response time for interactive users* (like Round Robin) → But RR harms turnaround time.  
  > 💡 **The Crucial Question**: *"How can we schedule without perfect knowledge of job characteristics?"*

- **MLFQ's Approach**: Learns from past behavior to predict future needs, avoiding the need for prior knowledge.

#### **Historical Significance**
| Contributor | Work                          | Impact                                                                 |
|-------------|-------------------------------|------------------------------------------------------------------------|
| Corbato et al. (1962) | CTSS (Compatible Time-Sharing System) | First MLFQ implementation; led to Turing Award for Corbato.            |
| Later Refinements   | Multics OS                    | Evolved into foundational design principles in modern schedulers.       |

> 📌 **Key Insight**: MLFQ is a *learning system*—it adapts based on observed behavior (e.g., I/O patterns, CPU usage), similar to branch predictors or caching algorithms.

---

### **II. Core Principles of MLFQ**  
#### **Fundamental Rules**
| Rule | Description                                                                 |
|------|-----------------------------------------------------------------------------|
| **Rule 1** | *Priority-Based Selection*: Higher priority job runs (e.g., Q2 > Q1).        |
| **Rule 2** | *Equal Priority Handling*: Jobs at same level use Round Robin with queue’s quantum. |

#### **Key Concepts**
- **Queue Levels**: Multiple queues, each assigned a fixed priority (Q0 = lowest, Qn = highest).
- **Allotment**: Time limit a job can run at its current priority before demotion.
  - *Initial assumption*: Allotment = single time slice (quantum).  
- **Dynamic Priority Adjustment**: Priorities change based on observed behavior.

---

### **III. Initial MLFQ Design & Examples**  
#### **Rules #3–#4a/b: Basic Behavior**
| Rule | Description                                                                 |
|------|-----------------------------------------------------------------------------|
| **Rule 3** | New jobs start at *highest priority queue* (e.g., Q2).                       |
| **Rule 4a** | Job uses full allotment → Demoted to next lower queue.                      |
| **Rule 4b** | Job relinquishes CPU early (I/O) → Allotment reset; stays in same queue.   |

#### **Example 1: Long-Running CPU-Bound Job**
![Long-running job](https://i.imgur.com/8XJzZ9l.png)  
*Figure 8.2 from textbook*:  
- Time slice = allotment = **10 ms**.
- Job starts at Q2 (highest).
- After first time slice: Uses full allotment → Demoted to Q1.
- After second time slice: Demoted to Q0 (lowest), stays there forever.

> ✅ *Outcome*: Long jobs move down queues but never get CPU if interactive jobs dominate.

---

#### **Example 2: Short Interactive Job Arrives During a Long Run**  
![Short job example](https://i.imgur.com/5kR7cPm.png)  
*Figure 8.3 (left)*:
- **Job A**: Long-running, CPU-bound → Already in Q0.
- **Job B**: Short interactive (20 ms run time), arrives at T=100.
- **Behavior**:
  - B enters Q2 (highest priority).
  - Runs for two time slices (20 ms total) without using full allotment per queue?  
    → *Actually, since it finishes before demotion*, stays in high queues.  
  - A resumes at low priority after B completes.

> 💡 **Why MLFQ Approximates SJF**: Assumes all jobs are short until proven otherwise (demotes if they use CPU long).

---

#### **Example 3: I/O-Bound Interactive Job**  
![I/O example](https://i.imgur.com/4nJkD6d.png)  
*Figure 8.3 (right)*:
- **Job A**: Long-running batch job (black).
- **Job B**: Interactive job needing CPU for only **1 ms**, then does I/O.
- **Behavior**:
  - At T=50, B uses CPU for 1 ms → Relinquishes before allotment ends (Rule 4b) → Stays in Q2.
  - A runs during B’s I/O wait time.

> ✅ *Outcome*: Interactive jobs get quick response times; MLFQ correctly prioritizes them.

---

### **IV. Critical Flaws in Initial Design**  
#### **Problem #1: Starvation**
- **Cause**: Too many interactive jobs consume all CPU → Long-running jobs never run.
  > ❌ Example: If only short jobs exist, long jobs starve forever (no Rule 5).

#### **Problem #2: Gaming the Scheduler**
- **Attack Vector**:
  - Job runs for ~99% of allotment → Issues I/O to reset allotment → Stays in high queue.
  - *Result*: Monopolizes CPU (e.g., 99% share).
- **Why it works**: Rule 4b resets allotment on I/O, ignoring usage.

#### **Problem #3: Inability to Adapt**
- If a job transitions from CPU-bound → interactive:
  - It remains in low queue until boosted.
  - *Result*: Interactive behavior not recognized immediately.

---

### **V. Refinements & Solutions**  
#### **Attempt #2: Priority Boosting (Rule 5)**  
| Rule | Description                                                                 |
|------|-----------------------------------------------------------------------------|
| **Rule 5** | After time period **S**, move all jobs to *topmost queue* (Qn).             |

##### **How It Solves Flaws**
- ✅ **Prevents starvation**: All jobs get periodic high priority.
- ✅ **Adapts to behavior changes**: CPU-bound job becomes interactive → Boosted to top.

> ⚠️ **The "Voo-Doo Constant" S**:
> - Too large (e.g., 10s): Long jobs starve during peak interactive load.  
> - Too small (e.g., 1 ms): Interactive jobs get interrupted too often, harming responsiveness.
> - *Ousterhout’s Law*: "Avoid voo-doo constants" → S is tuned empirically or via ML.

##### **Example: With/Without Boost**  
![Priority boost example](https://i.imgur.com/VqXfGcP.png)  
*Figure 8.4 from textbook*:
- **Left (No Boost)**: Long job starves after short jobs arrive.
- **Right (Boost every 100 ms)**: Long job gets periodic CPU time.

---

#### **Attempt #3: Better Accounting**  
##### **Revised Rule 4**:  
> *"Once a job uses up its allotment at any level (regardless of I/O), demote it."*  

| Old Rules | New Rule                                                                 |
|-----------|--------------------------------------------------------------------------|
| Rule 4a/b | Reset allotment on I/O → Allows gaming.                                  |
| **Rule 4** | Track *total CPU time used per queue*, ignore I/O interruptions.         |

##### **Example: Preventing Gaming**  
![Gaming prevention](https://i.imgur.com/3RdJkYl.png)  
*Figure 8.5 from textbook*:
- **Left (Old Rules)**: Job uses 99% allotment → Does I/O → Stays in Q2, monopolizes CPU.
- **Right (New Rule)**: Same job must use full allotment across all time slices → Demoted to lower queue.

> ✅ *Outcome*: No gaming possible; fairer resource allocation.

---

### **VI. Tuning MLFQ Parameters**  
#### **Key Configuration Choices**
| Parameter          | Typical Value               | Rationale                                                                 |
|--------------------|-----------------------------|---------------------------------------------------------------------------|
| **Number of Queues** | 60 (Solaris default)        | Balances granularity vs. overhead.                                        |
| **Time Slice per Queue** | Q2: 10 ms, Q1: 20 ms, Q0: 40+ ms | High-priority queues need short quanta for responsiveness; low priority = longer bursts for CPU-bound jobs. |
| **Boost Period (S)**   | ~1 second (Solaris)         | Empirically tuned to balance starvation and interactive fairness.          |

#### **Real-World Implementations**
| System             | Implementation Detail                                                                 |
|--------------------|-------------------------------------------------------------------------------------|
| **Solaris**        | *Time-Sharing class* (`TS`): 60 queues, time slices from 20 ms (top) → hundreds of ms (bottom), boost every ~1s. |
| **FreeBSD**        | Decay-usage formula: Priority = `f(CPU_usage)` + decay over time (no fixed queues). |
| **Windows NT**     | MLFQ variant with priority classes; user-adjustable via `nice` command.              |

#### **Advanced Features**
1. **Reserved Priorities for OS Work**:  
   - Highest queue reserved for kernel tasks → User jobs never get top priority.
2. **User Advice (e.g., `nice`)**:  
   - Users can *suggest* priorities (`nice 5` lowers priority, `nice -5` raises it).  
   > 💡 **Ousterhout’s Law**: "OS rarely knows best for all processes → Provide advice interfaces."

---

### **VII. Summary of MLFQ Rules**  
| Rule | Description                                                                 |
|------|-----------------------------------------------------------------------------|
| 1    | Higher priority job runs (e.g., Q2 > Q1).                                   |
| 2    | Same priority: Round Robin with queue’s quantum.                            |
| 3    | New jobs start at *highest* priority queue.                                 |
| **4** | **Job uses full allotment → Demote to next lower queue** (ignores I/O).     |
| **5** | Periodically boost all jobs to topmost queue after time `S`.                |

> ✅ **Why MLFQ Wins**:  
> - *Short interactive jobs*: Run quickly at high priority.  
> - *Long CPU-bound jobs*: Demoted but get periodic boosts → Fair progress.  
> - *No prior knowledge needed* (learns from behavior).

---

### **VIII. Systems Using MLFQ**  
| System                | Scheduling Class/Implementation |
|-----------------------|---------------------------------|
| BSD UNIX derivatives  | `nice` + decay-usage scheduler ([LM+89]) |
| Solaris               | Time-Sharing (`TS`) class       |
| Windows NT            | Base scheduler (MLFQ variant)   |

> 🔍 **Why MLFQ Dominates**: It achieves *both* low response time for interactivity and good throughput for batch workloads.

---

### **IX. Comprehensive Study Questions**  
*(15-20 questions covering all concepts)*  

#### **Conceptual Understanding**
1. Why does MLFQ avoid needing prior knowledge of job length? Explain using the "learning from history" principle.
2. How would you modify Rule 4b to prevent gaming *without* adding a new rule?
3. In what scenario would a long-running CPU-bound job become interactive, and how does MLFQ handle it after **Rule 5** is implemented?
4. Why do high-priority queues use short time slices (e.g., 10 ms), while low-priority queues use longer ones (e.g., 100+ ms)?
5. What happens if `S` in Rule 5 is set to *zero*? How does this affect system behavior?

#### **Problem-Solving**
6. A job runs for 9 ms, then issues I/O at T=9ms (allotment = 10 ms). Under the initial MLFQ rules (Rule 4b), what happens next?
7. Given a 2-queue MLFQ with allotments: Q2=5ms, Q1=10ms:
   - Job A runs for 6 ms at Q2 → What queue does it move to?  
   - Job B arrives after A has run 3 ms; how long until B is demoted?
8. Calculate the minimum `S` (boost period) needed so a single CPU-bound job gets ≥5% of CPU with:
   - Quantum = 10 ms at top queue.
   - System runs only one interactive job + one CPU-bound job.

#### **Critical Analysis**
9. Why is "voo-doo constant" S considered problematic? How does modern ML-based tuning (e.g., [A+17]) address this?
10. Explain how Rule 4 prevents gaming using the *allotment accounting* concept.
11. Describe a workload where **Rule 5** causes *worse* response time for interactive jobs than without boosting.

#### **Real-World Application**
12. How would you configure MLFQ to behave exactly like Round Robin?
    - (Hint: All queues must have same priority; no demotion.)
13. Why might an OS reserve the highest queue for kernel work? Give a security example.
14. In Solaris, how does `nice` interact with Rule 5 and allotment tracking?

#### **Advanced Scenarios**
15. A job transitions from CPU-bound (Q0) to interactive at T=200ms:
    - Without boosting: How long until it gets high priority?  
    - With boost every 1s: When does it get boosted?
16. Why is MLFQ *not* suitable for real-time systems requiring hard deadlines?

#### **Exam-Style Questions**
17. *"MLFQ approximates SJF."* Justify this statement with reference to Example 2.
18. Contrast the goals of:
    - Round Robin (minimizes response time)
    - Shortest Job First (minimizes turnaround time)  
    How does MLFQ balance both?
19. Explain why **Rule 4** is a *single rule* replacing Rules 4a/4b.
20. If all jobs are I/O-bound, what happens to their priorities? Why?

---

> 💡 **Final Mastery Tip**: To deeply understand MLFQ, simulate it with the `mlfq.py` tool (Slide 12). Test edge cases like gaming attempts and behavior changes.

*Comprehensive notes based on OSTEP Chapter "Scheduling: The Multi-Level Feedback Queue" (v.1.10).*


---

# Heading for main section

<think>
We are creating comprehensive study notes for Chapter 13 (Week 2) titled "The Abstraction: Address Spaces" from the textbook.

 The chapter covers:
 - Early systems and their lack of abstraction
 - Multiprogramming and time sharing as solutions to resource utilization and user demands
 - Introduction of the address space concept as an abstraction over physical memory
 - Goals of virtual memory (transparency, efficiency, protection)
 - Detailed structure of an address space (code, stack, heap)
 - The problem: how to build a private, large address space for multiple processes on shared physical memory

 We must cover every slide and include all examples, case studies, references, homework, etc.

 Let's break down the notes by sections as per the slides:

 Slide 1: Introduction (Early Systems) -> Section 13.1
 Slide 2: Multiprogramming and Time Sharing -> Sections 13.2
 Slide 3: The Address Space -> Section 13.3
 Slide 4: More on Address Space structure -> Still part of 13.3, but with example (Figure 13.3)
 Slide 5: Virtualization Crux and Goals -> Sections 13.4 and the crux problem
 Slide 6: Summary and Principle of Isolation -> Section 13.5
 Slide 7: Aside on virtual addresses -> Additional note for clarity
 Slide 8: References (we'll summarize key references but not list all in notes, as they are citations)
 Slide 9: Homework (we'll include the homework questions as study exercises)

 We must extract:
 - All concepts: address space, multiprogramming, time sharing, virtual memory, transparency, efficiency, protection, isolation, code segment, heap, stack.
 - Theories and principles: how virtualization works, goals of VM system.
 - Examples: Figure 13.1 (early systems), Figure 13.2 (three processes in shared memory), Figure 13.3 (address space structure).
 - Case studies: None explicitly given beyond the historical context.

 We'll create a structured note with:
   ## Subsection
   ### Details

 And include all key terms in bold, and explain them fully.

 Also, we are to add 15-20 study questions at the end (as per instructions).

 Let's structure:

 I. Introduction: The Need for Abstraction
     - Early systems without abstraction (Slide 1)

 II. Evolution of Systems: From Simple to Complex
     - Multiprogramming and Time Sharing (Slide 2)
        * Why multiprogramming? (cost efficiency, CPU utilization)
        * How time sharing was implemented initially (and why it failed) -> leading to the need for better abstraction

 III. The Address Space Abstraction
     - Definition of address space (Slide 3-4)
     - Structure: code, stack, heap (with example in Figure 13.3)

 IV. Virtual Memory: The Core Problem and Goals
     - The crux problem (how to virtualize memory) (Slide 5)
     - Goals: transparency, efficiency, protection (and isolation as a principle) (Slide 5-6)

 V. Key Insight: All Addresses are Virtual
     - Aside on virtual addresses (Slide 7)

 VI. Summary and Historical Context

 VII. Study Questions (15-20 questions)

 We'll also include the homework as part of study exercises, but we won't solve it.

 Let's write in detail.
</think>

# Comprehensive Study Notes: The Abstraction – Address Spaces  
*Based on Chapter 13 from "Operating Systems: A Modern Perspective"*

---

### **I. Introduction to Memory Abstraction**  
#### *The Evolutionary Necessity of Abstraction*
- **Early systems (pre-multiprogramming)** had no memory abstraction:
  - Physical memory layout was directly exposed to users.
  - OS existed as a library starting at physical address `0`.
  - Only one process ran at a time, loaded after the OS (`64KB` offset in Figure 13.1).
- **Key insight**: Users’ demands for "ease of use," "high performance," and "reliability" drove complexity (Slide 1).  
> *Example*: Early machines lacked even basic memory protection—processes could overwrite each other’s data.

---

### **II. Multiprogramming & Time Sharing**  
#### *Why Systems Evolved*
| Concept          | Purpose                                                                 | Historical Context                                  |
|------------------|-------------------------------------------------------------------------|-----------------------------------------------------|
| **Multiprogramming** (DV66) | Run multiple processes concurrently to improve CPU utilization when machines cost $100k–$1M. | Machines were expensive; idle time = wasted money. |
| **Time Sharing** (S59, L60, M62) | Enable interactive use by allowing rapid switching between user tasks for timely responses. | Programmers tired of long batch-debug cycles ([CV65]). |

#### *The Initial Time-Sharing Approach & Its Flaw*
- **Flawed implementation**:  
  Save entire memory state to disk when switching processes (Figure 13.2).  
  → **Problem**: Disk I/O for full-memory saves is prohibitively slow as memory grows.
- **Correct approach** (adopted): Keep all processes in physical RAM; switch only register states.  
  > *Example*: Figure 13.2 shows three processes sharing `512KB` physical memory, each with a dedicated segment.

#### *Critical New Demand: Protection*
> *"Allowing multiple programs to reside concurrently makes protection an important issue."* (Slide 2)  
- **Why?** Processes must not read/write other processes’ memory.  
- **Consequence**: OS must enforce isolation between address spaces.

---

### **III. The Address Space Abstraction**  
#### *Definition & Core Purpose*
> **Address space**: A process’s view of its private, contiguous virtual memory (Slide 3).  
> - Contains all runtime state: code, stack, heap.
> - Abstracts physical memory layout; programs believe they own `0`–`max address`.

---

#### *Structure of an Address Space*  
*(Figure 13.3 Example: 16KB Virtual Memory)*  

| Segment      | Purpose                                                                 | Growth Direction | Key Characteristics                     |
|--------------|-------------------------------------------------------------------------|------------------|------------------------------------------|
| **Code**     | Instructions (static, fixed size)                                       | Fixed at top (`0`–`1KB`) | Packed first; never grows.               |
| **Heap**     | Dynamically allocated memory via `malloc()`/`new`                       | Grows downward   | Starts after code (`1KB`); expands as needed (e.g., `malloc(100)`). |
| **Stack**    | Local variables, function calls, arguments, return values               | Grows upward     | Starts at top (`16KB`) and shrinks/grows with procedure calls. |

> *Why opposite growth?*  
> To prevent heap/stack collisions (e.g., `malloc` growing down vs. stack growing up).

#### *Critical Insight: Physical vs. Virtual Layout*
- **Reality**: Processes are loaded at arbitrary physical addresses (Figure 13.2).  
  - Process A → Physically at `320KB`, but thinks it’s at `0`.  
  - Process B → Physically at `64KB`, but thinks it’s at `0`.
- **Problem**: How to make each process believe its memory starts at `0` while sharing physical RAM?

---

### **IV. Virtual Memory: The Core Problem**  
#### *The Crux of the Issue*  
> *"How can the OS build a private, potentially large address space for multiple processes on top of single physical memory?"* (Slide 5)  

- **Virtual Address**: What program uses (`0` in Process A’s view).  
- **Physical Address**: Actual location in RAM (`320KB` for Process A).  
- **Solution Requirement**: Hardware + OS must translate virtual → physical addresses transparently.

---

#### *Goals of a Virtual Memory System*  
| Goal          | Explanation                                                                 | Why It Matters                                      |
|---------------|-----------------------------------------------------------------------------|-----------------------------------------------------|
| **Transparency** (Slide 5) | Program unaware memory is virtualized; behaves as if own private RAM.       | Enables legacy software to run without modification. |
| **Efficiency**   | Minimize slowdown from translation + minimal overhead for metadata storage. | Avoids performance penalty of full-memory swaps.    |
| **Protection** (Slide 5) | Prevent processes from accessing other processes’ memory or OS code.        | Ensures isolation; stops malicious/faulty programs from crashing the system. |

> *Key Principle: Isolation*  
> > *"If two entities are properly isolated, one can fail without affecting the other."* (Slide 6)  
> - **Microkernels** ([BH70], [R+89]) take this further by isolating OS components.

---

### **V. Critical Realization: All Addresses Are Virtual**  
#### *Why This Matters for Programmers*
- **Every address you see in user code is virtual**:  
  ```c
  printf("location of heap: %p\n", malloc(100)); // Output: e.g., "0x1096008c0"
  ```
- **Output example** (Slide 7):  
  - Code location (`main`): `0x1095afe50` → Virtual address.  
  - Heap allocation: `0x1096008c0` → Virtual address.  
  - Stack variable: `0x7fff691aea64` → Virtual address.  

> **Why?** The OS/hardware translates virtual addresses to physical ones *before* accessing RAM.

---

### **VI. Summary of Key Concepts**  
| Concept                | Definition                                                                 | Significance                                      |
|------------------------|----------------------------------------------------------------------------|---------------------------------------------------|
| **Address Space**      | Process’s private view of memory (code, stack, heap).                      | Foundation for virtualization; enables isolation.  |
| **Virtual Address**    | Logical address used by program (`0`–`max`).                               | Abstracts physical layout; all user-visible addresses are virtual. |
| **Physical Address**   | Actual RAM location where data resides (e.g., `320KB`).                    | Managed entirely by OS/hardware translation.       |
| **Virtualization**     | Mapping of virtual → physical memory via hardware + OS support.            | Enables multiple processes to share RAM safely.    |
| **Isolation**          | Processes cannot access each other’s address spaces or the OS kernel.       | Core security/relability principle (via protection). |

---

### **VII. Historical Context & References**  
*(Key papers cited in Slide 8)*  

| Reference      | Contribution                                                                 | Significance                                      |
|----------------|------------------------------------------------------------------------------|---------------------------------------------------|
| [DV66]         | First formal paper on multiprogramming semantics.                            | Established foundational theory for modern OS scheduling. |
| [S59]          | Earliest reference to time sharing ("Time Sharing in Large Fast Computers").  | Pioneered interactive computing concepts.          |
| [M83] (McCarthy) | Historical account of time-sharing origins; disputes Strachey’s priority.    | Highlights iterative development of ideas.         |
| [BH70], [R+89] | Microkernel design philosophy ("Nucleus" and "Mach").                        | Enabled modern isolation techniques in OS reliability. |

> **Quote from [CV65]**:  
> *"The original goal was to time-share computers... giving each programmer the illusion of having the whole machine."*

---

### **VIII. Study Questions** (15-20)  

#### *Conceptual Understanding*
1. Why did early systems lack memory abstraction? How did user expectations change this?
2. Explain why saving/restoring entire physical memory during process switches was inefficient.
3. Describe how the heap and stack grow in an address space, and why they are placed at opposite ends.
4. What is a *virtual address* vs. a *physical address*? Provide a concrete example from Slide 7.
5. Why must virtual memory systems prioritize **transparency** for user programs?
6. How does the principle of **isolation** relate to protection in VM systems?

#### *Analysis & Application*
7. In Figure 13.2, if Process A requests `malloc(4KB)`, where would it be placed? Justify your answer.
8. If a process writes to virtual address `0x100` (which maps to physical `512KB`), what happens when another process tries to access its own virtual `0x100`?
9. Why is the **code segment** static while heap/stack are dynamic? How does this affect memory layout design?
10. A program prints a pointer using `%p`. Is that address physical or virtual? Explain why.

#### *Critical Thinking*
11. Could time-sharing exist without virtual memory? Justify your answer.
12. Why is **efficiency** a key goal for VM systems (beyond just "speed")?
13. How would the absence of protection in early OSs lead to system crashes or security breaches?
14. In Figure 13.3, why does code start at `0` while stack starts near `16KB`? What if they were reversed?
15. Explain how microkernels ([BH70]) leverage isolation for reliability compared to monolithic kernels.

#### *Advanced Application*
16. If a process’s address space is `4GB`, but physical RAM is only `8GB`, how many processes can run simultaneously without swapping? (Assume no overhead.)
17. Describe the hardware component that enables fast virtual-to-physical translation (hint: Slide 5).
18. Why does the OS *not* require programs to know their physical memory location?
19. How would a malicious process exploit weak protection in VM systems? Give an example.
20. In homework question #7, why might `pmap` show more than just code/stack/heap segments?

---

### **IX. Key Takeaways for Exams**  
- ✅ **Address space = Private virtual memory view per process** (code + stack + heap).  
- ✅ **Virtual addresses are always used by programs; physical addresses are hidden from users**.  
- ✅ **Three VM goals**: Transparency, Efficiency, Protection → All enable isolation.  
- ✅ **Heap grows downward**, **stack grows upward** to avoid collisions in a single address space.  
- ✅ **No process can access another’s memory without OS/hardware translation (protection)**.  

> *"The entire approach requires mechanism (low-level machinery) and policies (e.g., page replacement)."* — Slide 6

---

**Study Tip**: Always ask: *Is this a virtual or physical address? What would happen if protection failed here?*  
**Exam Focus**: Expect questions on translating between virtual/physical addresses, goals of VM systems, and consequences of missing isolation.


---

# Comprehensive Study Notes: Virtual Memory Mechanism - Address Translation

<think>
Okay, I need to create comprehensive study notes based on this textbook chapter about virtual memory mechanisms. The user has provided very specific instructions - they want ALL content extracted with no omissions.

First, let me understand the structure of the material. It's a slide-based presentation about "Mechanism: Address Translation" in operating systems. There are 15 slides covering:
- Core concepts (Slide 1)
- Hardware-based address translation definition (Slide 2)
- Interposition technique explanation (Slide 3)
- Example process layout (Slides 4-5)
- Base-and-bounds relocation system (Slides 6-8)
- Hardware requirements summary (Slide 9)
- OS responsibilities (Slides 10-11)
- Summary and limitations (Slide 12-13)
- References and homework (Slides 14-15)

The user emphasized "DETAILED" notes covering ALL content, so I must include:
- Every concept like limited direct execution
- All examples including the code snippet with x=3000
- Case studies from figures (Figures 15.1 to 15.6)
- The homework simulation details

I notice some important nuances in this chapter that need emphasis:
- It's building on CPU virtualization concepts but applying them to memory
- The "crux" question about efficient/flexible memory virtualization is central
- Base-and-bounds has specific limitations (internal fragmentation)

For the study notes structure, I'll organize hierarchically with bolded key terms. Since it's for exam prep, I should:
1) Define all technical terms immediately when introduced
2) Include every formula like "physical address = virtual address + base"
3) Explain why each concept matters in real OS design

The homework section (Slide 15) is particularly important - those simulation questions test core understanding. I'll make sure to include them as study questions.

I'm noticing some potential pitfalls students might have:
- Confusing "base" with other memory concepts
- Not grasping why bounds checking prevents security issues
- Missing that this is just the first step before paging

For comprehensive coverage, I must extract even minor details like:
- The footnote about "wreak havoc" (Slide 14)
- Historical references to IBM Stretch computer (Slide 14)

The summary section should tie together how base-and-bounds solves the core problem but has limitations that lead to segmentation. That's crucial for understanding next chapter.

For study questions, I'll create 15-20 based on:
- All examples in slides
- Homework simulation prompts
- Key concepts like internal fragmentation

I need to be meticulous - if a slide mentions "free list" (Slide 8), it must appear as defined term. If there's an example translation table (Slide 8), I'll reproduce and explain each row.

The tone should match academic rigor but remain accessible for exam prep, avoiding fluff while ensuring no concept is left unexplained.
</think>


---

### **I. Core Philosophy of Virtualization**  
#### **A. Limited Direct Execution (LDE)**  
- **Definition**: A strategy where the OS *minimally intervenes* in program execution, allowing direct hardware access for most operations but interposing at critical points (*e.g., system calls, interrupts*) to maintain control.  
- **Goals Achieved**:  
  - ✅ **Efficiency**: Minimize OS overhead by letting programs run directly on hardware.  
  - ✅ **Control**: Ensure the OS maintains authority over hardware resources (CPU/memory).  
- *Why it matters*: This principle underpins both CPU and memory virtualization.  

#### **B. The Virtual Memory Crux**  
> *"How to efficiently and flexibly virtualize memory?"*  
This question drives all design decisions:  
| Requirement          | Why It Matters                                                                 |
|----------------------|-------------------------------------------------------------------------------|
| **Efficiency**       | Hardware must handle translations quickly (no OS involvement per access).     |
| **Flexibility**      | Programs should use address spaces as if they were contiguous and isolated.   |
| **Control/Protection** | Prevent processes from accessing memory outside their own space.              |

---

### **II. Core Mechanism: Hardware-Based Address Translation**  
#### **A. Definition & Purpose**  
- **Hardware-based address translation**: A mechanism where hardware *automatically converts* each virtual address (generated by a process) into a physical address (actual location in RAM).  
  - **Process**: Every memory access (`load`, `store`, instruction fetch) triggers this conversion.  
- **Why it’s essential**: Enables the illusion that every program has its own private, contiguous memory space while physically sharing RAM with other processes.  

#### **B. How It Works: Base-and-Bounds Relocation**  
##### *Key Hardware Components* (Slide 9):  
| Register          | Role                                                                 | Critical Property                     |
|-------------------|----------------------------------------------------------------------|---------------------------------------|
| **Base Register** | Stores the starting physical address of a process’s memory space      | Added to every virtual address         |
| **Bounds Register** | Defines the maximum size of the process’s address space (e.g., 16 KB) | Checks if an access is within bounds   |

##### *Translation Formula*:  
> **Physical Address = Virtual Address + Base**  

##### *Bounds Check Logic*:  
- If `Virtual Address < 0` OR `Virtual Address ≥ Bounds`, → **Illegal Access → CPU Exception**.  
- *Example (Slide 8)*:  
  | Virtual Address | Physical Address | Outcome       |
  |----------------|------------------|---------------|
  | 0              | 16 KB            | Valid         |
  | 1 KB           | 17 KB            | Valid         |
  | 3000 (decimal) | 19,384 (hex: 4BC8)| Valid       |
  | 4400           | —                | **Fault**     |

##### *Why Bounds Check is Critical*:  
- Prevents processes from accessing other processes’ memory or OS kernel space.  
- Without it, a malicious process could overwrite the trap table (Slide 12), hijacking system control.  

---

### **III. Detailed Example: Process Execution with Base-and-Bounds**  
#### **A. Setup (Figure 15.1 & Slide 4)**  
| Component          | Address Space Location | Value/Description                     |
|--------------------|------------------------|---------------------------------------|
| Code Section       | Starts at virtual address `0x80` (128) | Instruction sequence: `movl`, `addl`, `movl` |
| Variable `x`       | Virtual address `0x3C80` (15 KB)      | Initial value = 3000                  |

#### **B. Execution Flow**  
*(Tracing the assembly code from Slide 3)*:  
```assembly
128: movl 0x0(%ebx), %eax   ; Load x into eax (virtual address 15 KB)
132: addl $0x03, %eax       ; Increment by 3
135: movl %eax, 0x0(%ebx)  ; Store back to x
```
##### *Hardware Translation During Execution*:  
| Step                          | Virtual Address | Base (32 KB = 32768) | Physical Address |
|-------------------------------|-----------------|----------------------|------------------|
| Fetch instruction at `128`    | 128             | +32,768              | **32,896**       |
| Load from virtual address `15 KB` (15,360) | 15,360      | +32,768              | **48,128** (≈47 KB) |
| Store back to same location   | 15,360          | +32,768              | **48,128**       |

> 💡 *Key Insight*: The process thinks it’s using addresses `0`–`16KB`, but hardware maps these to physical memory starting at `32 KB`.  

---

### **IV. Hardware Requirements for Base-and-Bounds (Slide 9)**  
| Requirement                          | Purpose                                                                 |
|--------------------------------------|-------------------------------------------------------------------------|
| **Privileged Mode**                  | OS runs in kernel mode; user processes run in user mode (prevents tampering with base/bounds) |
| **Base/Bound Registers per CPU**     | Hardware registers for address translation and bounds checks            |
| **Address Translation Circuitry**    | Adds `base` to virtual addresses on-the-fly                             |
| **Privileged Instructions**          | OS-only instructions to modify base/bounds (e.g., `set_base()`)         |
| **Exception Handling Mechanism**     | Triggers when illegal access occurs → terminates process                |

#### *Why Privileged Mode is Non-Negotiable*  
- If a user program could change the base register, it would:  
  - Access other processes’ memory.  
  - Bypass bounds checks entirely.  
  > *"Imagine the havoc..."* (Slide 7) — This is why hardware enforces privilege separation.

---

### **V. Operating System Responsibilities**  
#### *Critical Tasks for OS Implementation (Slide 10)*:  

| Task                          | How It’s Implemented                                                                 |
|-------------------------------|------------------------------------------------------------------------------------|
| **Memory Allocation**         | Uses a `free list` (list of unused memory blocks) to find space for new processes.   |
| **Process Termination**       | Reclaims all allocated memory → adds it back to the free list.                      |
| **Context Switching**         | Saves/Restores base/bounds registers in process control block (PCB).                |
| **Exception Handling**        | Installs handlers for illegal accesses (e.g., terminates offending process).         |

#### *Example: Context Switch Workflow*  
1. OS stops Process A → saves its `base` and `bounds` values to PCB(A).  
2. OS loads base/bounds from PCB(B) into CPU registers.  
3. Resumes Process B with new memory mappings (transparent to the process!).  

> ⚠️ *Critical Insight*: The process never knows it was relocated — this is **transparency** in action.

---

### **VI. Limitations of Base-and-Bounds & Path Forward**  
#### **A. Internal Fragmentation Problem**  
- **Cause**: Fixed-size memory slots (e.g., 16 KB) waste space between allocated processes.  
- *Example*: Process uses only `8 KB` in a `16 KB` slot → wasted `8 KB`.  
  > *(Figure 15.2: Physical memory shows unused gaps at 16–32 KB and 48–64 KB)*  

#### **B. Why This Matters**  
- Limits system scalability (fewer processes can run concurrently).  
- Motivates the need for more advanced techniques like **segmentation** (next chapter) to avoid fragmentation.

---

### **VII. Key Concepts & Definitions**  
| Term                          | Definition                                                                 |
|-------------------------------|----------------------------------------------------------------------------|
| **Virtual Address Space**     | The memory layout a process *thinks* it has (addresses `0`–`size`).         |
| **Physical Memory**           | Actual RAM locations where data resides (`32 KB`, `48 KB`, etc.).          |
| **Dynamic Relocation**        | Hardware-based address translation that relocates processes at runtime.     |
| **Interposition**             | Hardware/software inserting itself between a process and memory access (e.g., MMU). |
| **Free List**                 | Data structure tracking unused physical memory blocks (`[16KB-32KB]`, `[48KB-64KB]`). |

---

### **VIII. Summary of Core Principles**  
> ✅ **Efficiency**: Hardware handles translation per access (no OS overhead).  
> ✅ **Control**: Bounds register + hardware checks enforce memory isolation.  
> ✅ **Flexibility**: Processes use contiguous virtual addresses; physical layout is hidden.  
> ❌ **Limitation**: Internal fragmentation wastes memory → requires next-level techniques.  

---

### **IX. Study Questions (Exam-Prep Focus)**  
1. What are the *two* key goals of limited direct execution? How does address translation support them?  
2. Explain why a process cannot modify base/bounds registers in user mode. What would happen if it could?  
3. Given virtual address `0x400` (1 KB), base register = 64 KB, bounds = 16 KB: Is this access valid? If yes, what is the physical address?  
4. How does hardware-based translation differ from *static relocation* (software-only)? Why was static relocation abandoned?  
5. In Figure 15.2, why can’t Process A use memory at `0–16KB` even though it thinks its space starts there?  
6. What happens during a **bounds violation** exception? Describe the OS’s response step-by-step.  
7. Why is *internal fragmentation* specific to fixed-size allocation (like base-and-bounds)? Contrast with external fragmentation.  
8. How does the `free list` data structure help in memory management? Provide an example of its contents from Figure 15.2.  
9. Trace the translation for a load instruction at virtual address `0x3C80` when base = 32 KB, bounds = 16 KB. Is it valid? Why or why not?  
10. What is *transparency* in memory virtualization? How does base-and-bounds achieve it?  
11. Describe the role of privileged mode during a context switch (Slide 12).  
12. If bounds = `8 KB`, what range of virtual addresses would be valid for a process with no offset?  
13. Why is *dynamic relocation* called "dynamic"? How does this differ from static relocation?  
14. What hardware component performs address translation in base-and-bounds? Where is it located?  
15. In the homework simulation (`relocation.py`), what happens when you set `-l 0` (bounds = 0)? Why?  
16. How does internal fragmentation affect system performance? Propose one way to mitigate it *without* changing base-and-bounds.  
17. Explain why a process cannot access memory beyond its bounds using the formula `physical address = virtual + base`.  
18. What would happen if the OS forgot to save/restore base/bounds during context switching?  
19. Compare and contrast **base register** vs. **bounds register** in terms of their roles in translation and protection.  
20. Why does Slide 7 state that "the bounds register holds the *size* of the address space" (not its end)? How is this logically equivalent to holding the physical endpoint?  

---

### **X. References & Historical Context**  
- **Early Work**: Base-and-bounds was independently developed for:  
  - IBM Stretch computer (multiprogramming),  
  - MIT time-sharing systems,  
  - Burroughs B5000 (hardware-interpreted descriptors).  
- *Key Paper*: Saltzer and Schroeder’s *"Protection of Information in Computer Systems"* (CACM 1974) [SS74].  
- **Modern Relevance**: Base-and-bounds is the foundation for modern MMUs, though today’s systems use paging instead to avoid fragmentation.  

> 💡 *Final Insight*: This chapter shows how OS design balances efficiency (`hardware translation`), control (`bounds checks`), and flexibility (`transparent relocation`) — a blueprint still used in all modern operating systems.

---

## 📝 Study Guide Summary

This combined study guide contains all notes for **Week 2**, automatically generated from 6 individual files.

### Individual Files Included:

1. `20250816_lecture_2.md`
2. `20250816_textbook_cpu_sched.md`
3. `20250816_textbook_cpu_sched_lottery.md`
4. `20250816_textbook_cpu_sched_mlfq.md`
5. `20250816_textbook_vm_intro.md`
6. `20250816_textbook_vm_mechanism.md`

### Usage Tips:
- Use the table of contents above to navigate quickly
- Each section is separated by horizontal rules (`---`)
- Individual files are still available for focused study
- This combined file is perfect for comprehensive review

---
*Generated by [LectureQ](https://github.com/yourusername/lectureq) - Your AI-powered study companion* 🤖

<!-- 
Generated by LectureQ
Date: 2025-08-09 19:30:01
Model: qwen3:30b
-->

<think>
I need to create comprehensive study notes from the textbook chapter "CPU Scheduling" based on the provided slides. I'll make sure to include EVERY detail without summarizing.

Let me break down the content slide by slide and extract all information:

Slide 1:
- Introduces scheduling as a high-level policy for OS scheduler
- Mentions that scheduling has origins in operations management
- Presents "THE CRUX" question about developing scheduling policies
- Lists key assumptions about workload (which will be discussed)

Slide 2:
- Lists specific workload assumptions:
  1. Each job runs for the same amount of time.
  2. All jobs arrive at the same time.
  3. Once started, each job runs to completion.
  4. All jobs only use the CPU (no I/O).
  5. The run-time of each job is known.
- Explains that assumption 5 (knowing run-time) makes scheduler "omniscient"
- Introduces scheduling metrics, specifically turnaround time
- Defines turnaround time: Tturnaround = Tcompletion - Tarrival
- Notes that with all jobs arriving at same time, Tarrival = 0 and Tturnaround = Tcompletion
- Mentions fairness as another metric (Jain's Fairness Index)
- Introduces FIFO (First In, First Out) scheduling

Slide 3:
- Explains FIFO properties: simple and easy to implement
- Gives example with 3 jobs (A, B, C) each running for 10 seconds
- Shows average turnaround time = 20 seconds
- Relaxes assumption 1 (jobs run for same time) and shows FIFO performs poorly with different job lengths
- Example: A runs 100 seconds, B and C run 10 seconds each
- Shows convoy effect (B+79 reference)
- Explains convoy effect: short jobs get queued behind a long job
- Mentions grocery store analogy for convoy effect

Slide 4:
- Introduces Shortest Job First (SJF) as solution to FIFO problem
- Explains SJF runs shortest job first, then next shortest, etc.
- Gives example with A (100s), B (10s), C (10s)
- Shows average turnaround time improves from 110s to 50s
- Mentions SJF is optimal under certain assumptions
- Notes that SJF is non-preemptive

Slide 5:
- Discusses preemptive vs. non-preemptive schedulers
- Explains that modern schedulers are preemptive
- Introduces Shortest Time-to-Completion First (STCF) or Preemptive Shortest Job First (PSJF)
- Explains STCF runs the job with least time remaining
- Gives example where A arrives at t=0 (100s), B and C arrive at t=10 (10s each)
- Shows STCF improves average turnaround time from 103.33s to 50s
- Notes STCF is provably optimal

Slide 6:
- Introduces new metric: response time
- Defines response time: Tresponse = Tfirstrun - Tarrival
- Gives example with schedule from Slide 5 (A at t=0, B and C at t=10)
- Shows response times: A=0, B=0, C=10 (average=3.33)
- Explains STCF is bad for response time
- Mentions interactive performance as a concern

Slide 7:
- Introduces Round Robin (RR) scheduling
- Explains RR runs a job for a time slice (quantum) then switches to next job
- Defines time slice as multiple of timer-interrupt period
- Gives example with 3 jobs (A, B, C) each running for 5s
- Shows SJF vs. RR with 1s time slice
- Calculates average response time: RR=1, SJF=5
- Explains trade-off with time slice length:
  - Shorter time slice = better response time
  - Too short = context switching cost dominates
- Mentions context switching cost includes cache, TLB, branch predictor flushing

Slide 8:
- Explains that RR is bad for turnaround time (worse than FIFO)
- Gives example: A, B, C each run for 5s, RR with 1s time slice finishes at 13, 14, 15 (avg=14)
- Explains the trade-off between fairness and metrics:
  - Unfair = better turnaround time (run short jobs to completion)
  - Fair = better response time (evenly divide CPU)
- Mentions "You can't have your cake and eat it too" as a saying

Slide 9:
- Discusses incorporating I/O into scheduling
- Explains that when a job issues I/O, it's blocked and scheduler should run another job
- Gives example with two jobs:
  - A: runs 10ms, does I/O (10ms), repeats (5 times)
  - B: runs 50ms without I/O
- Shows poor resource use in Figure 7.8 (A's I/O time wasted)
- Shows better resource use in Figure 7.9 (overlap of CPU and I/O)
- Explains that scheduler treats each CPU burst as a job

Slide 10:
- Discusses the problem of not knowing job lengths (assumption 5)
- Explains that OS usually doesn't know job lengths
- Mentions that we need a scheduler that behaves like SJF/STCF without knowing job lengths
- Mentions incorporating RR ideas for good response time

Slide 11:
- Summarizes two families of schedulers:
  - First family: runs shortest job remaining (optimizes turnaround time)
  - Second family: alternates between all jobs (optimizes response time)
- Notes the inherent trade-off
- Mentions incorporating I/O
- States that the fundamental problem is OS can't see into future
- Introduces multi-level feedback queue as solution for next chapter

Slide 12:
- Lists references with brief descriptions

Slide 13:
- Includes homework/simulation details and questions

Now I'll create comprehensive study notes following all the instructions, making sure to include every detail without summarizing.

I'll structure it with proper headers, bullet points, code blocks for formulas, and tables where appropriate. I'll make sure to include all examples, definitions, metrics, and explanations.
</think>

# Week 2: CPU Scheduling - Comprehensive Study Notes

## Chapter Introduction

Scheduling is a high-level policy employed by an OS scheduler that determines the order in which processes are executed. While low-level mechanisms like context switching should be understood (if not, review previous chapters), this chapter focuses on the high-level policies that govern scheduling.

The origins of scheduling predate computer systems, drawing from operations management and applied to computers. Early approaches were taken from fields like assembly lines, where similar concerns about efficiency exist.

**THE CRUX: HOW TO DEVELOP SCHEDULING POLICY**  
*How should we develop a basic framework for thinking about scheduling policies? What are the key assumptions? What metrics are important? What basic approaches have been used in the earliest computer systems?*

---

## 7.1 Workload Assumptions

Before exploring scheduling policies, we must make simplifying assumptions about the processes (sometimes called jobs) running in the system. These assumptions collectively form the "workload" and are critical for developing policies.

**The workload assumptions we make are:**

1. Each job runs for the same amount of time.
2. All jobs arrive at the same time.
3. Once started, each job runs to completion.
4. All jobs only use the CPU (i.e., they perform no I/O).
5. The run-time of each job is known.

**Important Note:**  
*We said many of these assumptions were unrealistic, but just as some animals are more equal than others in Orwell's Animal Farm [O45], some assumptions are more unrealistic than others in this chapter. In particular, it might bother you that the run-time of each job is known: this would make the scheduler omniscient, which, although it would be great (probably), is not likely to happen anytime soon.*

---

## 7.2 Scheduling Metrics

To compare different scheduling policies, we need a scheduling metric—a measure used to evaluate performance.

**Primary Metric: Turnaround Time**  
The turnaround time of a job is defined as the time at which the job completes minus the time at which the job arrived in the system. More formally:

```
Tturnaround = Tcompletion - Tarrival (7.1)
```

*Because we have assumed that all jobs arrive at the same time, for now Tarrival = 0 and hence Tturnaround = Tcompletion. This fact will change as we relax the aforementioned assumptions.*

**Additional Metric: Fairness**  
Fairness is measured (for example) by Jain's Fairness Index [J91]. Performance and fairness are often at odds in scheduling; a scheduler may optimize performance but at the cost of preventing a few jobs from running, thus decreasing fairness.

*This conundrum shows us that life isn't always perfect.*

---

## 7.3 First In, First Out (FIFO)

The most basic algorithm is **First In, First Out (FIFO)** scheduling, also known as First Come, First Served (FCFS).

**Properties of FIFO:**
- Clearly simple and easy to implement
- Works pretty well under our initial assumptions

**Example with Equal Job Lengths:**  
Imagine three jobs arrive in the system, A, B, and C, at roughly the same time (Tarrival = 0). Assume A arrived just before B, which arrived just before C. Each job runs for 10 seconds.

```
Time: 0 20 40 60 80 100 120
      ABC
```

*Figure 7.1: FIFO Simple Example*

- A finishes at 10
- B finishes at 20
- C finishes at 30

Average turnaround time = (10 + 20 + 30) / 3 = 20

**Relaxing Assumption 1 (Jobs Run for Different Amounts of Time):**  
Now assume A runs for 100 seconds while B and C run for 10 each.

```
Time: 0 20 40 60 80 100 120
      ABC
```

*Figure 7.2: Why FIFO Is Not That Great*

- A finishes at 100
- B finishes at 110
- C finishes at 120

Average turnaround time = (100 + 110 + 120) / 3 = 110 seconds

**Convoy Effect:**  
This problem is generally referred to as the **convoy effect** [B+79], where a number of relatively-short potential consumers of a resource get queued behind a heavyweight resource consumer.

*This scheduling scenario might remind you of a single line at a grocery store and what you feel like when you see the person in front of you with three carts full of provisions and a checkbook out; it's going to be a while.*

**TIP: THE PRINCIPLE OF SJF**  
*Shortest Job First represents a general scheduling principle that can be applied to any system where the perceived turnaround time per customer (or, in our case, a job) matters. Think of any line you have waited in: if the establishment in question cares about customer satisfaction, it is likely they have taken SJF into account. For example, grocery stores commonly have a "ten-items-or-less" line to ensure that shoppers with only a few things to purchase don't get stuck behind the family preparing for some upcoming nuclear winter.*

---

## 7.4 Shortest Job First (SJF)

**Definition:**  
Shortest Job First (SJF) runs the shortest job first, then the next shortest, and so on.

**Example with Different Job Lengths:**  
Three jobs (A, B, C), where A runs for 100 seconds while B and C run for 10 each.

```
Time: 0 20 40 60 80 100 120
      BC A
```

*Figure 7.3: SJF Simple Example*

- B finishes at 10
- C finishes at 20
- A finishes at 120

Average turnaround time = (10 + 20 + 120) / 3 = 50 seconds

*This is more than a factor of two improvement over FIFO (110 seconds).*

**Note:**  
*Given our assumptions about jobs all arriving at the same time, we could prove that SJF is indeed an optimal scheduling algorithm. However, you are in a systems class, not theory or operations research; no proofs are allowed.*

---

## 7.5 Shortest Time-to-Completion First (STCF)

**Definition:**  
STCF (also known as Preemptive Shortest Job First or PSJF [CK68]) adds preemption to SJF.

**How STCF Works:**  
Any time a new job enters the system, the STCF scheduler determines which of the remaining jobs (including the new job) has the least time left, and schedules that one.

**Example with Late Arrivals:**  
Assume A arrives at t=0 and needs to run for 100 seconds, whereas B and C arrive at t=10 and each need to run for 10 seconds.

*With pure SJF (non-preemptive):*

```
Time: 0 20 40 60 80 100 120
      A BC[B,C arrive]
```

*Figure 7.4: SJF With Late Arrivals From B and C*

- A finishes at 100
- B finishes at 110
- C finishes at 120

Average turnaround time = (100 + (110 - 10) + (120 - 10)) / 3 = 103.33 seconds

*With STCF (preemptive):*

```
Time: 0 20 40 60 80 100 120
      ABC A[B,C arrive]
```

*Figure 7.5: STCF Simple Example*

- A finishes at 120
- B finishes at 20
- C finishes at 30

Average turnaround time = ((120 - 0) + (20 - 10) + (30 - 10)) / 3 = 50 seconds

**Note:**  
*Given our new assumptions, STCF is provably optimal; given that SJF is optimal if all jobs arrive at the same time, you should probably be able to see the intuition behind the optimality of STCF.*

---

## 7.6 A New Metric: Response Time

**Definition:**  
Response time is defined as the time from when the job arrives in a system to the first time it is scheduled.

```
Tresponse = Tfirstrun - Tarrival (7.2)
```

*Some define it slightly differently, e.g., to also include the time until the job produces some kind of "response"; our definition is the best-case version of this, essentially assuming that the job produces a response instantaneously.*

**Example with STCF Schedule:**  
With A arriving at t=0, and B and C at t=10:

- Response time for A = 0
- Response time for B = 0
- Response time for C = 10

Average response time = (0 + 0 + 10) / 3 = 3.33 seconds

**Why STCF is Bad for Response Time:**  
*If three jobs arrive at the same time, for example, the third job has to wait for the previous two jobs to run in their entirety before being scheduled just once. While great for turnaround time, this approach is quite bad for response time and interactivity. Indeed, imagine sitting at a terminal, typing, and having to wait 10 seconds to see a response from the system just because some other job got scheduled in front of yours: not too pleasant.*

---

## 7.7 Round Robin

**Definition:**  
Round Robin (RR) scheduling runs a job for a time slice (sometimes called a scheduling quantum) and then switches to the next job in the run queue. It repeatedly does so until the jobs are finished. For this reason, RR is sometimes called time-slicing.

**Time Slice Requirements:**  
The length of a time slice must be a multiple of the timer-interrupt period; thus if the timer interrupts every 10 milliseconds, the time slice could be 10, 20, or any other multiple of 10 ms.

**Example with Equal Job Lengths:**  
Assume three jobs A, B, and C arrive at the same time in the system, and that they each wish to run for 5 seconds.

*With SJF (runs to completion):*

```
Time: 0 5 10 15 20 25 30
      ABC
```

*Figure 7.6: SJF Again (Bad for Response Time)*

- A response time = 0
- B response time = 5
- C response time = 10
- Average response time = (0 + 5 + 10) / 3 = 5

*With RR and 1-second time slice:*

```
Time: 0 5 10 15 20 25 30
      ABCABCABCABCABC
```

*Figure 7.7: Round Robin (Good For Response Time)*

- A response time = 0
- B response time = 1
- C response time = 2
- Average response time = (0 + 1 + 2) / 3 = 1

**Trade-off with Time Slice Length:**  
- Shorter time slice = better performance under response-time metric
- Too short = context switching cost dominates overall performance

**TIP: AMORTIZATION CAN REDUCE COSTS**  
*The general technique of amortization is commonly used in systems when there is a fixed cost to some operation. By incurring that cost less often (i.e., by performing the operation fewer times), the total cost to the system is reduced. For example, if the time slice is set to 10 ms, and the context-switch cost is 1 ms, roughly 10% of time is spent context switching and is thus wasted. If we want to amortize this cost, we can increase the time slice, e.g., to 100 ms. In this case, less than 1% of time is spent context switching, and thus the cost of time-slicing has been amortized.*

**Context Switching Cost:**  
*The cost of context switching does not arise solely from the OS actions of saving and restoring a few registers. When programs run, they build up a great deal of state in CPU caches, TLBs, branch predictors, and other on-chip hardware. Switching to another job causes this state to be flushed and new state relevant to the currently-running job to be brought in, which may exact a noticeable performance cost [MB91].*

**RR Performance on Turnaround Time:**  
*RR, with a reasonable time slice, is thus an excellent scheduler if response time is our only metric. But what about our old friend turnaround time? Let's look at our example above again. A, B, and C, each with running times of 5 seconds, arrive at the same time, and RR is the scheduler with a (long) 1-second time slice. We can see from the picture above that A finishes at 13, B at 14, and C at 15, for an average of 14. Pretty awful!*

*It is not surprising, then, that RR is indeed one of the worst policies if turnaround time is our metric. Intuitively, this should make sense: what RR is doing is stretching out each job as long as it can, by only running each job for a short bit before moving to the next. Because turnaround time only cares about when jobs finish, RR is nearly pessimal, even worse than simple FIFO in many cases.*

**Trade-off Between Fairness and Metrics:**  
*More generally, any policy (such as RR) that is fair, i.e., that evenly divides the CPU among active processes on a small time scale, will perform poorly on metrics such as turnaround time. Indeed, this is an inherent trade-off: if you are willing to be unfair, you can run shorter jobs to completion, but at the cost of response time; if you instead value fairness, response time is lowered, but at the cost of turnaround time.*

**TIP: OVERLAP ENABLES HIGHER UTILIZATION**  
*When possible, overlap operations to maximize the utilization of systems. Overlap is useful in many different domains, including when performing disk I/O or sending messages to remote machines; in either case, starting the operation and then switching to other work is a good idea, and improves the overall utilization and efficiency of the system.*

---

## 7.8 Incorporating I/O

**Relaxing Assumption 4 (Jobs Perform No I/O):**  
*First we will relax assumption 4 — of course all programs perform I/O. Imagine a program that didn't take any input: it would produce the same output each time. Imagine one without output: it is the proverbial tree falling in the forest, with no one to see it; it doesn't matter that it ran.*

**Scheduler Decision When Job Initiates I/O:**  
When a job initiates an I/O request, the currently-running job won't be using the CPU during the I/O (it's blocked waiting for I/O completion). If the I/O is sent to a hard disk drive, the process might be blocked for a few milliseconds or longer. Thus, the scheduler should probably schedule another job on the CPU at that time.

**Scheduler Decision When I/O Completes:**  
When I/O completes, an interrupt is raised, and the OS runs and moves the process that issued the I/O from blocked back to the ready state. The OS could even decide to run the job at that point.

**Example with Two Jobs (A and B):**  
- A: runs for 10 ms, then issues I/O (I/O takes 10 ms), repeats (5 times)
- B: uses CPU for 50 ms, performs no I/O

*Poor Resource Use (Figure 7.8):*

```
Time: 0 20 40 60 80 100 120 140
      A A A A A B BBBB
CPU
Disk
```

*Figure 7.8: Poor Use Of Resources*

*Better Resource Use (Figure 7.9):*

```
Time: 0 20 40 60 80 100 120 140
      A A A A A B B B B B
CPU
Disk
```

*Figure 7.9: Overlap Allows Better Use Of Resources*

**How STCF Accounts for I/O:**  
*The scheduler treats each 10-ms sub-job of A as an independent job. Thus, when the system starts, its choice is whether to schedule a 10-ms A or a 50-ms B. With STCF, the choice is clear: choose the shorter one, in this case A. Then, when the first sub-job of A has completed, only B is left, and it begins running. Then a new sub-job of A is submitted, and it preempts B and runs for 10 ms. Doing so allows for overlap, with the CPU being used by one process while waiting for the I/O of another process to complete; the system is thus better utilized.*

**Key Insight:**  
*By treating each CPU burst as a job, the scheduler makes sure processes that are "interactive" get run frequently. While those interactive jobs are performing I/O, other CPU-intensive jobs run, thus better utilizing the processor.*

---

## 7.9 No More Oracle

**Relaxing Assumption 5 (Job Lengths Known):**  
*With a basic approach to I/O in place, we come to our final assumption: that the scheduler knows the length of each job. As we said before, this is likely the worst assumption we could make. In fact, in a general-purpose OS (like the ones we care about), the OS usually knows very little about the length of each job.*

**The Challenge:**  
*How can we build an approach that behaves like SJF/STCF without such a priori knowledge? Further, how can we incorporate some of the ideas we have seen with the RR scheduler so that response time is also quite good?*

---

## 7.10 Summary

*We have introduced the basic ideas behind scheduling and developed two families of approaches. The first type (SJF, STCF) optimizes turnaround time, but is bad for response time. The second type (RR) optimizes response time but is bad for turnaround. Both are bad where the other is good, alas, an inherent trade-off common in systems. We have also seen how we might incorporate I/O into the picture, but have still not solved the problem of the fundamental inability of the OS to see into the future. Shortly, we will see how to overcome this problem, by building a scheduler that uses the recent past to predict the future. This scheduler is known as the multi-level feedback queue, and it is the topic of the next chapter.*

---

## Key Terms and Definitions

| Term | Definition |
|------|------------|
| **Scheduling** | High-level policy determining the order in which processes are executed |
| **Workload** | Simplifying assumptions about processes running in the system |
| **Turnaround Time** | Time at which job completes minus time at which job arrived: Tturnaround = Tcompletion - Tarrival |
| **Response Time** | Time from when job arrives to first time it is scheduled: Tresponse = Tfirstrun - Tarrival |
| **FIFO (First In, First Out)** | Scheduling algorithm where jobs are run in the order they arrive |
| **SJF (Shortest Job First)** | Scheduling algorithm that runs the shortest job first, then next shortest |
| **STCF (Shortest Time-to-Completion First)** | Preemptive version of SJF that runs the job with least remaining time |
| **RR (Round Robin)** | Scheduling algorithm that runs each job for a fixed time slice before switching |
| **Time Slice / Quantum** | Fixed time period a job runs before being preempted in RR scheduling |
| **Convoy Effect** | Situation where short jobs wait behind a long job, causing poor performance |
| **Preemptive Scheduler** | Scheduler that can stop a running process to run another process |
| **Non-preemptive Scheduler** | Scheduler that runs each job to completion before considering new jobs |

---

## Important Figures and Diagrams

*Figure 7.1: FIFO Simple Example*  
- Shows three jobs (A, B, C) each running for 10 seconds
- A finishes at 10, B at 20, C at 30
- Average turnaround time = 20

*Figure 7.2: Why FIFO Is Not That Great*  
- Shows A (100s), B and C (10s each)
- A finishes at 100, B at 110, C at 120
- Average turnaround time = 110

*Figure 7.3: SJF Simple Example*  
- Shows B and C running before A
- B finishes at 10, C at 20, A at 120
- Average turnaround time = 50

*Figure 7.4: SJF With Late Arrivals From B and C*  
- Shows A running to completion before B and C (non-preemptive SJF)
- A finishes at 100, B at 110, C at 120
- Average turnaround time = 103.33

*Figure 7.5: STCF Simple Example*  
- Shows STCF preempting A to run B and C
- A finishes at 120, B at 20, C at 30
- Average turnaround time = 50

*Figure 7.6: SJF Again (Bad for Response Time)*  
- Shows SJF running jobs to completion
- A response time = 0, B = 5, C = 10
- Average response time = 5

*Figure 7.7: Round Robin (Good For Response Time)*  
- Shows RR with 1-second time slice
- A response time = 0, B = 1, C = 2
- Average response time = 1

*Figure 7.8: Poor Use Of Resources*  
- Shows poor resource utilization with A's I/O time wasted
- A runs for 10ms, does I/O (10ms), repeats 5 times
- B runs for 50ms without I/O

*Figure 7.9: Overlap Allows Better Use Of Resources*  
- Shows better resource utilization with overlap
- CPU used while A is doing I/O
- Better overall system utilization

---

## References

| Reference | Description |
|-----------|-------------|
| [B+79] | "The Convoy Phenomenon" by M. Blasgen, J. Gray, M. Mitoma, T. Price. ACM Operating Systems Review, 13:2, April 1979. First reference to convoys in databases and OS. |
| [C54] | "Priority Assignment in Waiting Line Problems" by A. Cobham. Journal of Operations Research, 2:70, pages 70–76, 1954. Pioneering paper on using SJF approach in scheduling machine repair. |
| [K64] | "Analysis of a Time-Shared Processor" by Leonard Kleinrock. Naval Research Logistics Quarterly, 11:1, pages 59–73, March 1964. First reference to round-robin scheduling algorithm. |
| [CK68] | "Computer Scheduling Methods and their Countermeasures" by Edward G. Coffman and Leonard Kleinrock. AFIPS '68 (Spring), April 1968. Early introduction to basic scheduling disciplines. |
| [J91] | "The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling" by R. Jain. Interscience, New York, April 1991. Standard text on computer systems measurement. |
| [O45] | "Animal Farm" by George Orwell. Secker and Warburg (London), 1945. Allegorical book about power and corruption. |
| [PV56] | "Machine Repair as a Priority Waiting-Line Problem" by Thomas E. Phipps Jr., W. R. Van Voorhis. Operations Research, 4:1, pages 76–86, February 1956. Follow-on work generalizing SJF approach to machine repair. |
| [MB91] | "The effect of context switches on cache performance" by Jeffrey C. Mogul, Anita Borg. ASPLOS, 1991. Study on how cache performance is affected by context switching. |
| [W15] | "You can't have your cake and eat it" by Authors: Unknown. Wikipedia (as of December 2015). Discusses similar idioms in other languages. |

---

## Homework (Simulation)

**Program:** `scheduler.py`  
Allows you to see how different schedulers perform under scheduling metrics such as response time, turnaround time, and total wait time.

**Questions:**

1. Compute the response time and turnaround time when running three jobs of length 200 with the SJF and FIFO schedulers.
2. Now do the same but with jobs of different lengths: 100, 200, and 300.
3. Now do the same, but also with the RR scheduler and a time-slice of 1.
4. For what types of workloads does SJF deliver the same turnaround times as FIFO?
5. For what types of workloads and quantum lengths does SJF deliver the same response times as RR?
6. What happens to response time with SJF as job lengths increase? Can you use the simulator to demonstrate the trend?
7. What happens to response time with RR as quantum lengths increase? Can you write an equation that gives the worst-case response time, given N jobs?

---

## Review Questions

1. What are the key assumptions made about the workload in this chapter?
2. Define turnaround time and explain how it's calculated.
3. Explain the convoy effect and provide an example.
4. How does SJF differ from FIFO? What is the advantage of SJF?
5. What is STCF? How does it differ from SJF?
6. Define response time and explain why it's important for interactive systems.
7. Describe how Round Robin scheduling works and explain the trade-off with time slice length.
8. Why is RR bad for turnaround time but good for response time?
9. How should a scheduler handle I/O operations? What is the benefit of overlapping CPU and I/O?
10. Why is it unrealistic to assume the OS knows the length of each job in advance?
11. Explain the trade-off between fairness and performance metrics in scheduling.
12. What is the difference between preemptive and non-preemptive scheduling?
<!-- 
Generated by LectureQ (Enhanced Comprehensive Version)
Date: 2025-08-16 02:37:57
Model: qwen3:30b
Output Mode: Comprehensive
-->

<think>
We are creating comprehensive study notes based on the provided textbook slides (Week 2: Textbook CPU Sched). We must cover ALL content from Slide 1 to Slide 13.

Let's break down each slide and extract all concepts, theories, principles, examples, case studies, definitions, formulas, etc.

Slide 1:
- Introduction to scheduling
- Origins of scheduling (from operations management)
- The problem: How to develop a basic framework for thinking about scheduling policies?
- Key questions: key assumptions? metrics? approaches?

Slide 2:
- Workload Assumptions (simplifying assumptions):
  1. Each job runs for the same amount of time.
  2. All jobs arrive at the same time.
  3. Once started, each job runs to completion.
  4. All jobs only use the CPU (no I/O).
  5. The run-time of each job is known.

- Scheduling Metrics:
  - Turnaround Time: T_turnaround = T_completion - T_arrival
    Since all arrive at same time (T_arrival=0), then T_turnaround = T_completion.
  - Fairness (measured by Jain's Fairness Index) and the trade-off with performance.

- FIFO (First In, First Out / First Come, First Served):
  - Simple to implement.
  - Example: Three jobs A,B,C each run for 10 seconds -> average turnaround = 20s.
  - Problem when job lengths differ: convoy effect (long job blocks short ones).

Slide 3:
- Convoy Effect example: 
  - Jobs: A=100s, B=10s, C=10s
  - FIFO: A runs first -> completes at 100, then B at 110, C at 120 -> average = (100+110+120)/3 = 110s.
- Shortest Job First (SJF) principle:
  - Run the shortest job first.

Slide 4:
- SJF example with same jobs: 
  - B and C run before A -> completes at 10, then 20, then 120? Actually, let's recalc: 
    If we have three jobs: A=100s, B=10s, C=10s.
    SJF runs B (10), then C (10), then A (100) -> completion times: 10,20,120 -> average = (10+20+120)/3 = 50s.

- Note: The text says "more than a factor of two improvement" from 110 to 50.

Slide 5:
- Preemptive vs Non-preemptive schedulers.
  - Non-preemptive: run job to completion (like SJF).
  - Modern schedulers are preemptive: can stop one process and start another via context switch.
- Problem with non-preemptive SJF when jobs arrive at different times.

Slide 6:
- STCF (Shortest Time-to-Completion First) / Preemptive Shortest Job First (PSJF):
  - When a new job arrives, the scheduler checks which remaining job has the least time left and runs it.
  - Example: 
      A arrives at t=0 (100s), B and C arrive at t=10 (each 10s).
      Without preemption: STCF would run A until completion? But actually, with non-preemptive SJF we have the convoy problem. With preemptive:
        At t=0: only A -> runs.
        At t=10: B and C arrive. Now remaining time of A is 90s (if it ran for 10 seconds), but B and C are 10s each. So STCF preempts A to run B or C.
      Figure 7.5 shows the schedule.

- Turnaround times in example: 
    For jobs arriving at t=0, with B,C arriving at t=10:
        Job A: arrives at 0, completes at 120 -> turnaround = 120
        Job B: arrives at 10, first run at 10? Actually, the figure shows that from time 10 to 20 is job B (so it runs for 10 seconds) and then C. 
        So:
          A starts at 0, but gets preempted at t=10 -> so it doesn't complete until after B and C finish? Actually, the figure shows that from time 10 to 20: job B (so first run of B is at 10), then 20-30: job C. Then A runs from 30 to 130? But wait, the text says "only when they are finished would Aâ€™s remaining time be scheduled". 
        The figure caption says: TimeABC A[B,C arrive] -> meaning at t=10 (when B and C arrive), we switch.

      Actually, let's read the example again:
          Figure 7.5: STCF Simple Example
          It shows: from 0-10: A; then 10-20: B; 20-30: C; then 30-130: A? But that would be a long time for A.

      The text says: "STCF would preempt A and run B and C to completion; only when they are finished would Aâ€™s remaining time be scheduled."

      So:
        t=0 to 10: A runs (for 10 seconds) -> then at t=10, B and C arrive. Now the scheduler sees that B and C have less time left than A's remaining (90s vs 10s each). So it switches to run B for 10 seconds (t=10-20), then runs C from 20-30.
        Then at t=30, only A remains. It has been preempted and had 90 seconds of work left? Actually, we ran A for 10s so it has 90s remaining -> runs until 120 (t=30 to 120).

      Therefore:
        Completion times: 
          B completes at 20
          C completes at 30
          A completes at 120

      Turnaround for each:
          A: 120 - 0 = 120
          B: 20 - 10 = 10
          C: 30 - 10 = 20
        Average turnaround time = (120+10+20)/3 = 150/3 = 50.

      The text says: "average turnaround time: 50 seconds ((120âˆ’0)+(20âˆ’10)+(30 âˆ’10))/3" -> which is the same as above (but note: they wrote it as (120-0) for A, but actually we have to subtract arrival times).

      However, in the text: "average turnaround time: 50 seconds ((120âˆ’0)+(20âˆ’10)+(30 âˆ’10))/3" -> that's correct.

Slide 7:
- Response Time (new metric):
  - Defined as T_response = T_first_run - T_arrival.
  - Example from Figure 7.5: 
        A arrives at 0, first run at 0 -> response time=0
        B arrives at 10, first run at 10 -> response time=0? But the text says "for job A:0, for B:0, and for C:10" (average:3.33).
      Why is C's response time 10?
          Because in Figure 7.5, we see that from t=0 to 10: A runs; then at t=10, the scheduler switches to B? But wait, when does C get its first run?

      Actually, the example says "B and C arrive at time 10", so they are both in the ready queue. The STCF will choose one of them (say B) for the next slot. So:
          Job A: arrives at 0 -> runs immediately? But note: it's preempted by the arrival of B and C, but actually we run A until t=10 when they arrive.
          Then from t=10 to 20: job B (so first run for B is at 10) -> response time = 10-10=0? 
          But what about C? It arrives at 10, but the scheduler runs B next. So when does C get its turn?
          The figure shows that from t=20 to 30: job C is running (so first run of C starts at 20). Therefore:
            Response time for C = 20 - 10 = 10.

      Hence, the text says: "response time of each job: A:0, B:0, C:10" -> average= (0+0+10)/3 â‰ˆ 3.33.

- STCF is bad for response time because a new short job that arrives late might have to wait until the current long job finishes? Actually, in this case it didn't wait too much but note: if there are many jobs arriving at different times, the last one might get delayed by multiple preemptions.

Slide 8:
- Round Robin (RR):
  - Runs each job for a fixed time slice (quantum) and then switches to next.
  - Example: three jobs A,B,C each run for 5 seconds. Time slice=1 second.
      SJF would run one after another -> completion times: A at 5, B at 10, C at 15? But wait, no: in the example they say:
          Figure 7.6 (SJF): 
            A runs to completion (at 5), then B (to 10), then C (to 15) -> so response times: A=0, B=5, C=10.
          RR with time slice=1s:
            Time 0-1: A
            Time 1-2: B
            Time 2-3: C
            Then repeats until done? Actually, each job needs 5 seconds -> so it will take 15 units (each cycle of all three takes 3 seconds).
            So completion times:
                A completes at time 1+4*3 = 13? 
                Let's break down:
                  Cycle 0: [A,B,C] from t=0 to 3
                  Cycle 1: [A,B,C] from t=3 to 6
                  Cycle 2: [A,B,C] from t=6 to 9
                  Cycle 3: [A,B,C] from t=9 to 12 -> but at the end of cycle 3, A has run for 4 seconds (in cycles 0-3) and needs one more second? Actually, each job runs in a time slice.

            The figure shows "ABCABCABC" meaning:
                Time: 0-1: A; 1-2:B; 2-3:C; 
                3-4:A; 4-5:B; 5-6:C;
                ... until the last one.
                Since each job needs 5 seconds, it will take 5 cycles (each cycle has three jobs) but note: at the end of a time slice for a job that completes early? Actually, we run exactly 1 second per slot.

            So:
                A runs at t=0-1, then 3-4, then 6-7, then 9-10 -> so it finishes at 10 (because after the last run from 9 to 10, it has completed its 5 seconds? Actually, no: each time slice is one second and we need five slices for A. So:
                  t=0-1: first slice of A
                  t=3-4: second slice
                  t=6-7: third
                  t=9-10: fourth -> but wait, it needs 5? Actually, the example says each job runs for 5 seconds and time slice is 1s. So:
                    A will run at times [0,3,6,9] (each one second) and then completes at 10.

                Similarly, B: 
                  t=1-2,4-5,7-8 -> but wait, it needs 5 seconds? Actually, the figure shows "ABC" repeated until done. So:
                    A runs for 5 slices: so from time 0 to 1 (first), then next at 3,6,9 -> that's four slices and we need one more? 
                The text says in Figure 7.7: ABCABCABCABCABC -> meaning it cycles until all jobs are done.

            Actually, the figure shows:
                Time A B C
                So for job A: runs at time intervals [0-1], [3-4], [6-7], [9-10] -> that's 4 seconds? But we need 5. 
                The text says "each with running times of 5 seconds", so they must have run the last slice for each job in a way that completes.

            Let me recast: 
                Total time = (number of jobs * quantum) but actually it's more complex because when a job finishes, we don't schedule it again.

            The example says:
                In RR with 1s time slice: A finishes at 13? Actually the text says "A finishes at 13", meaning from t=0 to 13 (so 13 seconds total).

            How?
              Time 0-1: A
              Time 1-2: B
              Time 2-3: C
              Time 3-4: A -> now A has run for 2s, needs 3 more.
              Time 4-5: B -> runs again (now total 2s)
              Time 5-6: C -> runs again (total 2s)
              Time 6-7: A
              Time 7-8: B
              Time 8-9: C
              Time 9-10: A -> now A has run for 4 seconds? Then at time 10, it's the next turn but we have to see if it completes.

            Actually, after 3 cycles (each cycle of 3s) they've each run 3 times (so 3s). Now:
              Time 9-10: A -> now A has completed? No, because it needs 5 seconds. So at time 10, we have to schedule the next job.

            The text says in Figure 7.7: "ABCABCABCABCABC" meaning that after the first cycle (3s) they do another and so on until all jobs are done. Since each job requires 5 slices of 1 second, it will take 5 cycles? But note: when a job completes early, we don't schedule it again.

            Actually, for A:
              It runs at time intervals [0-1], [3-4], [6-7], [9-10] -> that's 4 seconds. Then the next slot is B (at t=10-11), then C (t=11-12). Now it's A again? But wait, at time 10, after running for one second in the last slice, does A complete? The text says "each with running times of 5 seconds", so we assume that when a job has run its full required time, it is done.

            How do they get completion at 13?
              Let me count:
                t=0-1: A (A needs 4 more)
                t=1-2: B
                t=2-3: C
                t=3-4: A (now A has run for 2s, needs 3 more) -> but wait, we have to count the time slices.

            Actually, a better way: 
              Each job requires 5 units of CPU. The scheduler runs each in turn for one unit until they complete.
              So:
                Job A completes after its 5th slice (which is at the end of the 14th second?).

            But note: the figure shows "ABC" repeated and then says "A finishes at 13". How?

            The text states: 
                In Figure 7.7, it's shown as a long string of ABCs.

            And in Slide 8: "A finishes at 13, B at 14, C at 15"

            So:
              A runs for the first time from t=0 to 1 -> then again at t=3-4 (so second run), t=6-7 (third), t=9-10 (fourth). Then it needs one more slice. The next available slot is after B and C have been scheduled? 
              After the fourth run of A, we are at time 10. Now:
                Time 10-11: B
                Time 11-12: C
                Time 12-13: A -> completes.

            So completion times:
                A: 13 (from t=12 to 13)
                B: after its fifth slice? 
                  B runs at time 1-2,4-5,7-8,10-11 -> that's four slices. Then the next slot for B would be at time 13-14? But wait, A is running from 12 to 13 and then completes. So after A finishes (at t=13), we run B: 
                  Time 13-14: B -> so B completes at 14.
                C runs at time 2-3,5-6,8-9,11-12 -> four slices? Then next slot is after A and then B? Actually:
                  After t=10 (when we ran B from 10-11), then C from 11-12. Then at time 12: A runs to completion (so ends at 13). 
                  Now the ready queue has only B and C, but note that after running for four slices each, they still need one more slice? Actually:
                    Each job needs five slices.

                So for C: it ran at t=2-3,5-6,8-9,11-12 -> that's four. Then the next slot is A (which we did from 12-13). Now after A finishes, we run B? But wait, the scheduler cycles through all ready jobs.

                Actually, at time 10: 
                  Ready queue: [A,B,C] but note that when a job runs for one slice and hasn't completed, it goes back to the end of the queue.
                So after t=9-10 (which was A), we have:
                  At t=10: B is next -> so time 10-11: B
                  Then at t=11: C -> 11-12: C
                  Then at t=12: A again? But wait, after the first run of A (t=0-1), it went back to queue. So when we get to the end of the cycle, we start over.

                Therefore:
                  Time 0-1: A -> then goes to end of queue
                  Queue becomes [B,C,A]
                  Then time 1-2: B -> queue [C,A,B]
                  Time 2-3: C -> queue [A,B,C]
                  ... and so on.

                So the order is cyclic. Therefore, for job A:
                  It runs at times: 
                    t=0 (first run), then after two others: t=3,6,9,12.
                  At time 12-13: it gets its fifth slice -> completes at 13.

                Similarly, B:
                  Runs at t=1,4,7,10,13? But wait, at time 13 we are running A until completion (so from 12 to 13) and then the next job is B. So for B: 
                    First run: t=1-2
                    Second: t=4-5
                    Third: t=7-8
                    Fourth: t=10-11
                    Fifth: t=13-14 -> completes at 14.

                C:
                  Runs at t=2,5,8,11, and then next would be at time 14? But note after B runs from 13 to 14, we run C for the fifth slice (t=14-15) -> completes at 15.

            Hence: A finishes at 13, B at 14, C at 15. Average turnaround = (13+14+15)/3 = 14.

- Trade-off between response time and turnaround:
    - RR is good for response time because jobs get a chance to run quickly.
        Response times: 
          A: first run at t=0 -> response time=0
          B: first run at t=1 -> response time=1 (if arrival at 0) but note the example says they all arrive at same time? So T_arrival=0 for all.
          C: first run at t=2 -> response time=2.
        Average = (0+1+2)/3 = 1.

    - But bad for turnaround because jobs are stretched out. 
        Turnaround times: A=13, B=14, C=15 -> average=14.

- The text says: "RR is one of the worst policies if turnaround time is our metric."

Slide 9:
- Incorporating I/O (relaxing assumption 4):
  - When a job issues an I/O request, it blocks and waits. Scheduler should run another job during that wait.
  - Example with two jobs A and B:
        Job A: runs for 10ms then does I/O (takes 10ms), repeats until done? Actually the example says:
          "A runs for 10 ms and then issues an I/O request (assume here that I/Os each take 10 ms)"
          So it has two CPU bursts of 10ms, separated by 10ms I/O.
        Job B: uses CPU for 50ms without interruption.

      Without considering I/O:
          Figure 7.8: 
            A runs first (for the first 10ms) -> then does I/O while B is running? But in this figure, it shows:
              Time 0-10: A on CPU
              Then from time 10 to 20: A doing I/O and B using CPU for 10ms? Actually, the figure has two rows: 
                CPU row: [A,A] (meaning at times 0-10 and then what?) but actually it's a timeline.

          The text says:
            Figure 7.8: Poor Use Of Resources
              Time line: A runs for first burst (0-10), then B starts running from 10 to 60? But wait, the figure shows "A A" on CPU and "B BBBB" meaning that during the I/O of A (which is at time 10-20) we run B. However, note:
                At t=0: start A -> runs until t=10.
                Then from t=10 to 20: A does I/O and B uses CPU? But then why would the figure show "B BBBB" meaning four blocks of 10ms for B (so 40ms) but we only have two bursts?

          Actually, let's read:
            Job A needs total 50ms CPU time -> so it has five 10ms bursts? But the example says: 
                "A runs for 10 ms and then issues an I/O request (assume here that I/Os each take 10 ms), whereas B simply uses the CPU for 50 ms"

            So A will have:
              Burst 1: t=0-10 -> then I/O from 10-20
              Then burst 2: t=20-30 -> I/O 30-40
              ... until five bursts.

          But the figure shows only two bursts for A? Actually, it says "A runs for 10 ms and then issues an I/O request" meaning one cycle. However, we have to assume that after the first burst, there are more.

          The text states: 
            Figure 7.8: Poor Use Of Resources -> shows:
              CPU: A (for two blocks? but it says "A A", so two bursts) and then B for four blocks.
              Disk: I/O for A during its waiting time? But the disk row has "A" at times when A is doing I/O.

          Actually, Figure 7.8 shows:
            Time line from 0 to 140 (in steps of 20).
            CPU: 
                [0-20]: two blocks for A -> meaning it ran twice? But actually the first burst was at 0-10 and then we see a second block at 20-30? The figure has "A" in time intervals that are not contiguous.

          Let me interpret Figure 7.8:
            It says: 
              Time A A A A A B BBBB
              But the timeline is labeled from 0 to 140, and each block represents a unit of time (say 20ms?).

          Actually, it's better to read as:

            CPU row: 
                From 0-20: "A" meaning that during this period A was running for two units? But the text says we have blocks.

          The figure caption says:
            Figure 7.8: Poor Use Of Resources
              Time line with events at multiples of 20ms, and each block is a time unit (so one block = 20ms).

          So in Figure 7.8:
            CPU row: 
                First two blocks (0-40) are for A? But that would be 40ms -> but we know A only runs 10ms at a time.

          Actually, the figure is simplified to show the pattern:

            Without I/O consideration (STCF without treating bursts as independent):
              The scheduler sees two jobs: 
                Job A has total CPU demand of 50ms? But actually it's broken into five 10ms bursts. However, if we don't break it up, STCF would see:
                  At time 0: choose between a job that needs 50ms (B) and one that is "a single burst" but the scheduler doesn't know about I/O -> so it might treat A as having to run for 50ms? But actually we don't have that information.

          The text says:
            Without considering I/O, STCF would choose between a job with total CPU time of 50ms (B) and one that is "a single burst" but the scheduler doesn't know about the breaks. So it might run B first? 

          Actually, in Figure 7.8: 
            It shows A running for two blocks (so 2*20=40ms?) then B runs for four blocks (80ms). But that's not matching.

          Let me read the text again:
            "Assume we are trying to build a STCF scheduler. How should such a scheduler account for the fact that A is broken up into 5 10-ms sub-jobs, whereas B is just a single 50-ms CPU demand?"

          Then it says: 
            Figure 7.8 shows poor use (A runs first then B) and Figure 7.9 shows overlap.

          In Figure 7.8:
            The timeline has for A: two blocks of "A" in the CPU row? Actually, the figure is drawn with time intervals as bars.

          We'll assume that each bar represents a fixed unit (say 10ms) and we have:

            Time [0-10]: A runs
            Then I/O for A from 10-20.
            But in Figure 7.8, the CPU row shows "A" at time intervals: 
                First block: 0-20? (but that's two units) -> no.

          The figure has a timeline with marks every 20ms and each bar is labeled by which job was running on CPU during that interval.

          So:
            Time 0-20: A runs for the first burst? But actually, it should run only 10ms. However, in this poor example, they are not breaking up the bursts so the scheduler doesn't know to switch after 10ms.

          The text says: "Clearly, just running one job and then the other without considering how to take I/O into account makes little sense."

          So Figure 7.8 is a bad schedule because:
            A runs for its first burst (say from t=0-10) but then instead of switching during I/O, it waits until after I/O? Actually, no: the scheduler should switch when A blocks.

          How it's drawn in Figure 7.8:
            CPU row: 
                [0-20]: "A" -> meaning that for two time units (say 20ms) A is running on CPU? But actually we know A only runs 10ms and then does I/O.
            Then from t=20 to 60: B runs continuously.

          This means the scheduler did not switch during A's I/O, so it wasted time when A was blocked (waiting for disk) but the CPU was idle? Actually no: in Figure 7.8, while A is doing I/O (which should be from t=10 to 20), we see that B runs on the CPU from t=10 to 60? But wait, the figure has a row for disk too.

          The text says:
            "Figure 7.8: Poor Use Of Resources
             Time A A A A A B BBBB
             CPU
             Disk"

          And Figure 7.9 shows better overlap.

          So in Figure 7.8:
            For the first time unit (0-20): 
                On CPU, job A is running -> but actually it should run only for 10ms and then block? But they are not accounting for that.
            Then from t=20 to 60: B runs on CPU.

          This means:
            From t=0 to 10: A runs (on CPU) 
            At t=10, A issues I/O -> but the scheduler doesn't switch because it's still in a "run" state? Actually, when an I/O request is issued, the process blocks and moves to blocked queue. Then the scheduler would run B.

          However, Figure 7.8 shows that during the time when A was doing I/O (which should be from t=10 to 20), we are not using the CPU for anything? But wait, in reality, while A is waiting for disk, the CPU could run other jobs. So why isn't B running?

          The figure has a row "Disk" that shows:
            Disk: 
                [0-10]: ? (but it's labeled with an 'A' meaning A was using disk? but actually at t=0-10 A is on CPU, so not doing I/O)
                Then from 10-20: the disk row has "A", meaning that during this time A is waiting for its I/O to complete.

          And in Figure 7.8, the CPU row shows no job running? But it says "B BBBB" starting at t=20? So actually:
            From t=10-20: disk busy with A's I/O -> so CPU could run B but they didn't (because of poor scheduling) and then from 20 to 60, B runs.

          This is inefficient because the CPU was idle during [10-20]? But wait, no: in reality, when a process blocks on I/O, the scheduler immediately switches. So Figure 7.8 represents a bad schedule where we didn't switch and let the CPU sit idle? Or it's just that they are not showing B running during A's I/O.

          The text says "Poor Use", so likely they mean: 
            They ran A for its first burst (0-10), then instead of switching to run another job while waiting for disk, they let the CPU sit idle until t=20? Then started B at 20. But that's not what happens in reality.

          Actually, Figure 7.9 shows:
            Time A A A A A B B B B B
            CPU: 
                [0-10]: A (first burst)
                [10-20]: ? -> but the figure has "A" for two blocks? 

          Let me read the text about Figure 7.9:
            "Figure 7.9: Overlap Allows Better Use Of Resources"

          And it says: 
            "Doing so allows for overlap, with the CPU being used by one process while waiting for the I/O of another process to complete; the system is thus better utilized."

          So in Figure 7.9:
            During A's first burst (0-10), then from 10-20: B runs on CPU? But wait, at t=10, A issues an I/O and blocks -> so scheduler switches to run B for the next time slice.

          Therefore, in Figure 7.9:
            Time 0-10: A (CPU)
            Time 10-20: B (CPU)   [while A is waiting on disk]
            Then at t=20, A's I/O completes -> so it moves to ready queue.
            Now the scheduler can run either A or B? But note that A has a new burst of CPU time.

          The figure shows:
            CPU: 
                0-10: A
                10-20: B   [but then why is there an "A" at 20?] 

          Actually, the timeline in Figure 7.9 says for CPU: "A A A A A B B B B B", meaning that each letter represents a time unit (say 10ms). So:
            Time 0-10: A
            Time 10-20: A? -> but wait, no: the text says they are showing two jobs.

          I think we should trust the description and not get bogged down in the figure. The key point is that by treating each CPU burst as a separate job (so when an I/O completes, it becomes available for scheduling), the scheduler can run other processes during the waiting time of one process.

Slide 10:
- No More Oracle: 
  - We cannot assume we know the length of jobs in advance.
  - How to build a scheduler that behaves like SJF/STCF without knowing job lengths?
    -> Use recent past (history) to predict future behavior.
  - Also, incorporate RR-like fairness for response time.

Slide 11:
- Summary: 
  Two families:
    Family 1: Run the shortest remaining job first (SJF, STCF) -> optimizes turnaround time but bad for response time.
    Family 2: Round Robin (RR) -> alternates between jobs to optimize response time but bad for turnaround.

  Trade-off: fairness vs. performance.

Slide 12:
- References

Slide 13:
- Homework questions and simulation program.

Now, we must create comprehensive study notes covering all of this.

We'll structure the notes as follows:

# Comprehensive Study Notes: CPU Scheduling (Week 2)

## I. Introduction to Scheduling
### A. The Problem Statement
[Explain why scheduling is important]

### B. Origins and Context
[How it relates to operations management, assembly lines, etc.]

## II. Workload Assumptions
### A. List of Simplifying Assumptions (with explanations)
1. Each job runs for the same amount of time.
2. All jobs arrive at the same time.
3. Once started, each job runs to completion.
4. All jobs only use the CPU (no I/O).
5. The run-time of each job is known.

### B. Critique of Assumptions
[Why they are unrealistic and how we relax them]

## III. Scheduling Metrics
### A. Turnaround Time
- Definition: T_turnaround = T_completion - T_arrival
  (with the assumption that all jobs arrive at time 0, then T_turnaround = T_completion)
- Example calculation

### B. Response Time
- Definition: T_response = T_first_run - T_arrival
- Why it matters for interactive systems.

### C. Fairness and Trade-offs
- Jain's Fairness Index (briefly mentioned as a metric).
- The trade-off between performance (turnaround time) and fairness/response time.

## IV. Scheduling Algorithms

### A. First In, First Out (FIFO)
#### 1. Description
[Simplest algorithm: run jobs in the order they arrive]

#### 2. Example with equal job lengths
- Three jobs of 10s each -> average turnaround = 20s.

#### 3. Problem when job lengths differ: Convoy Effect
- Definition and example (A=100, B=10, C=10) leading to high average turnaround time (110s).

### B. Shortest Job First (SJF)
#### 1. Description
[Run the shortest job first]

#### 2. Example with different lengths
- Jobs: A=100, B=10, C=10 -> SJF runs B then C then A.
- Turnaround times: B=10, C=20, A=120 -> average = (10+20+120)/3 = 50s.

#### 3. Optimality
[Under the given assumptions, SJF minimizes average turnaround time]

### C. Shortest Time-to-Completion First (STCF) / Preemptive SJF
#### 1. Description
[Preempt a running job if a shorter one arrives; always run the remaining shortest job.]

#### 2. Example with late arrivals
- A arrives at t=0 (100s), B and C arrive at t=10 (each 10s).
- Without preemption: FIFO would have convoy effect -> average turnaround = 103.33.
- With STCF:
    - Runs A until t=10, then switches to run B (t=10-20) and C (t=20-30), then runs A from t=30-130? 
      Actually: completion times: A at 120, B at 20, C at 30 -> average = (120+10+20)/3 = 50s.

#### 3. Optimality
[STCF is optimal for minimizing average turnaround time when jobs arrive at arbitrary times and can be preempted.]

### D. Round Robin (RR)
#### 1. Description
- Run each job for a fixed time slice (quantum), then switch to next.
- Time slice must be multiple of timer interrupt period.

#### 2. Example with equal job lengths
- Three jobs, each 5s, quantum=1s:
    - Completion times: A at 13, B at 14, C at 15 -> average turnaround = 14.
    - Response times (first run): A=0, B=1, C=2 -> average response time = 1.

#### 3. Trade-offs
- Shorter quantum improves response time but increases context switch overhead.
- Longer quantum reduces context switches but hurts responsiveness.

### E. The Role of Context Switching Cost
[How the cost of switching (context-switch) affects performance, especially for RR]

## V. Incorporating I/O Operations

### A. Why I/O Matters
- When a job issues an I/O request, it blocks and waits.
- Scheduler should run another job during this time.

### B. Treating CPU Bursts as Independent Jobs
#### 1. Example with two jobs:
   - Job A: alternates between 10ms CPU bursts and 10ms disk wait (so total for one cycle: 20ms).
   - Job B: runs continuously for 50ms.

#### 2. Poor scheduling without I/O awareness:
   - Figure 7.8: Runs A until it blocks, then waits idle? Actually, the figure shows inefficient use of CPU during disk wait (B starts late).

#### 3. Good scheduling with I/O awareness:
   - When A issues an I/O at t=10, scheduler runs B for the next time slice.
   - Then when A's I/O completes (at t=20), it gets a chance to run again.

## VI. The Oracle Problem: Unknown Job Lengths

### A. Why Knowing Job Length is Unrealistic
- In general-purpose OSes, we don't know job lengths in advance.

### B. Solution Approach
- Use recent history to predict future behavior.
- Build on RR for good response time and use prediction (like multi-level feedback queue) for turnaround.

## VII. Summary of Scheduling Families

| Algorithm Family | Goal          | Metric Optimized | Response Time | Turnaround Time |
|------------------|---------------|------------------|---------------|----------------|
| SJF/STCF         | Shortest job  | Minimize average turnaround time | Poor (long waits for short jobs arriving late) | Excellent      |
| RR               | Round-robin   | Minimize response time           | Good          | Poor           |

## VIII. Key Trade-offs and Principles

### A. The Fundamental Trade-off
- **Fairness vs. Performance**: 
  - Being fair (evenly dividing CPU among active processes on a small scale) leads to poor turnaround time.
  - Optimizing for short jobs (unfair) improves turnaround but hurts response time.

### B. Overlap Principle
- When possible, overlap operations (e.g., while one job is waiting for I/O, run another) to maximize utilization.

### C. Amortization of Costs
- Context switch cost can be amortized by increasing the quantum: 
  - Example: context-switch cost = 1ms per switch; with time slice=10ms -> 10% overhead.
  - With time slice=100ms -> ~1% overhead.

## IX. Study Questions (15-20)

We'll create a set of questions that cover all the material, including examples and concepts.

Now, let's write out the notes in markdown with the above structure.
</think>

# Comprehensive Study Notes: CPU Scheduling  
*Based on Textbook Chapter "Week 2: Cpu Sched"*

---

## I. Introduction to Scheduling
### A. Core Problem Statement
> **THE CRUX**: *How should we develop a basic framework for thinking about scheduling policies? What are the key assumptions? What metrics are important? What fundamental approaches have been used in early computer systems?*  

- **Origins**: Scheduling principles originated from operations management (e.g., assembly lines), emphasizing efficiency.
- **Key Insight**: Human endeavors like manufacturing and computing share core concerns: *resource utilization*, *efficiency*, and *fairness*.  
  > *"Assembl[y] lines... require scheduling, and many of the same concerns exist therein."*

---

## II. Workload Assumptions (Simplifying Conditions)
*All assumptions are unrealistic but serve as a starting point for analysis.*

| **Assumption** | **Description**                                                                 | **Realism Check** |
|----------------|---------------------------------------------------------------------------------|-------------------|
| 1              | Each job runs for the same amount of time.                                      | âŒ Highly Unrealistic |
| 2              | All jobs arrive at the same time.                                               | âŒ Unlikely in practice |
| 3              | Once started, each job runs to completion (non-preemptive).                     | âš ï¸ Partially Realistic (batch systems) |
| 4              | All jobs only use CPU (no I/O operations).                                      | âŒ Highly Unrealistic |
| 5              | Run-time of each job is known *a priori* ("oracle" assumption).                 | âŒ **Worst Assumption** |

> ðŸ’¡ **Critical Note**:  
> *"The run-time of each job is known: this would make the scheduler omniscient... not likely to happen anytime soon."*

---

## III. Scheduling Metrics
### A. Turnaround Time (Primary Metric)
- **Definition**:  
  \[
  T_{\text{turnaround}} = T_{\text{completion}} - T_{\text{arrival}}
  \]
- *Simplified under Assumption 2*: \(T_{\text{arrival}} = 0\) â†’ \(T_{\text{turnaround}} = T_{\text{completion}}\).
- **Goal**: Minimize average turnaround time (e.g., for batch processing).

### B. Response Time
- **Definition**:  
  \[
  T_{\text{response}} = T_{\text{first\_run}} - T_{\text{arrival}}
  \]
- *Best-case assumption*: Job produces response instantly upon first run.
- **Goal**: Minimize average response time (e.g., for interactive systems).

### C. Fairness
- Measured by **Jainâ€™s Fairness Index** [J91].
- **Trade-off with Performance**:  
  > *"Performance and fairness are often at odds... a scheduler may optimize performance but prevent some jobs from running, decreasing fairness."*

---

## IV. Scheduling Algorithms & Analysis

### A. First In, First Out (FIFO) / FCFS
#### *Description*
- Simplest algorithm: Run jobs in arrival order.
- **Non-preemptive**: Runs job to completion before switching.

#### *Example 1: Equal Job Lengths*  
| Job | Arrival Time | Runtime | Completion Time |
|-----|--------------|---------|----------------|
| A   | 0            | 10      | 10             |
| B   | 0            | 10      | 20             |
| C   | 0            | 10      | 30             |

- **Average Turnaround Time**: \(\frac{10 + 20 + 30}{3} = 20\) seconds.

#### *Example 2: Unequal Job Lengths (Convoy Effect)*  
| Job | Arrival Time | Runtime |
|-----|--------------|---------|
| A   | 0            | 100     |
| B   | 0            | 10      |
| C   | 0            | 10      |

- **FIFO Schedule**:  
  - A runs (0â€“100), then B (100â€“110), then C (110â€“120).  
  - *Turnaround*: \(A=10
SPEAKER 0
Pay attention, but hopefully, hopefully I'll get through this without sounding like a complete buffoon. That's what I'm hoping for, um, we might get there, I don't know, is it recording yet? Damn it, I already look like a buffoon. Alright, so what are we doing today? Operating systems, yeah. Um, today we're gonna be talking about scheduling, but before we do this, I guess it's probably time to do the housekeeping thing, right, where we talk briefly about things that should happen, right? So it is the end of week 2, which means that your tutorials or workshops or whatever the hell they're called, are next week, week 3. that means that your preparation for said workshops and tutorials are due. Uh, at the Sunday, Sunday night this week, right, so basically, the expectation of the preparation is that it's preparation. Right, so don't freak out if you can't do all the questions, right? That said, do all the questions, right, like attempt, right, like um. We'll be covering what's in the tutorials during the tutorials and then we'll be covering extra stuff within the tutorials, so there is a preparation component which is you are expected to do if you give a desultory effort, that is to say that you basically hand up a blank piece of paper, then you won't get marks for it. Um, and then for participation you're expected to do stuff in your actual tutorial, right? So, um, the tutor will explain what you have to do and you just take a photo of what you did and upload it to my uni and then everything should be fine. Um, assignment one is out, um, there's been a few questions online, a few people have attempted it so far. It looks like section one is the easy section, section 2 is the hard section. As a hint, um, and it's not, uh, there will be a different version of Minihell appearing at some point in time, um, that's got a few more helpful comments, um, basically, for example, um, I don't know, who here has actually looked at assignment one? Oh wow, yeah, cool. Right, so in the Minshell thing, it creates a, it's own little shell shell environment, right, so you type stuff in and you can do commands and stuff. Um, it has a prompt in the final version you need to get rid of the prompt, but there will be a comment to tell you which bit of the code you have to remove in order to get it to match the output to grade scope. Um, and that's due in week 4. I had a couple of questions about, um, Assignment 2, which is group work, 2 to 3 people, generally 3, so if you want to now try and organise with your friends to be in a group of 3, I would suggest that you do that. Alright, are there any other questions regarding the administration of operating systems? There are no other questions, everyone understands everything. If I get any other questions later, I'll be like, whoa, it was a lie. OK, what are we doing today? scheduling, not very complicated. Well, like, to describe in one word, very easy to describe in one word. There is a 2 hour lecture coming, so, um. So last week we were talking about virtualization, right, like each process thinks it has its own computer, that's basically the goal and you know. We did direct execution, so what were we doing? Direct execution is where basically the operating systems saying yes you may, I'll do it on your behalf, do some dodgy things that are really fast, but hopefully I can control them, I'll do them on your behalf so it's not really, really bad, um but you can ask me to do it for you. Um, and one of the, one of the interesting things about it was this idea that the hardware provides a lot of OS functionality, that is to say, um, In order for a lot of the things that we're going to learn in OS going forward, operating system going forward is that we need hardware to solve some of the problems. Some of the problems cannot be solved without hardware. Um, we have to have like physical things to represent things we want to actually happen, cos we try and do everything in software because we have all these processes running around, um, it's not gonna work. Alright, so what are we doing today? We're doing scheduling, OK, cool. Um, that's a summary of the schedulers we're gonna look at today. It's not scheduling's not super hard, right? Like it conceptually, right, like, um, everyone does it on a day to day basis in some way, right, like as in the fact that you are here indicates that you're at least vaguely familiar with the concept of a schedule, right, um. And if you have assignments, everyone, I, I, here's a question, when you're doing assignments, who does round robin for their assignments? Does anyone know what that means? Oh, we got we got a couple of round robins, they're like, I'll, I'll do 10 minutes of that assignment. Oh wait, oh no, no, I'll do 10 minutes of the assignment. No no. Alright, um, as far as I know, earliest deadline first, they don't have it here actually. Earliest deadline first. Who uses earliest deadline first, cause that's what I use for all of my assignments, right? Which, which one is the most due? OK, I'll work only on that one until it is due. Um, alright, uh, OK, so. You're gonna hear the word dispatch and you're gonna hear the word scheduling, basically dispatches the process of taking a process, oh God, I'm doing it again, right? It is the act of taking a process, and then running said process, right, um. And it's all the stuff that we do with, you know, saving process control box, that's what PCB stands for and user mode kernel mode, um. And then scheduling is determining which process to dispatch, right? So the immediate assumption here is that we have many processes and we have some reason or some kind of logic of things that we want to, you know, choose which one to do first. So in the classic example of the university, I have assignments, I have many assignments. Which one should I do now? And the answer is always none, right? Because I could be sleepy, alright, so um, this is the how, mechanical how, this is the conceptual when, right, so we're gonna ask when do we run these things, right? um. Basically processes exist in 3 possible states, right? Pretty simple, you can be running, that is to say that the operating system thinks you're doing stuff, you can be ready, which means you want to do stuff but aren't doing things, and you are blocked, which means that for some reason you can't do anything, right? Um. Locked is pretty easy to understand. It basically means that like you want to use IO in some way, so you're like, I would like to use the printer as a process, and now I can't use the printer, so I'm just standing there waiting, waiting. So I tell the operating system that I'm waiting by saying that I am blocked. Um, when I finish my print, you know, I, I finish with the keyboard or I finish with the mouse or I finish with the printer, I then tell the operating system that I'm ready to go and I'm I'm I'm good, but the operating system might be doing something else instead. Um, I'm sure all of you have many ready assignments, right, but if we're gonna use that analogy again, right, um, only one running at a time unless you guys are more computationally savvy than I think you are, um, or less human, I guess, right, most people are only running one assignment at a time, right, um, and some of them are blocked, right, um, for whatever reason. OK, so, um. Just some words that we need to define in order to have this conversation, right, you know, uh, so we, we did job last week, what a job is, it's like I have a job, it's some run processy bit for a bit, and that's kind of what a job is, um, but we're gonna talk about bursts and things like that, um. A little bit later on, so you know, like as a conceptual unit for the like the length of a job, what what a job's going to do, um we have a workload which is the list of jobs, we have a scheduler which is the thing that decides of the list of jobs, which job to execute, and then we have what metric. I mean, if you don't know what metric means at this point in your university careers, I, I feel very bad for you. No, um, it means to determine if something is good or bad, so we need to have, you know, like if we are going to even have the conversation about scheduling, right, we need to determine good scheduling and bad scheduling, otherwise, like why are we here, we just like just do it in any order, it doesn't really matter. OK. Oh, there we go, see, like I'm almost psychic. um, good scheduling versus bad scheduling, OK, what, uh. Uh, is this a user one? Oh, it's an it says activity. I was, I was wondering. Alright, alright, alright, so as an activity, the goal is, you're running a hospital and you have an operating room. OK, so like, Does everyone know what a hospital is? Everyone know what an operating room is? There, there are people getting their livers removed because they drank too much or something, um, and you have patients, OK, in your operating room. In like groups of 2 and 3, get together and basically say to yourself, OK, if I had said operating room and I wanted to be a computer person and I could measure things, what would I measure in order to have a good operating room, right, like one where I, I like, Uh, and, as a, as a hint, I'll give you one to start with, less dead people could be a good one, right? You know what I mean? Um, cool, so I'll give you about like 5 minutes and I'll remember to turn the recording on this time, I swear. Um, if I don't remember to turn the recording on, please someone yell at me cause I, I stuffed that up last week. I don't want to have to repeat myself. Alright, and we'll go for like. I, yeah, 5 minutes-ish, 3 minutes. Thank you. Alright, alright, alright, alright. Anyway, OK, so we have some answers. Alright, so people at home, oh my God, I have to do it again. Alright. The, actually I have the answers here. I'm just gonna go through this like this. Alright, so we have some answers. One is the time between arrival and finish, we recall as turnaround time. I, I've just got to stop pausing it while I'm this tired. Alright, uh, the other one is, um, the difference between start time, uh, the start of your treatment and the arrival time, which we call response time. Um, we also have a level where we might not want to swap things out that much, right? So this is one of the things that like, isn't really obvious with an operating system, like an operating room example where you're like, hey, hey, look at this patient, I'm gonna take the heart out. Oh wait, context switch. I'm gonna get another patient. Just hang on a second, I'll put your heart in the freezer, alright, we'll get the second patient in and we'll be like, alright, we're gonna cut your lungs out, right, like that, that's not a very realistic example, but like it can happen in operating systems. You just like pause a um thing, but you might want to try and minimise the number of those changes, right? Um, we also have the idea of wait time, right, which is separate from. You know, like it's separate from the start time or the response time and the turnaround time, um, because if you have to pause it for whatever reason, right, like maybe you're doing some kind of procedure where they have to wait. Right, I don't know what it is, what what's it, what's a useful analogy of something that, you know, I don't know, we, we need to get a blood test or something, um, uh, and we have to wait, we can't actually do any more operating. Um, the thing that they'll care about is the time that they were waiting when they could have been treated, right? Um, and then the last one that we might consider is like throughput, right, like is how many happy people do we get through at the end, right, which is the non-dead patients. Um, There's another way of thinking about it is we could say we care about resource utilisation, so we just want something to be happening at all times. We don't want the operating room to be empty ever, right, that that might be the way that we consider it. Um, and then yeah, again we have this idea of overhead and switches, we don't want to be swapping patients all the time. Um, the other one that we could consider, which is, you know, something we haven't really thought about, is this idea of fairness, right, like, like who's getting operated on when, right? Um. So you could imagine that like, The good is the left where it's kind of even nominally and the right is bad where green is dominating and blues getting not much treatment. Um, cool, alright, so that's, that's where we're at, we're like, like, you know, conceptually, we're trying to schedule things, why are we scheduling things? We're gonna schedule things to try and meet these kinds of criteria. OK. The next thing that we need to be familiar with is a Gantt chart. Who here has made a Gantt chart? We're here as a software engineer. Oh, we have software engineers, non-software engineers who use game charts, sorry, it's just software engineers, Gant touch any kind of engineer really, Gantt chart is your life once you have a job, right, um, or at least that's what your manager will think your life is, it's a G G chart, um. So basically here we have tasks and what we're gonna do is we're gonna take tasks that might arrive at particular times. So if I think of time is on the left, going to the right, you know, task one is going to take a certain amount of time, task 2 might arrive before task one is finished, task 3 might arrive before task 2 is finished, and what we're gonna do is we're gonna draw it in a big line like this, right, so we're gonna schedule ta task 1, then 2, then 3, something like that, yeah. All our solutions are gonna look something like this. Alright, so we're gonna use the term burst time to reflect how much time the process wants to do stuff, right, so we're not even really gonna be thinking in terms of like flops or operations or seconds or anything, we're just gonna talk about burst time, so we're just gonna be like this process wants to be here for 3 seconds, right? Um, we have arrival time which is when does it arrive, right, and so now we've got this thing and you know, tasks may arrive at any time, they may run for any amount of time. How do we schedule them? You know, what is the best way of doing it? OK. Um, So In the textbook, I mean, I never really thought about it this way, but in the textbook they talk about this like we start with 4 assumptions. All tasks run for a fixed time, all jobs arrive simultaneously, all jobs are CPU only and run time for each job is known, right? Um, these are some, none of these are true, right, like as in like in a real scheduling system, but when we're building algorithms we can start with like an easy example and then work out what's wrong with it and then expand on it. So this is, this is the easy example we're gonna have, right, so we have task 1, task 2, task 3, We have an algorithm called first come, first serve. What do we think first come, first serve means? Yes. It means first come, first serve, very good. Alright, it's not a very complicated algorithm. Basically, if we have a whole bunch of tasks and they all come at the same time, right, nominally we're gonna have to put them in some kind of order, so one of them is, you know, alphabetical order, I guess, you know, ABC, right, they're all the same length. They all come at the same time which is gonna do task ABC, right? Um, not very complicated. Um, OK, so what do we got here? Oh yeah, um, and then we're gonna be talking about our turnaround time, right, which is what we care about, which is gonna be the completion time minus the arrival time. So now they're all gonna arrive at the same time. Right, and I can calculate their turnaround times pretty easily, right, because I'd just be like, oh, so when did A arrive? It's arrived at time 0. When did it finish at time 5? So it turnaround time is 5. When did task B arrive at time 0? When did it finish at task at time 10, right? And so it's turnaround time is 10. Not complicated. OK, and to find the average turnaround time, I can do division. Wow. Complicated stuff, right, like we're really scratching. Everyone here did like year 3 math or year 4 math, right? OK, cool, um, alright, so. Now I'm gonna give a less good example, that is to say it's, it's a, it's a indicative example, it's good in the sense that for teaching purposes it's good, but for this algorithm it sucks, right? So I'm gonna give you this algorithm task A. Task A is gonna go for 20 seconds, task B is gonna go for 4, and task C is gonna go for 4. Task A came before task B, so we're gonna do task A first. And our turnaround times suck, right, because we've got like 20 for task A, which is fine, right, like we're expecting 20 for task A, but for task B we've got 4, and task C we've got 4, but they go for like 24 and 28 seconds, right? Alright. When you're doing assignments at university, does anyone do this where they're like, oh, I've got a tutorial. It'll take me half an hour to do. But I'll do my large assignment first. That's gonna take weeks, and I'm not gonna touch that tutorial until my large assignment is done. No one, no one operates this way. This is an insane way of operating. Um, the only reason that you would do first come, first serve is because, like, how hard is it to implement, not very, uh, OK, you gave me a list of things to do. I'll just do them in the order you gave them. Yay. Um, but it's not very good, right, and we can see that because, well, sorry, we can see it's not very good because of what's called the convoy effect, right, so you can see here you've got some, Person driving, I think an automobile on a highway, and there are all these cars behind it going, I hate you, right? And this is really just the analogy, you have this one task that's gonna take forever to do anything, and all these little tasks that are really quick and speedy, who are just sitting and waiting for the long task to complete, right? Um, and basically, if we talk about and it's problems, you know, it's dependent on the job order and big tasks can starve small tasks, so you're gonna hear that a lot, this concept of starving. So if you're a little task waiting for a big task, you're starving. Which makes it sound like a buffet, I guess, um. But yeah, the convoy effect is, I don't know, let let me draw it. OK, so you can imagine I've got all these little tasks. I've got a big task. If I put the big task first. Then all the little tasks that arrive during the big task are gonna be waiting for the big task, not a complicated idea. And that's called the convoy. For those of you who are not familiar with the concept of the word convoy, that's just what it means, it's like basically a lot of cars in a road driving. Um, so, now we go, OK, how do we get rid of this problem? We want to reduce our turnaround time. We can reduce our turnaround time by putting small jobs early, right, so if I have a tutorial, I'll do it now, and then I'll wait and do the big job later, right? Um, I don't suggest you do this in your actual university assignment, this is what most people sort of do. Um, and so we've got 3 tasks, they all arrive at the same time, A, B, and C, and we basically go, OK, well, which is the shortest one, and we do that one first, which in this case is B and then C and then A, right? And if we look at the calculation of the turnaround time, so if I go back to the other one, we saw that the average was, 24, right? But if we put. Um, the short job first, the average is 16, so we've saved on our turnaround time, so hooray, hooray. Shorter's job first is the solution to all our problems. Nothing will ever go wrong. Um, unless task B and task C arrive slightly late. Right? And so this is the this is the bit that isn't immediately obvious and you probably need to pay a little bit of attention cos it's one of those things that you just it like you'd be like, oh, I don't understand what that means, shortest job first. It says, you want a job, the shortest job goes first. How complicated could that be? Right? And he's like, except if the other task comes first, it's not, right? Like, it's not shortest job first. It still does the long task because it only counts shortest job first if they all come at the same time. Right, which really doesn't feel like shortest job first to me, it feels like nonsense. But, um, basically now we've got a situation where we're saying task A goes and then task B and C arrive, and because they came later, and because we don't have the context, like the concept of a context switch, like we can't actually switch tasks, and the the the fancy word we're gonna use here is called preemption, right, so preemption apparently means to. Pre-empt someone sort of is to interrupt, I guess, and that's why they use the word preempt, but um basically, We're not going to interrupt, we're gonna just do task A and then B and C, right, and we're gonna have a crappy, crappy, crappy, crappy turnaround time, average. OK, OK, so we've sort of dealt with the fact that the jobs don't all, Run at a fixed time, so that's cool. Alright, and now we're at the situation where we're like, OK, so the jobs are not arriving simultaneously, we need to come up with a better way of thinking about this, right? And so this is where we have the word preemptive and non-preemptive, right, so if something's non-preemptive, we don't interrupt and if something is preemptive we do interrupt. So if anyone asks you what's a preemptive algorithm and you're like it's an interruptty kind of algorithm, and you use interruptty with a Y in it in that sentence, I swear. And they will understand what you mean, and so it is actually English. Um, but yeah, you can see here it's not very complicated, right, like, again, what we're gonna do is like, oh well task B came. I guess I'll stop task A because it's really long and I'll do the short one first, right? And this is actually closer, I would say to how uni students work, right? Like if you've got a big assignment and you're a diligent student, you'd be like, I work, I'm a diligent student. Ah, I've got a tutorial, I guess I'll do that now, and then I'll go back to my big assignment, right? Like it's kind of a bit more closely matched to everyone's intuition of how to schedule things well. Oh right, what are we moving on to next? OK, then we have shortest time to completion first. OK, so what does that mean? So shortest time to completion first is, Earliest deadline first, right, which is what we call it in real-time systems, um, But basically what you're trying to do is you're trying to, you know, we need a definition of response time, but it's, it's basically what we want to get stuff that's due or we want to get stuff that's due done as quickly as we can, right? And so basically what we're gonna do is whichever one has, whichever one's gonna get finished first is gonna be the one that we're going to do, the cheapest and easiest thing to finish, right? Um, and that, you know, like I mean it's, it's a little bit different from I guess from shortest job first. It is preemptive, right? Um, but basically, you know, like, if we look at the numbers here, this is a really boring example where we're just like, OK, we're gonna do task A, then B, then C, it's pretty much all the same, you get the thing, but the main thing, the difference here is now we're calculating response time because response time is the thing that we're gonna be worried about. We want each of these tasks to start as soon as possible, and we can see here that there's a bit of a problem, right? Um, and part of the problem is that like basically task C has to wait for 20 seconds before it gets to start, right? That's, that's the problem, this is the problem that we're trying to solve. OK. Q round robin, right, and this makes sense in a way, right, depending on how long the interval window is, but you can see here that I have task A, task B, and task C. Well, actually their response time is 0, right, because basically, just coincidentally, I mean it wouldn't happen every time like this, but their response time would be a lot shorter, right, because we're basically going to give everyone a go for a while, and then another go, and then another go, right, and we're gonna swap between task A and B and C, and then swap between that task A and B and C and swap between task A and B and C. And then, yeah, so again it's not very, very complicated. The main thing with round robin, I think from a conceptual level is like how long, how, how often do you switch, right? Like, because if you switch a lot from an operating system point of view, you know, you're gonna grab stuff and get into memory and then run across and be able to put it back in memory, right, you don't want to do that too often, right? So you want to give people a certain amount of time, but you don't want to give them too much and you don't want to give them too little, cos if you give them too little, there's a lot of overhead, and if you give them too much, your response time performance will go down. your turnaround time sucks, doesn't it, right? In this case, right, they, it's really bad, right? They, they're, in fact, did I get that right? I said 26? Well, task A finishes, yeah, 26, and then the next 1, 26, and the next 126. Um, they take a really, really long time to finish, right, because they're just getting these little chunks. Um, and this could be good or bad, but they're, you know, like, There doesn't seem to be a good solution, it feels fairer in a way, it sucks for everyone, right, you know what I mean? All of them have a very similar turnaround time, rather than having some with a very short turnaround time, some with a very long one. Um, Cool, alright. So we've dealt with all of those kinds of algorithms, now we're going to talk about input output blocking, right, because this is where things start becoming a little bit more complicated. So, We've gone through our scheduling idea, right, and we we're scheduling our jobs. Now we're gonna have the situation where a process is doing stuff and then it's like, no, don't need to do anything for a while. I can take a break, I'm on holiday, I need to do something else, and it's usually to do with IO so it's usually reading something from memory or um trying to get the keyboard to do something or you're waiting on user input or something like that, and you can block, right, so you can be like, I don't need to do anything now, right? So what we're gonna have is we're gonna have these processes that are sort of on and off and on and off and on and off, and usually just as a conceptual sort of or a rule of thumb, if a process is interruptible like this, where it has a lot of little interruptions, it generally means that it doesn't do very much, right? So when you press something on the keyboard, it's not a very complicated process, right, you're just getting like a signal, right? But you are waiting for it, right? So you wait and then you do something quickly, and then you wait and then you do something quickly. And so we need to think about an algorithm that suits that kind of process. At the same time, there are other processes where you're like, excuse me, now calculate 900,000 things for me, right, which is other kinds of jobs, you know, if someone's doing a simulation or something like that, they're gonna be like, I would like all the power for all time on your computer, um, and we have to work out a way of balancing those things. OK, so we're gonna get rid of this assumption, all jobs are CPU only, OK. Here's the logic, we have a task, task one, and we've gone from being CPU only to CPU plus some IO, right, so it's gonna take, I guess, 1, no, 2 seconds of CPU, then 4 seconds of IO, then 2 seconds of CPU. And clearly we want to do this, right? Clearly, I mean this is pretty obvious, right, we just want we want the CPU to be doing stuff. Um, don't waste CPU not complicated, OK. Oh dear, alright, let's assume now we, we don't have a run time anymore. This is where stuff starts getting a little bit, a little bit nasty, cos you've got, you're like, alright, alright, I want to run all my jobs, and I don't know how long they're going to go for. And if I ask them, I, well, I, here's a, here's a question for the audience. If you let a process, If you let a process decide how long you thought it was gonna run for, how long do you think it would tell you that it was gonna run for? Any guesses? You are too trusting. That would never happen. Sorry. Anyone else? Yes. I'm actually, again, you are too trusting. Who are all these trustworthy people? I'm, I, I'm writing, OK, if Doctor Evil, right, were to write a process, right, and the operatingist would tell how long do you think you're gonna take, right, yeah. Yes, exactly, right, it would, it would say, I am going to take 0 seconds. I have all the CPU forever, yes, right, that's how it would actually operate, right? So you can't ask a process because the process will lie to you essentially because it'll be like, well, actually to hack the system, right, um. OK, uh, but yeah, normally you would want it so all your processors actually told you how long they're gonna go for, right? Um, but here we have this issue where, you know, the jobs can be a very length, they can arrive at any time, we don't know when they will finish, and different tasks have different, Um, goals, right, so. Interactive job, using the keyboard, you want response time. So when, like when you're using a computer, if you click on something and nothing happens for 3 seconds, do you get pissed off? The answer is yes. But if you run like a big long programme and it runs for 10 hours, you don't care, right? Like you're like, oh if you try and put something big on the queue, right, if it doesn't start for 2 minutes, it's gonna run 7 hours. I don't care if it doesn't start for 2 minutes, but if I, I'm clicking on something on the screen and it's not going for 3, you know, 3 seconds is about my limit, and then I'm like, my computer's broken, I want to kill myself, right? um. Alright, what do we do? OK, so we've got priority, so we gotta, we gotta have this idea of a priority, right, so this is how we introduce this idea, right, so some jobs we're gonna say are more important than other jobs, right, cos if we assume that some jobs are more important than other jobs, we can use this idea of priority to basically take all the interactive jobs to the start of the queue so that they get their job done really quickly and then leave all the batch jobs, the really long ones, to the end of the queue and say they're low priority. OK, but then all the batch jobs will pretend to be interactive jobs, they'll just say I'm an interactive job. Um, but basically this is how we're trying to get things sort of sorted out. All right, we have two jobs, same priority, job, job, job, job, job, fine. No problem. This is, this is, uh, you know, we robbing round rob it if their priority is the same, um, we just run them sort of interspersed with each other. OK, that's fine. Um, if we have a lower priority job though. What's gonna happen here, right? I have two little jobs and they're like taking up all the CPU and then the big jobs are gonna be like I am never going to start and I am never going to finish, right? Um, and so we're gonna have to think about this, right, because we can't just say, you know, low priority jobs never get to run because that also feels a little bit unfair. Um, And so the solution to this, and this is where we start getting to like, this is where it actually starts getting a little bit conceptually complicated, is you say, alright, alright, alright, what if, what if I said all the jobs are really high priority, but if they run for too long, then I tell them that their priority is lower. That way when I get a new job, it's priority is higher and I interrupt and I do the new job, and if it is actually short, it disappears and I don't care and I go back to the other old job, right? But the longer a job runs, The longer or the lower priority I'm going to consider it, right? So it's not, it's not a bad way of doing it, right, like I can just slowly over time, depending on the length, drop the priority. OK. In groups, come up with an answer to this, right? If a job can only lose priority, wait, no, that's that's a rhetorical question, Bernard, that's not an activity. Yes, good sleep. Alright, OK. Won't it eventually get ignored, right? Like so and so this is kind of the, sorry, yes, this is the problem with this, right? Is that say I do have a long job, right? If I have designed my system and I tell you, tell any individual that over time, the only thing that can happen to you is your priority goes down, right? Eventually your priority will be so low that I will never pay attention to you again in my life, right? You know, I was like, oh, the longer I know you, the less I like you, right? Um. Is that is the example, right, so. Now we're gonna have what is called a multi-level feedback queue, right, so basically the idea is we've got like Q0, Q1 and Q2, and we're gonna run jobs on each of our different queues, right, and they're gonna have different priorities, yeah, and basically we're gonna start with one of these queues and if it's empty, we'll go to the next queue and if it's empty, we'll go to the next queue. Um, Next question. Oh, this one is for the evil, the, what do you call it, let's. Is under this system, how can we be evil? Right, so let's let's think about this. I'm just gonna give you a quick description of how it's gonna work, and then in groups you're gonna try and tell me how you break the system, like as in if you're a malicious programme, you're gonna adopt a strategy. So this is how the system works. If you are in queue I can never remember, if you're in the high priority queue, which means you have a really short run time. If you run for the whole time, I'm going to reduce your priority. Yeah. That's how it works, and then you're gonna be in the the medium priority. In the medium priority queue, if you run for a reasonable amount of time, I'm gonna say you run too long and I'm gonna demote you to the low priority, and if you're in the low priority, you're gonna stay there. So the question for the audience, you discuss with your people, I'm do my little walk around and forget to turn on the recording again, um. How would you design your system if you were a process to make sure that you're always in the high priority queue? Discuss, discuss, discuss. Congratulations Bernard, you're actually, you actually, you actually turned the recording on before you started talking again. Alright, does anyone want to volunteer and answer? Oh yeah, alright. Wrong All right. More correct.

SPEAKER 1
OK, so our groups discuss 3 ways of doing it. Um, so one way, um, we're thinking we could have a programme running for a long time and then switch, turn off, and then turn back off again. So it's have a higher priority, but it's still the same programme. Another way we could have a programme that creates smaller programme that running for a short time. So basically, it's still the same programme. Another one is we have a big programme that running throughout the queue, so one with super long time running, one super short, but it's still the same programme anyway.

SPEAKER 0
You guys are really thinking outside the box actually. I, those are none of the answers, but I like all of them, they're all actually good malicious answers. What I'll do is I'll break my programme up into lots of miniature programmes. Um, anyone else got an answer? Oh.

SPEAKER 2
I thank you. Yeah, I was just thinking about um oscillating the programme between like, I'm ready to run and blocked, so.

SPEAKER 0
Congratulations, you have won nothing. But yes, that is the correct answer, right? Oh, it's sorry, all of the answers that I have received are actually technically correct, but it's the one that's like the, the easiest is what I would say, where you don't have to change your programme at all. All you do is you work out how long you're allowed to run on the operating system before it demotes your priority, and you sleep one microsecond before that happens, right? And then it's like, oh, OK, you didn't run for too long, and then you get back on the queue and you're like, ha ha, I still have time. Right? And then, but that basically is the way that you can stay at the top. Um, Runs for too long, drop priority, OK, Q1, Q2, big sad big job. Alright. So we have this problem, right, we have sad big job. That's eventually going to end up. I always forget if it's at the top or the bottom. Q0 I think is the highest priority, right? So it's gonna end up in Q2 where it's like gonna be ignored. I probably should have drawn that up the other way to make it high and low, um, but basically, we're gonna have to need a way to make sad big job get priority eventually, right? Um. Wait, why am I going on that? Yeah, I am tired. There we go. Let's, let's rewind. Sorry, let me just rewind a bit because I've obviously confused myself. How would I solve that problem? The way that I have to solve that problem if I don't want people to like, yeah, I think if someone breaks themselves up into little jobs, they almost deserve to be run quickly. Does that make sense, right? Like you can pretend to be lots of little processes, you're like, oh you can paralyse them, I can put them on different courses, everything's gonna be great, right? So like, we almost want to encourage people to do that in a way, um. So, yes, you could get around it that way, but if, if you've got a situation where you've got people who are like pretending to sleep just before the epoch, what it means is that you now need to keep track of how long that process has used your time. So basically what you do is you go, OK, I'm gonna give you 10 milliseconds and use 9, and then you, I'm gonna give you another 10 milliseconds. Oh, but you've already used 9, so now you've used 1 and I'm gonna demote your priority, right? So basically you sort of accumulate their use over time. So that's how you would solve that problem. But we've got another problem which is these big sad jobs, right, so we know that the big sad job is going to get demoted constantly until it's so irrelevant that it never gets run. How does it get back to the top of the queue, right? And the answer is, you run your high priority jobs first, you start high priority, and if you run for too long, your priority drops, but every so often, we just reset all the priorities. And it sounds really lame, right, but it actually works quite well. So basically what you can imagine is that if you're a really big job, right, you, you run and then you're like, oh I've used my time, lower priority, I've used my time, uh lower priority, I've used my time. I want time, I want time, and then eventually it'll just reset. Like why am I using the microphone and the other microphone? Alright, we're back to normal, right, so every so often you're just gonna reset all the priorities, start again, and it means if you're a really long job, basically you go like high priority, medium priority, low priority, wait, wait, wait, wait, high priority, medium priority, low priority, and eventually you get done, right? Like it even if, even if there's thousands of jobs on the um on the queue, right, and it it it it backclogs for days, you will eventually get done in a finite amount of time. You cannot wait forever under this system. Right, and so this is the glory that is the multi-level feedback cube. Um, This is just the visualisation of the priority evil, right, so you leave the tiny gap, ha ha ha ha ha, right? Yes, OK, um. This is where we have to say if you use a full allotment, not your full allotment, just a full allotment, so we need to start counting allotments, like how many allotments of, you know, the priority queue have you used? Um, and now we're into voodoo, right? Like, I, I shouldn't make fun of voodoo, it's a real religion, uh, in the, in the sense of, you know, like the, the big bad voodoo from Warcraft 3 is what, uh, you know, like we're we're gonna use orc magic, right, um. How many cus? Who knows? Who, who knows, it depends on what you're running, right? Like as in like, but we have to make this kind of decision. The way that you make these decisions is you run it and you see, right, and you experiment because these tasks are so like stupidly complicated, like as in if you've got hundreds of different processes running on a computer, you have no idea really how to calculate it, it's impossible to calculate, you just run it and see, right? You know, how long is that allotment, who knows, right? Are they all the same for each queue? Maybe your allotments for the, the high priority queue are really small and your allotments for the low priority queues are really, really big. But by how much? A factor of 2, a factor of 10, a factor of 1000, right? Um, how often do you refresh priorities? Again, it's a design decision. Like it's, it's not, it's not like there's no obvious answer to this. You can't be like, well, actually it's 1 divided by the total number of priorities and the angle to Venus, right? You know, um, it just doesn't work that way. So another idea that we have, right, like you say so, I think we've covered that's multi-level feedback cues, actually I should get. Are there any questions about multi-level feedback? Yes, OK. That's a really good question. So the question was, When you reset the priorities, does that apply equally to all cus? And the answer is, I think in practise, yes. And I don't know why, right, like as in like it might just be easier, like as in like they just went, oh well, you know, trying to make it smarter doesn't really improve your performance. But yeah, I think it's quite reasonable to ask that question where you go, OK, well maybe you just jump one priority every so often, right? Or maybe like obviously it kind of means that eventually you're gonna, maybe you'll get into some kind of oscillating pattern where you're like going up and down over time, right, um. But yeah, you could have it so that like you only jump back a certain amount or you jump back depending on how much time, you know, like how much time you've been given on the CPU, right, so you can imagine that like if I've been given absolutely no time on the CPU then maybe I go to the top of the queue because otherwise it would be unfair. So there I think there are ways that you could like nuance that algorithm. Um, but again it's a design, you know, like so, I don't know, at at a certain point it becomes black magic, and this is me explaining the basics of how they work, right, and not, not how they currently work cos if, uh, you know, like we'd be here for days kind of, you know, going, well actually I wrote a research paper on how, you know, scheduling in these systems actually operates, it becomes very, very complicated, but it's just the ideas of how it could work. Um, but yeah, that's a really good question. Uh, any other questions before I move on? Hey, excellent, alright, so now we have this idea of lottery scheduling and lottery scheduling is one of those things where you're like, computer scientists or like a certain kind of person with a certain kind of mind goes, That doesn't seem like that would be fair or would work, right? It turns out it's actually not too bad. So basically the idea is you just, you just have a random number generator and you and you have tickets and you give processors tickets depending on how high a priority they need to be, and then you just like roll the dice and you be like, you, you get to run and you get to run and you get to run, right? Um, and if you have a good random number generator and you've got a good system of, To distributing tickets, then it's actually really, really effective. And it does, it does lead to some kind of interesting design decisions where you're like maybe processors could barter. You could have a, you know, like a a process market where they're like, well actually I'll give you some of these now, if you give me some later. Right, um, and, and scalping I guess. I don't know, um, and you can have this idea of ticket inflation that you're like, is your ticket worth more overtime than it was, you know, like my old ticket becomes slowly better. I don't know, it's, it's it's, it's kind of weird, but like it's a pretty simple idea, right, like it's just like if I gave you all, you know, a ticket that said it's your turn to speak now and then some of, you know, the people I actually want to speak, I gave you 100 tickets, right, you know, all the um. Cause I, I thought for some reason you're high priority, I don't know, um, that sounds very unjust, but nonetheless, I could do it that way, right, like then you can see how that would work, you'd just be like, oh I win more often we'd feel fairer I guess if you had a lot of tickets and unfair if you didn't. Um, We also have the completely fair scheduler, right, like, I wonder what this does. It, it does, this one is a bit harder to work out what it does, right, cos you're like, OK, so what makes something completely fair? Alright, so at a at a baseline we're gonna have this idea of the V run time, so like how long have you been running, right, so we're gonna have to measure some idea of a of a run time, right? Um, and smallest runtime tasks are run with priority, that is to say, in a sense, right, if you've not run very much, then I consider you to be higher priority. Yeah, um, each task will run for a bit over a target latency period, right, so what does this mean? OK, so what we're gonna do is we're gonna pick a period of time that starts here and finishes there, and everyone is gonna get to have a go in that time, right, so that's how we're gonna do it, right? Everyone's gonna get to have a go. And some people will get more of a go than others, but over a particular period of time I can guarantee that everyone gets at least a little bit of a go, right? Now, that's good because if everyone gets a go, really low, long tasks will eventually finish in finite time, right, because if you know you're gonna get a minimum amount of go every so often, you can calculate how long it's gonna take for you to complete, even if there's a million processes on the um the system. Um, and so everyone gets a, a share depending on their weight, no starvation, and it works using a red black tree. Who here has done red black trees? Oh, excellent. Alright, I don't have to explain red black trees, um. So why are we using it? um. Basically we need to work out what the priorities are, and we can use a red black tree where basically we're just gonna pick, I'll show you, we're just gonna pick this one on the left, we're gonna pick this one because it's gonna be, you know, it's ordered, it's like a binary tree, it's ordered like a binary tree, um, and basically by being a red black tree, we can guarantee insertion and deletion times are O log in, right, so it's not too hard to put stuff in and take stuff out of the tree. Um, and a red black tree I think is like the advantage of a red black tree is essentially, um, actually, do, do people know what the advantage of a red black tree is over an AVL tree off the top of their heads? You don't actually have to answer, do you, do you, have you, if I'm not gonna ask you to answer because you uh you know, like this is kind of a bold question, but who would actually think that they could answer it? Oh right, OK, cool. Alright, so what is the difference between an AVL tree and a and a redback tree? So an AVL tree has rules that basically say your child to your left and your child to your right have to be within a certain distance of each other, right? OK, or like a certain height, right, so I can't have a really long left tree and a really short right tree. Um, and it basically applies these rules to every node, and you can sort of get that to philtre up through the tree in a kind of reasonable amount of time. Um, and that means that the tree is very, very balanced, right? You can imagine that like for any node, you know that the depth of the subtree on the left and the subtree on the right differ by no more than 1 or 2. If it differs by 2, then it has to shift, right? Red black tree is different, right, so red black tree is basically making the assumption that, You have these black nodes and they may have a red node in between, and every tree has an equal number of every subtree has an equal number of black nodes, right, but it could have be filled with red nodes or it could have no red nodes, right, which means that basically you know that no left or right subtree is going to be bigger than any other one by a factor of 2. Right, and that means that you still get like Ologn behaviour, right, rather than like ON behaviour, um, but it means it's very imbalanced because you can imagine you basically have a really deep tree here and this subtree is like half as deep, right, um, but it implements a lot more efficiently, right, so like basically. It's much easier to build a red black tree and it requires a lot less operations to run it than a ABL tree because it's not as balanced, um, but it's still balanced enough to get good behaviour, so we use red black trees to manage our priority cues. Oh, I don't want to go to journal club. I'm in the middle of a lecture, um. So, Oh yeah, yeah, yeah. So basically the idea here is that we've got two tasks, this is how the completely fair schedule works sort of at a high level, is that we've got some task that's running for a really long time, right? And it's, it's doing stuff the whole time, it's running the run time, and so it's vrun time, it's, it's used a lot. Right? Whereas the interactive task, basically, it's not accumulating the run time most of the time, it just does a little thing and then it's like, I'm not running, and then it sleeps. And it's like my vrun time is really, really small. And so this other task has a really, really big run time and so I'm allowed to interrupt, right? And then I interrupt and then I, I wait, and then I interrupt again, and then I wait and I interrupt again. That's basically how it works. It's not super complicated, um, and I don't know. Um, One of the things that's kind of fun, this is what, uh, you, it's like a Linux scheduler, by the way, so you can actually run these commands. There is actually a nice command, it's called N nice, right? Where you're like, I would like to be nice to this process, which is a little bit random, right? Like you're like, OK, uh, this is one of those points where like they, they've run out of words to use, um, but it's basically a bias where you're basically going, I would like this process to run a little bit more in the wait for the shortest, um, They're completely fair share, so you're basically changing its weight, so you can type the process number and then you give it a nice value and basically what that means is that it'll get a bigger proportion of this giant, um block, right, so it's, it's kind of like a weighted round robin. Um, Yeah, so when we're doing multi multi multi CPU, um, multi-core CPU's on a Linux machine, um, basically we're gonna have 11 of these priority queues for each CPU that we've got, well, per core, um. And now we're gonna run into trouble, right, right, because now we're going to go, oh, OK, what if I put you on the wrong call, right, like, like it's like, oh, I put you, you know, you know, who who's done this at like a, um, uh, what do you call it, uh, like a shopping centre or like when you're waiting at the airport or something like that, and you're like, there are like 6 queues and you're like, I shall pick the queue. And then there's like someone who has a complaint with the manager in front of you on the line and you're just there going, right, I picked the wrong queue. Look, 15 people have gone through that queue by the time. I should change queues. Ah, they got started, yeah, alright, I hate my life. Um, I certainly have been in that situation right when you're just like, why, why can't I get served? It's horrible. There's actually a really cool little animation they have that of that it's like um where there's a little dude, he keeps changing queues he can never get served, um. But yeah, so basically, um. It's not I'm not gonna explain it in this lecture, but you can imagine that there's a case where you've been assigned to the wrong core and that core has a lot of jobs on it and there's another core with no jobs on it, how do you get transferred back to that core? So that's something that you might want to have to think about like how you would actually try and solve that problem, right, cos it implies that all the cores have to like have some kind of system where they know what every other core is doing, and that that becomes kind of complicated, um. But yeah, I think, I think, I think we're done, right, that's first come, first serve, round robin, shortest job, first multi-level feedback cues, yeah. Any questions? Everyone understood everything, or no one understood the question, are there any questions? is the other option. But I'm gonna pause the recording and if there are any questions, you can come up and talk to me at the end. Um, just a reminder that my consultation time is now. The like, it's gonna be from 3 until the last student leaves and then I'm going to go home and sleep. Alright, so, OK. Uh, have a good weekend everyone.

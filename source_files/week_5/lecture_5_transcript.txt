SPEAKER 0
In the event that it's not, right, um. Oh yeah, Capture has not begun. There we go. Whole awkwardness, I can be avoided now cos I can just start talking about something relevant. Right, you can do it. I hope I have the right lecture actually. Capture has not begun. Capture has not begun. 1 hour and 49 minutes, there we go, OK. OK, let me just check. Is this the right lecture? Week 5. We're in week 5, right? Week 5? Yeah, all right, all right, all right, all right. So this week, apparently we are studying concurrency. That sounds Possible, yes, OK, so today we're gonna be doing like locks and condition variables. Um, for people who have done systems programming, you're gonna be here going, why am I here? What is the point of this? Um, I'm gonna go into more detail than I would have done in systems programming, um, and we're gonna go and look at a couple of more different kinds of things. But in broad terms, if you understood like the threads and locks and condition variables and systems programming. Um, you will probably understand this lecture and sort of be sitting there going, oh, revision, excellent, um. Last week we did memory virtualization and page swapping. Um, one thing that did come up, right, um, that I was not entirely clear about in the lecture, right, uh, which I was a little bit disappointed by, um, is the difference between a TLB swapping algorithm. Right, and memory swapping, right, like I, I didn't make enough of a distinction between those two things, because they are two different things. So in one case, you have your translation look aside buffer and you're deciding which bit of memory to remove, you're not actually, it's not even really memory, it's just like a cache reference where, you know, a bit of memory to remove, so that's where you get the pages and you're like, do I want to remove page 4, or what I do I want to remove page 5, that kind of question, right? You also have page replacement, right? Um. And that's where you're like, oh OK, I'm going to load things into memory, when am I gonna shove them out to disc, right, because sometimes I wanna, you know, I can only hold a certain number of things in memory, and they use basically the same algorithms, right, like as in there's a FIFO, there's a least recently used, right, and so, No, it can get a little bit confusing between the two because you know they, they've literally labelled the same thing. But usually speaking, um, if you're asked, um, you know, in a question in a formal context for like an exam or something like that, and they're asking for like a replacement algorithm, it's probably going to be a TLB replacement algorithm and it's probably gonna be 1s, 2, 3s, 4s and 5s and stuff like that, um. Whereas memory, the, the kind of question that you'd be asked about memory page replacements or, um, would be more about, you know, things like write on copy or uh dirty bits and whether it's worth, you know, can you just delete that page and not care about it because no one's changed it so it doesn't cost you anything to get rid of it, that kind of thing. Um, cool, alright, on to this week, what are we doing this week? OK, so this week we're gonna be talking about concurrency, right, two things happening at the same time, um. Yes. Things happening at the same time and not at the same time. Um, here's a fun picture. You have two CPUs. Well, I mean, we just had this question about the difference between a CPU and a and a core. I don't know what the actual difference between a CPU and a core is, right? Like, I feel like they've probably got a technical definition. That is to say that like maybe a CPU is a piece of hardware and a core is a thing inside a CPU depends on who's talking about it. Um, they're sometimes used interchangeably, but for this particular example, look at these two things and consider them as separate processing units, right? Like as in a, that's a good way of describing it, it's a separate processing unit. You can imagine that you've got two processing units and they're doing two different things at the same time, and they're connected by a bus, so they're trying to connect to RAM or this, you know, um, memory, right? And. One of the things that comes up from this is, if they're two threads, right? In one process, right, I could have 11 process that has two threads that are run on two different um two different CPUs or two different cores. Um, you've got the code, right, like you've got a code, you've got a heap, and then you have two stacks. OK, cool, so that is the like the, the, the mental space that we're gonna be operating in today. One of the things here is that uh threads in general, have access to all of this bit of memory, right, so we're gonna go back into logical space where we sort of ignore the page stuff where we've flipped everything around. We're just gonna line everything up in sort of the logical way, um. They have access to all of this stuff, OK, that seems reasonable. um, OK, now here's the fun part, here's the fun part. I think this function is probably the most boring function that you can pretty much write, give or take that does anything of substance, right? So what does this function do increment, all it does is it takes the value balance. And it sets it to balance plus 1, right, um, if this was C++ or Java, you could just even have it as like balance ++. Um, yeah, so. That's cool, now we need to think, alright. What actually happens when you do that, right, so when I write balance equals balance plus one. What the computer is actually doing is it's saying aha, there's a thing, it's called balance. I would like that bit thing called balance. OK, I'm going to grab the thing called balance, right, and I'm gonna walk it across into my CPU. OK, now I have balance. Now I'm going to add one to the balance, and now I'm going to walk it across back to where it came from, right? So. That's the process, there's 3 steps. You take the thing, you do the calculation, you turn around and you put the thing back, right? So that's what's actually happening, right? So if that's what's actually happening, we have a problem. Because if we do the same thing twice, right? Maybe at the same time, we're gonna run into trouble, right? Because the order in which we do it is going to become important. Yeah. So, Here's a question. I have 2 threads. This should be revision, right? Um. One is X equals X + 11 is I equals I + 11 is X equals X + 1, the other one is O equals 0 + 1, and it prints X and O and prints X and I. OK. What will we print it out? Yes. X is a global variable. Any guesses? Who thinks that I will equal 1 at the end of this? I'm, I'm gonna assume that they start at 0, by the way, right, so, so who, who has a vote for I will be equal to 1? Alright, we've got some votes for I equal 1, alright. Who votes that I will be equal to 1? Who thinks INO will be equal to 0? Alright, cool. Who thinks X will be equal to 2? Oh, who thinks X will be equal to what? Who disagrees with both of those stated positions. 1, OK, we've got 1 right person in the class, 2 right people in the class. Everyone else is wrong. If you thought the answer is 1 or 2, you're wrong, right? Because the answer is, who knows, right? Um, so let's go through why this is the case. OK, so what we're gonna do is we're gonna take this example, we're gonna say before, um, we're gonna do this ordering, right, of arrows, right, so basically the idea is the numbers are gonna start at 40,000, yeah. We're gonna increment it to 40,0001, and then we're gonna increment it to 40,0002, right, like so we're gonna call both of these bits of code, right? Um, and we're gonna look at how the ordering of these two bits of code running theoretically, simultaneously is going to affect the outcome of the calculation. Right, so if we go red, orange, yellow, green, blue, purple, which is gonna be the colours of the arrows, right, we're gonna get this, we're gonna go, OK, so what we're gonna do is we're gonna take some memory, 40,0001, right, we're gonna copy it into this other 1, 40,000 and 1, right, so we're gonna take 40,000 and 1, we're gonna add them together, put them in 40,00001, and then we're gonna return it to the value of X, right, so that box is X, right, so it's gone from being 40,000 to 40,0001 at the end of the yellow step, right? Now, what we're gonna do is we're gonna take this 40,0001 and 1 from somewhere, right, we're going to, Add them together, put them somewhere, make it 40,0002, and then we're going to return it to the value of X and we're gonna get 40,0002. So everyone who put up their hands and said X is equal to 2, you are sometimes right, right? Um, OK, what's the alternative? OK, well, because this this code is running at the same time, what can happen here is we can say, oh well, OK, we'll take the 40,000 and the 1, that's fine. And then the second bit of code will be like, hey, hey, I will also take the 40,000, right? So. At this point, when we're calculating X equals X + 1, because the first operation is to grab X, you can have two threads. That both grab X at the same time, before they've done any calculations. So we can see here now that like the second box, instead of being 40,000 and 1, like it was in the previous slide, right, so it was 40,0001 over here, the 5th box. Now it's 40,000. And now it doesn't really matter what the order is because it is definitely gonna stuff up. But basically now we can take the 40,000 and the 1, we put it together to get 40,000 and 1, and then, you know, we can chuck that back and that'll put down 40,000 and 1, that's fine, right? And then this one will add 40,000 to 1, get 40,000 and 1, and chuck that back, which will be 40,0001, right? So we've done 2 X equals X plus 1s, and yet at the same time, we've only incremented it once. So those of you who put up your hand and said it will be one, you are also sometimes right. Um, so this, every other concurrency problem is the same problem, give or take, right? There's a series of operations where the order or the execution of the instructions matters, right? So if you're ever asked what a like a race condition is, it is a series of instructions where the order of execution is important and changes the outcome. So if anyone ever asks you, for example, can you give me an example of a race condition in code, right, all you need to do is engineer a bit of code. Where if you change the order of the operations, the outcome will be different. So, In order for this to not happen, Functionally, what we must do. we must pretend that these three instructions happen at the same time. That is to say that there is no going in between them, right? If you can generate a bit of code where from the start to the finish, there are no extra things being done, then everything will operate as intended, right? This would guarantee, for example, in this example, that the order is this red, orange, yellow, green, blue, purple, right, because we're saying that red, orange and yellow must occur as a single unit. And then green, blue and purple must occur as a single unit, and in our case it wouldn't matter if you swapped the order, right, if I did, Um, what is it, green, blue, purple, and then I did red, orange, yellow, it would not affect the outcome. What matters is that when I do these three things, the, you know, pull the register out, add a number to the register and put it back in. Work out what the sum is and chuck it back in when. I do that operation that it occurs as a single unit, which is what we refer to as atomicity, right, which is the magic word. So if anyone ever asks you what is atomicity and you'll be like, well, atomicity is the smallest unit that something can happen. In the case of programming. What it means is a series of instructions that are considered to occur at a single time, and it requires, you know, mutexes and semaphores and all these other beautiful things to make it work. But that's basically the logic of it. Um, Yeah. And if you can. Break them apart, then it's not atomic, and if it's not atomic it won't work. OK, so. Um, for those of you who've done systems programming, this can be very repetitive because I'm going to use the same analogy. Um, who hears read Lord of the Flies? Few people, excellent, alright, um, not that many though, that's a bit disturbing. Maybe my reference is getting old. This is from like a 1980s film. Um, Lord of the Flies in the movie The Lord of the Flies, they're a bunch of children and they get, uh, marooned on an island, and they discover very quickly that when they're in a group in a meeting, um, if they all talk at once, no one is listening. And so a rule is established that in order to talk, the only person who is allowed to talk is the person with this thing called the conch shell, which is a big shell. So if you have this shell, you're allowed to talk, and if you don't have the shell, you're not allowed to talk. And this is how they resolved their ability to um communicate with each other. And we are going to use the same mechanism, the same principles, same idea, in order to ensure that two programmes that run at the same time don't interfere with each other in a destructive way. Now, obviously we want to have multiple programmes running at the same time because we have multiple, you know, CPUs and multiple cores, we want to switch between different processes, we want to have, you know, Um, different things, doing things at different times, um, but this is the mechanism that we're going to use. Every other thing that we do today and next week is going to be a variation on this thing, right? It's not becoming like there's no secret special sauce here, it's a conch shell, everything is a conch shell. Alright. OK. So, Attempt number one at a conch shell. What is attempt number one? Maybe we could write a conch shell in code, right? So here's the logic. What we're gonna say is that there is a variable called editing, yeah? Editing function, and while editing is true, it implies that someone is editing, right, so I'm gonna get stuck in a wild loop, right, so as I come into this function, someone is editing, someone else is editing, I'm gonna be stuck in the wild loop. If no one else is editing, then editing is false and I'm allowed to edit. What I will do then is I'll set editing to True to tell everyone, I am editing. And then I will do balance equals balance plus one or whatever, right? And then when I am done, I will say editing is false. And now other people can try and edit. So you can imagine another function would come in here, right in the middle of me doing my balance equals balance plus one, and it would come and it would say, oh well, the other function, they are editing, so I can do nothing. I shall go around in a wild loop forever, waiting for them to finish. And when they are finished, I will start editing. Does this work? Who thinks yes? Who thinks no? Ah, you guys are too smart. Why does it not work? Yes. Exactly, so the answer which we received from the audience is, what if two programmes went to the condition while editing at the same time? At that point in time, no one is editing. Both of those programmes would decide, oh well, no one is editing, I'm allowed to continue. Both of them would then simultaneously set editing to true. Horrible things would ensue, and then they would both set editing to false, right, and maybe not even in that order, maybe they would go, oh I will set editing to true, then I will update something and then I'll set editing to false. You could end up with all kinds of problems, it wouldn't work. So this is why I call it kicking the can down the road, because, Really, you can't do it, right? And the reason is it's because you've just changed the location where the race condition exists. Instead of the race condition being really concerned with the balance line, the race condition is now just concerned with the wild line and also coincidentally with the with the balance line, right? OK. What is the problem? We've done that. So, nominally, here's the visual explanation that we've already just had, right, so we have check conch, has conch, no conch. Check con, has conch, no conch, right, so that's the logic we're gonna say, right, like do they have it? Then you said it that I do have it and then I said that I don't have it, right, and that's, you know, process one and process 2. But if I change it to this ordering, then it stuffs up, right? So we both check the conch, right, so we both check this while loop, someone has conch, then we both set has conch and then we both set no conch, everything is broken, right? Yes, that is true. Yes, yes, yes. I have a typo. There we go. Thank you. I will update the PDF later. And then I have my little oh no guy, right? OK, so we go oh no, this is a reference that no one understands now, right? Has anyone seen this before? One, excellent. Alright, this was a meme and I don't even know what like era this was a meme from. This was an early meme. It was like lots of things and then you'd have to spend and go, oh no, um, but anyway, I will spread the meme. I will, I will, I will, I will die on that hill. um. OK, so the cons is not enough conceptually, right? Like we need, because the problem is, and, and this kind of makes sense. It's like how do you fight over the conch, right? Like as in like there's gotta be something, you know like how do I know someone else has the conch? Well, OK, I'll I'll I'll I'll grab the conch from them, right? Now in the, in the real world it does work. Now why does it work in the real world? Yeah. There is only one physical conch, right? Like as in there is no operation that we can conduct in the real world yet, where two people go for one conch and suddenly there are now two conscious, right? Like, but in computer science land that is definitely always true. There is, you can go for the same bit of data and now suddenly there are two bits of data, right, and they're both one, right? You can set 2 variables to 1 simultaneously. It's totally fine. So what we're really saying is that we need to be able to check the lock, and at the same time take the lock. We need to have some kind of mechanism that allows this, right, so we, we gotta, like, we gotta check, does anyone have the conch, and if no one has the conch, then I will take the conch, and that has to be an atomic action, right, which means that we can't write this in code, right? Like, well, technically there are ways of writing this in code in a really convoluted way, um, but we don't do this, we do this via hardware, right, so, oh yes. Oh, OK. Well, yeah, so if we check something and say it's uh if we check to go, oh, does no one else have the conch, right? And then we take it. Between us checking it and us taking it, someone else can have also checked it and then also take it, right? And then we both simultaneously take the content now we have two contras. So yeah, like, unless this, these two actions here are atomic, that that is to say, they occur as one operation, it's off, right, it's just not gonna work. Alright, so the way that we're gonna do that in C is via mutexes, right, so mutex stands for mutual exclusion, right? Um, and in code wise, you know, like we have a what's called a P thread mutex T, that's the type, it's called a lock, and then we have an initializer and the initializer is basically just a way of setting up all of the values so that it works, right, like I don't know, it's like a instructor basically. Um, and then when we're using it, we're basically gonna do this, we're gonna say P thread, mutex lock, balance equals balance plus 1, P thread, mutex unlock, and then we're gonna be good, right? So that's where we're gonna end up, right, or at least for this lecture that's where we're gonna end up, right? It's a bit of code, right? Um, and it works, right, so at any point in time you want something to be like, Untouchable Right, so you don't have two threads interfering with each other, you just need to put the same lock around each one, and no matter what, no matter what happens, they won't interact, and that's guaranteed by the hardware. Um, So an example that we might want to have a look at as as like a slightly more complicated example is that of a linked list, so you can imagine that you've got a list, we use them all the time, um. You know, you insert something to a list, you remove something from a list, you read something from a list, there's a bunch of things that you can do. In this case, what we're gonna do is consider the example of inserting, inserting and reading and and see what happens. OK. So, generally speaking, this is how we build a list and see, right, so we have our struct for a node, it's just got a number in it for its key, um, and it's got a next node, that's all the node is. So it's a note, it's a linked list, it's basically nodes pointing at other nodes. The list is just really a reference to the head of the list and then list in it is just gonna say that the list is empty because the head is not, right? So far, so good. OK, so what we're going to do is we're gonna apply an insert function, right? So we're gonna create a new node, we're gonna allocate memory for that node, we're going to say that that new node is actually real, right, so that's what the assert does, it's kind of pointless. Um, most languages you wouldn't really have to worry about this step. Um, we're then going to apply the key for that node, so put the number into this new node that we wanted to put in this integer link list, and then we're going to set the next node to the head, right? Yeah, and then we're gonna set the head to the new node, right? So, so far so good. So this, the next, where you put something in the list, the the new things next was the old head and the new head is you, right? Um, in terms of list lookup. It's pretty easy, we're going to basically get the head node and then while temp temp key is equal one, we're gonna go through it. Right, um, until we get the right key, right, so we're gonna just loop through it and then say tempers temp.ex, right? Um, and if we don't find it, we'll return 0, and if we do find it, we'll return 1. And so lookup doesn't actually return the value, which I think is quite hilarious. Um, all it does is it says is this node in this list, OK. In pairs I want you to discuss where you would break this code to make it so that it doesn't work, assuming that you've got two functions that are trying to insert simultaneously and also another function where maybe you're trying to read. Oh, and then I'm gonna pause the recording, walk around. And then wait for people to start murmuring at each other. Alright, alright, alright, alright. Wait, is this one on? OK, does anyone want to have a go at explaining where the problem is? Yes, OK, I saw a hand. And when I say the problem, I mean all the problems one.

SPEAKER 1
Yeah, right here. Well, I thought of one problem where I'm getting that feedback thing that you were talking about last time.

SPEAKER 0
Your brain like trips, right?

SPEAKER 1
So if you list insert at the same time, then one thing might be trying to set itself to the head, while the other one is also trying to do the same thing and it'll be stuck kind of like on like a new branch of the list. And it can never really be accessed.

SPEAKER 0
Yes, I like this one, so one of the um one of the clip, oh yes. That's you. Oh yes, yes. Thank you very much. Um, so yeah, one of the, one of the, one of the, the big, um, uh, well, I don't know, like flawed places where this can break, right? So we can imagine that like, um, when you try and grab something from the list, you know, like maybe you inserted it and it's in the process of inserting it and before you insert it, you like try and read it and it's not there yet. So there are a couple of places like that where it can stuff up, but yeah, this is the big one, right, so I think I have this one in pictures, so. Basically, The problem here is what we're gonna do in this bit of code is we're gonna say, oh we're gonna set the key, that's fine, we'll set the key, that's fine, we've got two new nodes, right? And what we're gonna say is new node next is L.head, and this one's gonna say new node next is L.head. So both of these new nodes think that the next node is the head, so we're already in trouble, right? Cause we've basically got some kind of hydra list, right, like it's multi-headed list, right? Um, now, obviously it can't have more than one head, right, so the order in which, you know, um, what do you call it, the head gets assigned, L head equals new node or L head equals new node is going to affect which one actually becomes the new head. But there is a moment where you've got two nodes pointing at the same location. So the idea here is I've got new node A. And it's trying to say that, OK, that's the old head, I'm going to say that's my next, so I'm the new head, and then we've got uh new node B, and it's going to point at that same node, and then this one is gonna say, oh yeah, I'm the new head, and then this one's gonna say I'm also the new head, and so node B never actually gets entered into the link list, right? Worse than that, it's gonna cause a memory leak because it's going to be created in memory, it's just never going to be referenced by anything. OK. So, how do we get around this? What we're gonna do is we're gonna create a lock in the the list structure, and we're going to init it in the in it, so that stuff's all really boring, um. So what are the critical sections? Like what, where in this code do we need to worry, right now we saw that before, we, we definitely had this thing here around like the new node.ex is Lhead and Lhead equals new node, right, like we, we said that if we break, if we sneak in there, then we're gonna have a problem. Um, that's. Nominally doesn't really, we don't need that to all be an atomic action, but that's kind of where our atomic action lives, right, so we could do it that way, where we're just like ha ha, yeah right, I'll just check it around everything, um. Would this work? Yeah, I think it would. At a break of this. Oh no. Is this breakable? All right, I'll give you guys. I'll have another go, have another chat, let's see if you can break this. All right. I just want to double check that this thing's actually still recording cause it's a little bit temperamental. Alright, do we have any takers? Does anyone want to try and guess how to break this code? Yes. Alright, alright. I shall reach Uh, this is a bit of a

SPEAKER 2
well technically answer, but I love well technical answers, basically, right, and this is from experience from systems programming where if you take two threads, split them off, and then just put one of those threads doing the list lockup in a uh forever while loop. Then it never lets go of the mutex, it never lets go of the lock, and then you can't actually insert anything into the list. So technically, you've broken it.

SPEAKER 0
Yes, good. That is the correct. Well, technically answer. This is where we're gonna break things, because if we look at it and we ask ourselves, what's gonna happen with this lock, Oh God, no no no no no no no. Oh, sorry, that was, that was about to break my eardrums, um. We look at it, how it's gonna work, the world technical answer is, OK, so I run. He thread, mutex lock, and I go into the Y loop, and then if the temp is equal to key, I'm done and I return, yeah. And now my entire program's dysfunctional for the rest of time, right, cause the lock is done, right? Um. Be afraid, is all I'm gonna say, right? Like like this was not this was not like, oh yeah, obviously this is where it's gonna break, right? Um, this is one of those, oh yeah, yeah. Ew, right, and you're like, OK, does that mean I chuck an extra line of P thread on lock in there? Mm. Sure. Probably it will probably work. I'll need some more uh squarely brackets, right? Like this is the kind of thing that I, this is why I hate this, by the way, right, like as in just with a fiery passion. Any if statement that doesn't have the squeakly brackets, like makes my brain hurt. Now in lectures it's OK because I have to make everything really, really tiny, but like this is exactly where I hate it because then you're like, oh well, I'll just put an extra line in. And then it will lock and never return. Ah, OK. OK, so number 2, what are we gonna do? Alright, that's our atomic action, that's the thing that we care about, right? So now what we can do is we can put in all the the relevant parts, we can say this is where it's got to go, for this side we can say we need to um unlock before we exit the function, otherwise we can have it locked permanently. Um and generally speaking, what you want to do whenever you're building any kind of thing like this, you want to minimise the lock, you wanna make the lock as small as is conceivably possible. OK. So now we've got an idea of how we would build one. Let's have a think about, sorry, build one, use one, not build one. We've got an idea of how we would use one, how we could write some code, right? How do we actually, how does it actually work in practise? OK, so like, like how would you actually make it, right? So. We think about our design goals. I love this how this is a design goal, is that like my answer is correct. One of my design goals is that it actually works. And it's kind of funny because like in some of these things, you know, like the design goal, that's never really explicitly stated, but yes, one of the design goals is that I don't get race conditions, right? Um, another thing that I need to worry about is deadlock. Now who here has heard of deadlock? 123, not that many, OK. We'll go through what deadlock is later. Um, we also want it to be bounded, right, so we don't want to have a situation where one thread never gets to do anything because it's always waiting for a lock, right? Um, we also want it to be fast and cheap, and we also want it to be fair, and we've made the conclusion that the software solution seems to be vaguely impossible. Now there are software solutions to this, but they are very, Uh, what's the word for it? Uh, circuitous tortured, they feel tortured. Um, OK, so I don't know who came out of that, this is not where I go when I think, oh yeah, what I could do. What I could do is I could just disable all interrupts in the computer, right? So the idea is that When I'm trying to, you know, do my critical section of code, I'm like, hey everyone. Everything belongs to me, right? The OS, the kernel can no longer preempt me because I'm just like na na na na na na na, right? Um, basically going, no, you can't actually kill me as a process, you can't tell me to pause, you can't tell me to wait, you can't tell me to do anything. I do the thing that I wanna do, right, whatever it happens to be, and then I'm like, oh yes, I'm done, you may have your resources back, right? um. Like this should be obviously dumb, right? Like as in like, oh, I just disable how the computer works in order to solve the, uh, uh, what do you call it? That said, I think some people actually did this, right? Like as in, this is why it's in the slides, is that they're like there are these weird use cases where they're like, oh actually I'm in control, I'm the OS, I'm writing something where I need, you know, synchronisation within the OS. Well, maybe I'll just tell the rest of the US to shut up, maybe that's the solution, right? And it, it does work, right? Sort of. Um, OK. Attempt #2. Butte number 2 is we have this idea of a lock, right, and we have a flag and we init the lock, whatever, right. Now, so we have the lock code, right, and we have the wild loop, right, so while the flag is true, and we've sort of kind of been through this, right, which is this idea that we do, you know, we basically have a wild loop and we're just like, oh, if someone else has got the lock, then I can't have the lock, but if we have two people ask for the lock at the same time, it's not gonna work, right? So this is a bad idea number 2, right, we've been here, spin locks. Oh yeah, sorry, and the other, the other part is that being in an infinite wild loop is a really dumb idea, right? I think everyone can agree that if I have a programme and all it does is while, it's just using CPU for literally no no reason, so it's also bad for that reason. Um, and also, yes, we can be evil, right, we can jump in here, um, using the same example that we had before, OK. Number 3, the Peterson's algorithm. OK, this is a little bit more involved, right? Basically, what we're gonna do is we're gonna have, One flag for each lock, right, so each of these bullions represents like a flag, right? Uh, sorry, one flag for each lock, is that a good way of putting it? It's sort of one, it's more one flag for each process, right? And I'm gonna initialise them to all be false, right? And the logic here is that if flag, oh sorry, I'm gonna set flag self to true, right, so I'm gonna set the flag for myself, I'm gonna be like either 0 or 1, I'm gonna set it to true, right, so I've got an array, it's got two numbers in it. If I'm process 0 as opposed to process 1, I'm going to set the first value to 1. I'll be like, I'm true, right? And then I'm gonna set this turn value to 1 minus self, and this is some convenient math because basically now what I can do is I can be like, hey, OK, 1 minus self, that's 0, right? I'm 0, so 1 minus 0 is 1. I'm gonna turn turn to 1, right? OK, so far so good. So basically what I'm gonna say is I'm gonna be like, uh, by default, you get to go first, right? So that's how that that's basically like the, the conceptual logic. So I'm gonna go, I wanna have a go, but you get to go first, right? And then you end up when, no, you have a goat. No, you have a go, right? Um. And if we go to this wild loop, basically the logic is if the turn, Why is that so loud? Um, if the turn, we'll look at the second condition first, turn is equal to 1 minus self, right? So if it is equal to 1 minus self, right, um. Then we're gonna loop. That means it's the other guy's turn, right? So in my, if I'm, I'm process 0, turn is 1, I've set turn to 1, while turn is equal to 1, so the other guys go, I'm going to, uh, spin lock, essentially just spin, right? And then the other condition is flag 1 minus self, right? So we're gonna look at the other flag, right, and if the other flag is zero, it means that the other guy doesn't want to do anything, right? And so we're allowed to go, right, and then we can go, right? So this is the basic principle behind it, right, so we basically go, I wanna have a go, right, and we go after you, right, and then if we, we're sort of sitting there going, are you doing anything, are you doing anything, are you doing? Oh, you're not doing anything, and then I go, right? There are 2 flags, one by red. Uh, you can see how this would become really awkward if you wanted to do large multi-threaded programmes, right, because you're like, I've created this one cute solution for two threads to access one thing, yes. No, because basically, basically what happens is that like, um, What happens is you're basically checking to see if the other guy goes, but there's only one turn, so it's either your turn or their turn. So if, if you go after you first, and they go, no, after you, and then you're like, oh, OK, so I can go, yeah, cool, right? And if you're the second one in that race, they go after you, you know, they say after you first, and you're like, no, after you, they're very polite, then they get to go, right, so no matter what happens, it's someone's go, right? So that's, that's why like this kind of works, it's because of this idea of turns, you know, like we're explicitly working out whose turn it is, and, If if both people want it, right, so we've set this sort of precursor thing saying, well, someone's gonna want it, right, and then setting the turn of whose turn it is, right, so this is how we can get around it, basically saying, I want to do something, and then in the future I'm allowed to do something. So the intuition is, you know, like, do, do stuff if the other thread isn't interested, the other thread gave you permission, right? There's no infinite waiting, one thread must pass, um, and what's cool about this particular algorithm for the purposes of starvation. Because you're taking turns, like you never have a situation where one thread is stuffed, right, like it's always one thread's turned, because they're always deferring to the other thread. Um, but. Right. Here's the problem, if you're running these on two different CPUs, Their definition of what turn is might not be the same. Right? Um, So there are problems here, right, like as in that there is a there is a, there, there are problems, right, um, because you might not have the same, you know, you might be loading from cache and the caches might actually be different across the different machines, right? um. And this is the other one which I found like this is, this is one of those moments in your life where you're like, I thought I understood computers, and then suddenly I'm like, oh I don't understand computers anymore. So, you know, like we, we would know, right? Um, that when you write code and you run line 1 and then run line 2, and then run line 3, that when the computer runs it, it does that, right? It runs line 1 and then line 2 and then line 3. That is not actually true. Right, like this is one of those where I like, sorry, what? Oh God, now I have to think about, you don't really have to think about this too much, um, but in multi-processor, uh, multi-threaded programmes you do have to think about this. Um, operating systems nowadays, in order to optimise, sometimes they're like, well actually if I did it in a different order, it would be a little bit faster, so I'm just gonna do that, right, um, over which you now have no control. So like, yeah, instructions may not actually be run in order and so yeah, it could also break, um. Yeah, no, I, no, at run time as well, so I compiled, yeah, at at at at at at runtime, some operating systems at least change the order of execution of the things that they're doing to like group them into similar operations essentially to optimise, right? It's like you're asking me to do a whole bunch of things. You're assuming that they're checking which operations to do. Magic hardware, I don't know. I all I know is that when I read that I was just like, sorry, what, that, that breaks my break. You're right, like the, the other place that you have is in compiler optimisation, that's another place where it could break where we change the order of operations or just say, well actually these 5 operations, I'll unwrap loops and I'll move things around just because I think it's slightly faster. But yeah, um, but in modern processes because they're running multiple threads on multiple things, sometimes they fiddle with the order a little bit. Um, and so this gives rise to the idea of a memory barrier, right? So essentially the idea is what we do is we, we lay down this fence, right? And it doesn't matter what the C code before it is, and it doesn't matter what the C code after it is, but when we write C syn sync synchronised, it means that everything before has to be done before you're allowed to continue on after sync synchronised, right? So basically it's like a a fence, nothing can cross the fence, so you just put a fence in your car and you're like this is a fence, nothing can cross, right? Um, Yes. Um Um OK, so this is the hardware solution, right? What we're gonna do is we're gonna create a function, and it's gonna be called, Exchange, but really it's test and set, right, and you might be see it called test and set. The idea is, what we're gonna do is we're gonna look at a thing, we're gonna test some condition about it, right, and then we are going to set it, right? Um, so you can see here, where is it, exchange. In exchange, right, so what we're gonna do is we're gonna go into address, so the address is the thing that we're going to change, that we're gonna have a new value, right, we're going to set the address to the old value, we're going to set, Oh sorry, we're gonna set the old value to the the address, so we're gonna take what's there, pull it out, we're going to put the new thing in, and then we are going to return the old thing, right? So the logic behind this is that basically if you can imagine, let's say it's a semaphore or let's say it's a mutex, right, let's let's say we're just, you know, using it as like a bullion, right? And we say, OK, if it's one, that means someone else is using it, right? So what I'm gonna do is I'm gonna go, hey you, I'm gonna set you to one, and I'm gonna pull out the one and then I'm gonna be like, oh you're a one. I didn't want a 1. I needed a zero cause that means someone else is using it if it's a 1, right? And then I'm gonna go into a wild and I'll do it again. I'll go, OK, I'm gonna set you to 1, and then I'll be like, OK, cool, and I grab it out and it's like, oh, you're a zero, so I can actually continue, right? Um, and that way I can sort of blindly, you know, say what I want to do with the, the value, right, but also at the same time simultaneously check whether the value is the kind of value that I want, right? And once we've established that we're going to create this function and this function's going to be an atomic action and it's going to be supported by hardware, right, then we're good, it's done, right, we've just got this base atomic unit and once we have this base atomic unit, we can't do anything, right, like pretty much, um. And so we can use it in a like, you know, standard while loop. The idea here is that we can say, oh, I want to set it to 1, but I will loop if it's still 1. So if, if I try and set it to 1 and it's already 1, I'm going to loop again until it's 0. So the only way to get out of this while loop is I set it to 1, but it was 0. and yeah, so it's, uh, becomes pretty easy and if I want to unlock it, I can just set it to 0, I don't care. Um. And then we got lock and unlock. We've got a mutex, we're done. Sweet, um, there's also a different implementation called Compare and swap, right? Um, so if we go through the code for this one, Where we compare and swap, right, and this is our old, this is how we're gonna use it, but um basically we're gonna have an address, we're gonna have what we expect it to be, um, and what we want it to be, right? And if it is what we expect, then we will update it, but either way we're going to return what it actually is or what it actually becomes. So in this case, the idea again is I wanna, I have, it's a 1 and I'm expecting it to be 0. So I go, Hey you, you must be one, right? And it goes, oh, but I am already 1, so I'm not going to change, right? And it returns 1 and then I'm like, ah, it was already 1. But if I, I throw 1 at it and I go, I'm expecting it to be zero, it's 0 and it says, ah, well I'm 0. I will set myself to 1, and I, I am, I, I, I, it's fine, right? And we're all good, right? Um. Yeah. So in this case we can use it as a lock as well, so we can say lock flag. I expect it to be zero and it's gonna be 1, and it's gonna come out as a 1. Nothing. Nothing. It's a It's just a different way of thinking about the same problem. Right, like it's, it's, it's simply a different implementation of the same idea, right, like as in what what you'll find with this mutex stuff, as long as you, like, once you have one atomic thing, then, then you, you sort of go, oh well then I can use that to construct almost anything, right? And so it was, I think this was just a case of people like, oh, test and set, I don't like that. I, it doesn't gel with how I think about the problem. Instead I'm going to show what I expect it to be and what I want it to be so that the, the check happens inside the function rather than outside the function maybe, I don't know. But um. To be honest, for me, I, I was like, you're right, it, it, it's not different. Um, Uh, I mean it is different, but it's not different in like a, a meaningful way. Alright, cool, so we've now achieved, does everyone agree we've achieved mutual exclusion, right? Like that seems like a reasonable thing that we've, we've achieved. We've we've worked out either via hardware or by like funky algorithms that only work in like very limited circumstances, um, that we can achieve correct mutual exclusion. Um, The problem is this question of fairness, right, now, so far we've sort of looked at it and gone, oh yeah, we could just do it, right, and pretty much all of us situations have been these weird wild loops, right, like we, we're just like, oh well if I can't get it, I'll try again, right, if I can't get it I'll try again. If I can't get it, I'll try again. um. I can't get it I will try again sounds very risky, right? Because it kind of implies that you could be out of time when it is available and you're like, you know, it's sort of like, oh, Uh, what? I guess Sorry, OK, I'll go with the, my first, my first, my first thought is that you're like, I need to go to the toilet. You walk to the toilet, right, and you're like, oh, it is full, and then you walk away, right? And you're like, I'll just wait a bit and not pay attention, and then I'll come back, right? And I'll be like I need to go, oh, it's full again, right, and then walk away. And then come back and then you're just constantly like this, right, because you can imagine that someone left and then came back in the time that you've, you know, done your little walk back and forth, right, so you, you're spending the whole time there just waiting for this resource, right? Um, and that's what a wild loop will do because you'll get de-scheduled by the, you know, like the CPU at some point because your time is up in the round robin or however the programme is doing its scheduling, and every time that you come back it just so happens to be that the resource is in use and then you get basically starved and in my analogy, that would be really bad. Um, Where, where are we at? OK, fair, right, um, and the problem is that this idea of like the spin lock is just really, really arbitrary, you're basically just going around in a loop, um, you know, what is it, you, you go around in the loop, the Mex becomes free, another thread gets in, and then you go back and then you check, right? um. And I think this is just a visual version of that, yeah, so like you can see here you lock, A locks, B gets a go, waiting for it, then A unlocks and then re-locks it, and then B gets waits for it, and then A locks it and then then uh unlocks it and then locks it again, right? So at no point in time did B ever get a go. Um, so we have this idea, um, that what you could do is you could have this idea of like a, like essentially a queue, right? So you have an incremented value, while returning the old value, right, so what I'm gonna do is I'm going to basically, You fetch an ad, so I'm gonna grab the pointer, the old value is gonna be whatever I give it, and the pointer is going to increment by plus one, right, so like basically I'm going, hey, OK, we're gonna increment this value, and if it's not your turn, you spin, right, that's fine, um, but now, right? Um, now there's like numbers associated with it, right, so now I can create sort of a queue. I can have a ticket in a, in a machine, right? So just to explain how this would work, right, like, so, A runs lock and it's ticket is 0, you know, um, A runs. Now, um, has finished running, so it's turn one, right? And so B is like, hey, whose turn is it? Oh, it's B's turn, right, because I'm ticket one, right? B had ticket one. And then A locks it again, so A now has ticket 3, right, and now B's done, so it's turn 2, who has ticket 2, so C runs. You know, big tries to run again, it gets ticket 4 and so on and so on and so forth, right, so basically you have this like ticketing system where whenever you try and lock, you basically go, I, I have a number, and whenever it becomes unlocked, you unlocked, you're like, OK, who's got this number? Alright, if you've got this number you're allowed to go, right, and so you just have a really, really basic sort of first in, first out queue. Um, And that sort of helps with that problem, like the idea of starvation. So you can make it more fair, but it's still not good, right? Um, a spin lock's actually not that bad in a couple of circumstances, like for example, one time that the spin lock is actually pretty good is when you don't expect to wait, right? Like as in if you know that the resource is gonna be there like really soon, then a spin lock's not too bad cause you're like, is it there? No, is it there, no, is it there, yep, done, right? Um, the time where a spin lock is not good is where the other thread is going to be using it for quite a long time, right? Because then you can imagine you'd be like, oh, OK. you know, the CPU's gonna schedule me, I'm going to like just wait, reschedule, another process will come on. OK, it's my turn again, I'm gonna wait. Reschedule, right, and so we're gonna be using up a lot of like um CPU time and achieving basically nothing, right? So if you have lots of CPUs, it's usually not too bad because you've got lots of CPUs. Things can run on all the CPUs, you've got the spare CPU usage, wasting time isn't that big an issue, but if you have one CPU then it's a really bad idea, right, and if the locks are held for a very long time, Um, it's a bad idea, so in general spinning is pretty bad. Um, what are we doing for time? Right. Um, so, Actually, maybe we'll have a break because it's like 2. Yeah. I'll, I'll have like a 5 minute break cause it's like 2:04, and then um we'll come back in at 1002, right. Part two. OK, so we're just talking about spin locks, OK, so the problem with the spin lock is that it's going to use up an entire block of CPU for no reason. So the idea here is that you're like, what if I just told this, had it in my system, so they would just yield. So they're just like, OK, if it's not my turn, right, so I basically go into my wild loop and I go, if lock.turn, if it's not my turn, um, then yield, just be like, I'm going to reschedule. I am now a blocked process, right? Um, now, I'm not really a blocked process, I'm just gonna sort of quickly say, you know, uh, I'm going to de-schedule myself essentially from the um dispatcher. Right, um, and so I'll come up again soon, but it's basically going, OK, run for one cycle then do nothing, right, um, and then that allows other processes to run, right, and so this seems, nominally like a pretty cool idea, right, cause you just, you know, sort of disappearing from the, the schedule, um. Because there's no point in waiting, or there's no point in busy waiting, right? You might, you might have to wait, but like there's no point in actually doing stuff while you wait. Um, So when we're trying to determine whether a lock is good or it's bad, you know, the way that we're setting up our system, um, we have two situations where we want to think about its performance. One is when the locks are really empty, and the other is where the locks are basically constantly in use, right? Um, and a situation where locks are constantly in use is like CPU scheduling because the CPU is a lock, right? So once you get up to a high enough, um, use of your shared resource, it basically becomes everything that we did in the last 34 lectures, right? It's, it's just trying to work out what is the best way to schedule tasks, give or take, like who gets the lock when, right? Um, but when it's low contention, you also want to have a situation where like if it's freely available, you don't waste too much time trying to get it, right? Um. So One of the things that will happen is that you sort of think about it and you're like, OK, now I have to think about, um, oh wait, who here's done CNA, by the way? CNA right, for the CNA people, it's the difference between Ethernet and uh like a, Uh, a token, a token round robiny thing, right, like, um, ethernet was the logic behind ethernet is like send something, oh damn it, send something, oh damn it, send something, oh damn it, right? And then like, making the pause between the odd damn its like slightly larger every time, right? Um, and that works really, really well when your network has not that much traffic, right? Now you can imagine if you have hundreds of people saying, oh, send something, oh damn it, because someone else was also sensing something at the same time. Um, then you just end up with a situation where everyone's just yelling, damn it constantly, right? Um, and so that's in high contention versus low contention. So different architectures work better in when there's freely available resources and there's minimally available resources. OK. So if we think about it in terms of the actual cost, right, the complexity or like the time it's gonna take, if you don't have a yield, right, so you don't, you don't yield, it's going to be, Like the threads multiplied by the block of memory, like the time slice, how long do you take up for each, you know, round robin scheduling time, you know, how many, how many milliseconds of or microseconds of computer CPU do you get at a time, right? Whereas if you do it with yield, right, the cost is actually going to be the number of threads multiplied by the context switch, right? Because you're gonna have to like push yourself out of the scheduling queue queue and then do all the stuff where you load memory and all those, you know, you know, paging, all those things that you'd have to consider about, you know, changing processes, um. And That can also be really, really annoying, right? Because if you think about it, if it's freely available, would you rather run in a while loop for 2 more loops, or would you prefer to be rescheduled from the CPU and then have to be rescheduled into the CPU and basically have to swap in and out your memory, right? Like, now if you're gonna wait a while, then yeah, that kind of makes sense. You don't want to be busily waiting, but if you're only gonna wait for one or two like cycles of the loop, why on earth would you reschedule yourself? That's like an expensive process, right? So yielding is not free, right? So, so even though it seems like a pretty cool solution, it's not free, so it's not actually that great. Um, so one of the other ideas that we could have is we could have the idea of, OK, if a thread is waiting, it goes on the blocked queue. And the scheduler only runs ready processes, right? And uh so we discussed this really early, there's, you know, like there's ready, blocked and running, right? What if you could have it so that you, you set it up so you're like, as soon as you ask for something you can't have, I put you on the blocked queue and you just wait on the blocked queue, you don't just like, You know, wait for your turn again, pop up and then yield again, because that again is a waste of time, right? Like we're we're also paying the cost of yielding and unyielding arbitrarily every so often. Um, now what I'm gonna do is I'm gonna just put you on the block queue, and when it is your time to shine, I will tell you that it is your time to shine. OK. So, in this example, what we're gonna do is we're gonna have A is going to be, where is it, uh, A is gonna be running, right, and B, C, and D are ready. B is going to try and call um the lock, right? But it, OK. Right, it's giving me feedback there. Um, B's gonna try and call lock, right, so this is where B calls lock, but because the lock is already in use, right, cause we've already got one lock, what's gonna happen is we're going to put B onto the blocked queue and then C is just gonna start running, yeah. And then we're gonna have D and D is going to try and run and it's gonna try and call lock, right, and so as a consequence, it's also gonna be put onto the blocked queue. Right, and then A is going to run, and then C is going to run, and then A is going to run and at some point it will unlock, but because it doesn't finish immediately when it unlocks, A has other stuff to do, it will finish its time slice, right? And what will happen as a an immediate result of the unlock is that it will shift B from the blocked queue to the ready queue. Um, and as you can see, the ready cue starts with C, so C gets to go first, um, but now, you know, B is first on the ready cue, A is back on the, um, ready cue, and C is running, D is still blocked. Um, He's probably feeling a little bit hard done by at this point in time, right? Like if you think about it, right, like be be sitting there going, hang on a second, but, but why did C get to go? I've been waiting for ages, right? Like, like, I don't want to start at the back of the queue again. I've been waiting since like forever, right? And so we also have a, a spot here that we could do a little bit of intervention, right? Um Uh, this is kind of what the code looks like essentially, right, so we have this idea of parking and unpacking, um, processes, so basically we're going to have, you know, we have a, a flag and we have a guard. It's all very complicated, this one actually. Um, so this is our lock, we're going to initialise our lock with a flag and a guard. When we acquire the lock, we're going to test the guard. If the flag, so if the locks flag is good, right, so we're gonna test the guard first, so we need to basically um use the guard before we actually bother to check and set the flag, right, so the guard is basically going, we're gonna do stuff with it. Yeah, so we're gonna test and set with the guard. We're gonna check the flag, if the flag is true. Um, we're gonna add ourselves to the queue, right, and then we're gonna park ourselves. So basically, the flag indicates the status of the lock, right? So if the, the, the lock is in use, we're gonna park ourselves. Otherwise, we're gonna set the flag to one saying that we've got it, right, and now the flag is 1, we're gonna set the guard to 0, we're outside of the guard code and everything is fine. Now I'm gonna do all of the things that I needed to do to acquire the um, The lock, um, uh, like, uh, once I have the lock, cause I've got this lock, right, this is me just getting the lock, so it's kind of a bit weird, right, cause I'm sort of like doing a second kind of locking in order to avoid, um, the guard kind of functions as a mutual exclusion just for the, the, the process of grabbing the lock, which is kind of crazy. Um, and if I release, basically what I'm gonna do is I'm gonna try and grab the guard again, right, like and wait until I get the guard, um. If the queue is empty, I'm going to set it back to zero, but if it's not empty, I'm going to unpack, remove from the queue, the process that's at that, that's in the queue, right? So the logic here is pretty sound, right? So the idea is, firstly, I'm going to make sure that when I'm doing all of this, I'm mutually exclumutually excluding it, and so I've, I've got basically like a mini mutex, right, this guard, which I'm gonna use test and set which makes it basically like a mutex, um. So that I can't like acquire and release locks at the same time, but once I'm there, it's all good, right? So once I've got past the guard, right, I've I've done the test and set for the guard, so that's working like it's mutex, I've got the guard flag, right, then I'm gonna be like, OK, OK, I'm gonna do stuff. I'm gonna take my flag, right, um, except if the flag is already won, then I'm going to queue to the side, right? But if I'm not, if the flag has not already been used, I'm gonna use it, and I'm like, yeah, I've got my flag, yeah, and then I can do my stuff and at some point I'm gonna release that flag, right? So, in order to do that, I need to get the guard to release the flag, and then I'm going to wake up whoever's first in the queue, right? So now I've got a situation where like it feels fair, right, like basically I can unpack the be process, so to speak, it doesn't actually solve that problem because all he's gonna, all that's gonna happen is he's gonna get chucked back into the the scheduler um. But um maybe I could do something with the unparked to make that process a little bit more fair again. Yeah, here we get stuck by the guard. Oh yeah, this is like this is where it can stuff up again, isn't it? You get stuck by the guard. OK, so how can it stuff up? So we, we're stuck in the guard, yeah, that's fine. The flag should normally set to zero at the start, so we set the lock and reset the guard. Yep, no, that's fine. They can process as we get it, oh no, this is just the explanation, sorry, yeah, the, the colour, the colour here is just to represent the different threads, so like, First thread gets stuck by the guard once it's done, um, because it's the first one to access the, the thing, the flag will be false, so it gets to the L, it sets the flag, sets the guard to zero. When a second process comes through, it's definitely the flag's gonna be definitely in use and so it's gonna get parked. Um, when the first thread does test and set for the release, it's gonna get stuck here, worried about the guard, but once it gets past that, it's going to unpark because the queue's not empty, it's going to unpark the first element on the queue. Yeah. We're all good. So, Some questions, OK, so, so there are a few things here to unpack. The first is, What the hell with the guard, right? So the guard kind of makes a bit of sense, right, we're not, you know, like we, we've already got locks, and I guess we could just have another lock for this, right, you know, like, um, but we've explicitly basically built the lock out of the, the low level function testing set. Um, Question number 2 is like what, I thought, I thought the whole point of this was to get rid of these um spinning functions, right? The question is how long is that spinning function going to be running? Right? A long time, not really, right, because the only thing that you need to do when you acquire the guard is acquire the guard to set a lock or unset a lock, right? So the operation that you're going to do is actually quite short, right, because all you're going to be doing is like directing threads, you're not going to be doing any actual real work. It's not like a regular um mutex where maybe you've grabbed the mutex and now you've got to print a whole page of text via like a bit of hardware, so no one can steal the printer from you in the in the interim or something. Um, all you're doing is acquiring a lock, so it's actually quite quick, and that's why you would do it that way, um. Unpack does not reset the flag, right? Any guesses why not? No? Why, why, why does Unpark not reset the flag? Yes. Um, sort of, it's, it, yeah, that's kind of, that's kind of, yeah, that, that's kind of a good way of thinking about it. Um, basically. If you can imagine, If thread one has set the flag to one to say I'm doing stuff, right? And then it tells another thread to unpack, right? There is a chance that someone might try and grab the flag if I change the flag in that period of time, right? Like I don't wanna be, I don't wanna be changing flags, right? Because if you think about it, when that thread gets unpacked, it's going to have the flag, right? Like that's what's gonna happen, it's just gonna go, right? So it'll be fine, right? So that other thread is gonna be fine at that point in time. Right, so you're doing sort of like a seamless um transition between the two. Um, and is there a race condition? OK, so the guard acts to separate flag settings and queuing behaviours, yeah, sure. The guard spins minimally, it allows a new process handover, so the new process basically has priority because it, it like the, the, we're not actually resetting the flag, so there's no chance that it's trying to get the flag again. um. All right. There is a race condition This one's a little bit finicky, right? There is a chance. I'm told to wake up before I went to sleep. Right? So you can imagine you're like, OK, I want the thing, OK, cool, I want the thing, I've decided that I can't have the flag, and now I'm going to be rescheduled by the CPU. Right, and another thread comes along and it's and it goes, OK, cool, um, alright. What I'm gonna do is I'm going to do stuff, I'm gonna take it, and there's a guy in the queue, excellent, I'll tell him to wake up, right? And then what happens is the, the second thread who, who basically determined that he needed to fall asleep. Right? Then he goes, oh I just got told to wake up. Cool. What am I supposed to be doing? Oh I'm supposed to be going to sleep, cool. And then he falls asleep and then he never wakes up, right? So there is a situation here where we can have another race condition where basically, Um, and it's kind of a really annoying one because you, you'd say, OK, well, what do I do about the guard, right? Like if I set the guard to 00 wait, do I have a, do I have a mouse here? Where's my mouse? Where is my mouse? Or maybe I'll go to a slightly more like explicit pointer. Uh, laser pointer, right? You can imagine a situation here it's like, OK, if the guard is, is, is set here funnily, right, you know, like if I like if I set the guard here to zero, then this bit of code can execute all the way through, right? Um, and if that bit of code can execute all the way through, then someone can then tell me to um fall asleep. So the oh sorry. Now I have to get off laser printer, this it's not working anymore. And, OK, I'll try this again, right? So what I'm gonna do is I'm going to basically add this like set park, right, so, so there's gonna be a set park. So instead of saying, uh, I'm going to park myself, what I do is I say, everybody, I'm about to fall asleep. That's my plan. And what that means is if you tell me to fall asleep between when I say I'm going to fall asleep, and when I do fall asleep, I can detect that, so I can basically go. I said I was going to fall asleep, and when I get to the I will fall asleep part, I can detect, did anyone tell me to wake up? And if anyone told me to wake up, I won't actually do the fall asleep part, right? So there are ways of getting around it, but you can see here that it's getting more and more convoluted. Um, If we think about like how spin locking works, you know, like for um versus block. Maybe my battery or my mouse is running out, that's why. So let me just, sorry, this is just driving me nuts. OK, so. Do I, do I not have a mouse? Alright, um, let's go to. Yes. Do I have one of these? Where is my cursor? OK, there we go. Um. Right. OK, so on a spin locking versus blocking, right, so on a uni processor, when we're doing spin locking versus blocking, We're basically waiting so you can see that, you know, we spin, and then, you know, like if something gets locked, we're gonna spin and then we're gonna release, right, so we're holding up processes that we shouldn't really necessarily be holding up, um. On a multi-process, there are ways that you can sort of get around this, you know, like, like, um, because things are slightly out of um out of sync, um. You have this um opportunity where like as soon as something is available you can immediately start because you're on a different process, or you're a diff on a different processor. So um spin locks are actually superior, um. In that circumstance, so if your locks are released quickly, because you have, you always have like memory available, you should be using spin locks, basically, um, but if they're released slowly, then you should be trying to use this kind of blocking mechanism. OK, cool, um, and the way that we can just calculate it is that basically what you do is you work out how much does the contact switch cost and how much does like one time unit cost and, If the context switch is more expensive than your time slice, then you use a spin lock, and if it's less than your time slice, then you should block, right? Um, there's also a few texts. Um, yeah, uh. The advantage of the few tax essentially is that um it doesn't use the low level instructions unless it has to, right, and that's basically all you need to know about few taxes, right, so you can, I think this is one of those ones where maybe it is worthwhile googling, um. At some point if you are really interested, but basically, um, Yeah. What you're doing is you're going, oh I'm gonna put some, it says here, you're like, you put the calling thread to sleep and if the value at the address is not expected, you return the call immediately, right? um. So, and the wake one does, you know, just wakes something from a queue, but like the, the whole point of this is really that, um, if you can imagine there's a series of actions that are involved in calling a mutex that go down the chain, um, and in some cases you don't actually need to do all of those calculations cause you can just say, oh, the mutex is available, I'll just run, right? um. So it's slightly faster as an implementation, um, it exists if you're interested, um, I am not. um, so. Uh, the next one, everyone should be familiar with this if you've done systems programming, condition variables. OK, so. Why are we doing this one, right? Um, basically, if you think about how, um, mutex has worked so far, what we have is a situation where if you wanted to do a relay race, so a relay race is a really cool, easy way of thinking about something, so with a relay race, what you've got, you've got a baton, right, and you run and you give it to the next person and they take the baton and then they run. And then they give it to the next person and they take the baton and then they run, right? Um, at the moment our mutexes don't have that property, right, like our mutexes we've got like this one flag, and everyone's trying to grab the same flag all at the same time, and there's no like guarantee of the order in which these functions will run, right? Um, and this is really problematic because in, you know, in, you know, uh, multi-threaded applications, one of the things that's actually a lot more important is trying to make sure that, you know, like, for example, if I'm writing data to a buffer, right? Well, I wanna make sure that the buffer is empty, right? And then after I've written something, I want to tell someone else that you can now read it, right, and they need to wait for me to tell them that they've written something so that they can check to pull something out, right? So if I want to get things into some kind of sequence or order and make sure things that work in the, you know, like make sure I put on my socks and then put on my shoes, right, not put on my shoes and then put on my socks, then I need to have something more sophisticated than a mutex in order to do it. So The idea here is this handover, right, which is basically, you know, there's 22 things that are going on, right, like if you ever run in a real light race, you'd be like here going, come on, come on, come on, I'm waiting for someone to come, and then I wait, and then there's a handover process, right, where they tell me by going, here, here is the baton, it's time to grab it, I grab the baton and then I start running. Right, um, and so what we're trying to do basically is implement that kind of logic, as a, the logic of a, um, a relay race. And the way that we're gonna do this is via these uh condition variables, so it's P3 coned weight. Right, so this again should be revision from systems programming, but basically what we've got is we've got a condition, we've got a lock, right, so the condition is sort of the thing that we're gonna be waiting on. The lock is the thing that's going to guarantee the, the timing between the two, and then we've got like a mailbox which I guess contains some data that we care about, um, and likewise we've got the signal, right, so we've got weight, so one of them is going to wait and the other one is going to signal, so signal is basically gonna be the thing that goes, you go, right, so we have, You know, the thread that needs to wait for the resource is going to call wait and the thread that is going to give the resource is going to be called signal, right? And if you signal and no one was waiting, this doesn't actually do anything, which is kind of fun. OK, so this is the animation that I have painstakingly made to um explain how this all goes together sequentially, so if you understand this animation. You should be capable of understanding what's going on. So you can see here on the left, I have thread one, it's going to lock the mutex, it's going to signal, it's gonna do some stuff, it's going to write, done is true, it's going to call the signal, and then it is going to unlock. I have this second thread that's going to take the stuff and do stuff with it. What it's going to first do is it's going to take the mutex. If it is not done, right, so that is to say if there's more stuff to be done, we're going to wait. Until stuff is done, and then after we receive the signal, we're gonna stop waiting because that's how signal and we interact, we're going to do our stuff and then we're going to unlock the mutex. So, Right 2. In order for this to make sense, thread 2 needs to be the first one there, and done is false, right? We can see done is pointing to the left, which means false in this case, right, so thread 2 is going to, uh, be, if not done, it's going to have to wait. But the first thing we're gonna do is we're gonna lock the mutex, right, so mutex is gonna be locked, then we're gonna wait. All good. Now here's an issue, what would have to happen now? If I'm thread one. How on earth do I get the mutex? Like it's locked, right? So what does that mean that conned weight has to do? So if I'm going to take a mutex, Right? And then I'm going to wait with the mutex. I need to make my conned weight signal release the mutex or otherwise it's not going to work. So conned weight actually does release the mutex. That's what it does, right? So that gives everyone an opportunity to, to use it, and if it didn't do this, then this whole system would break, right? Yes. That is why it has lock as a variable, right, that's why, yeah, you need to have both con and con and the lock in the. I feel like Con Signal usually has a lock as well. Let me just check. Oh no, it doesn't, OK, maybe you don't, but you definitely need it here, right, cos that's the lock that it's gonna release. OK, so we now wait and we release that lock. OK, cool. So what are we doing next? OK. Well, we're waiting, right, so it's gonna be thread one, right, so thread one's gonna go, thread one's first action is going to be to lock the mutex, thread's gonna do stuff which is nominally in this case, dumping some data into shared data. Thread 1 is then going to tell everyone in the world that it is finished doing whatever it was supposed to be doing, and then it's going to send a signal to thread 2. Bread 2 It's going to unlock the mutex, does that make any sense? No, OK, so the signal function also has to unlock the mutex by default, right? Otherwise this is not gonna work. Then thread 2 is going to grab the mutex. I think this is all actually implemented in weight, by the way, um, so it's gonna grab that mutex again. And there we go. And then it's going to do its stuff, right, so it's grab the mutex as part of the conduit thing, right, and then it's going to grab the stuff, take the shared data, and then, Gonna set itself to, you know, not done anymore cos nominally, no, well, I mean this is not strictly necessary, but basically, now that you've done something with the shared data, normally you would have like some variable that you set to not done so that it can now tell thread one to do some stuff again and you could set up another like thing, so thread one knows to dump stuff in the uh in the shared data. It's not strictly necessary here but for that, this example I did it. And then anyway, um, after you've done the stuff, um, you can unlock the mutex and you can start again. Any problems? How can we break this one? Yup. Yeah, yeah, yeah, that would definitely break it. So how the, the, the signal that doesn't actually reset the lock, that is one way that we can break this. Um, one of the issues here is that like, um, you could be told to wake up and done could be wrong. Right, um, you could have, after you've said, oh it's not done, you could have, you know, like another, um, thread come in and do some stuff between the not done and the weight, um, so that it's actually done. um, so, um, in general this is usually done as a wild loop just to make sure that it doesn't, um, stuff up. All right. Um. Next one is pipes. Alright, who here has done this before? Yeah, alright, cool. Um, basically we're gonna use this as another example of where we could use, um, locks, uh, uh, sorry, conditions, conditions, right, so when you implement a pipe, LS is going to produce a bunch of text, word count reads in a bunch of text and then spits out a bunch of texts, nothing very complicated. The word count just prints out numbers. Um, but in order to get this to work, right, we need to be able to buffer the, the string of characters that are getting pumped out by LS and then read those characters into word count. So it's basically gonna work like this, if the buffer is full, we're gonna wait, and if the buffer is empty, we're also gonna wait. So um if LS thinks the buffer is full, it'll stop pumping characters out and it'll it'll stop and if wordcount thinks the buffer is empty, it will stop and wait until there's stuff in the um the buffer in order to continue counting words and characters. Um, In pictures this is basically what it's gonna look like, basically we're gonna have a start and end for our buffer, so like just in memory, um, when we write to it, the stuff is going to fill up and we're gonna have a start and an end, um, when we read from it, uh, we're going to, you know, like basically take stuff off, we're gonna increment the start, so basically when we write we increment the end and when we, uh, read we increment the start and remove things from it. And if we read too much, then we're going to wait, right? And then likewise, we can have a situation where if we if this is our start and end, and we continue to write and then we continue to write, eventually we're going to now we wait, right, because we can't put anything more into the buffer. Nothing too complicated here. So the idea is we're gonna have a producer and it's going to add stuff to the buffer, right, and we're gonna have a consumer and it reads stuff from the buffer and we're gonna do this all over the place in life, it's going to be, you know, web servers or pipes or any anything where you're like dumping something into a repository and then you're waiting for it to be processed and then you're pulling stuff out at the other end. Um, it happens surprisingly a lot, um. So let's have a look at this code in detail. Um, basically we have our producer, the producer for a number of loops is going to grab the mutex, it's going to say while it's not full, we're going to wait. So it's like, if it is full, we're gonna wait, sorry, if it's full, we're gonna wait. Um, and if it's not full, then we'll fill stuff up, and then, um, we'll say that we filled stuff up to the other thing to say that we've now put some stuff in it so it can't be empty anymore, and then we're gonna unlock the mutex. For the consumer, what we're gonna do is something similar, we're gonna lock the new text and we're just gonna do this forever, right, we're just gonna keep reading because we don't know when it finishes, um. While, um, it's not well, oh sorry, num full is like how much is in it, right, so, um, while, um it's empty, we're going to wait. And then we're gonna grab whatever's in it, so we do get whatever that means, so do get it's gonna be like sort of the equivalent of do fills so we have to pull stuff out of this buffer. um, and then once we've pulled stuff out of the buffer, the buffer can't be full anymore because we've definitely pulled something out and so we'll signal and then we'll mutex unlock and then we'll print whatever we pulled out of the buffer. Right, so, quite simply, this is this section here, so we're gonna fill it up with something, um, and then here we're gonna, uh, what do you call it, uh, sorry. So we'll lock the mutex, right, we'll fill it up and then we're gonna con signal, right? So nominally if this was waiting for it, right, because nominally this should be waiting for it, this one will now get woken up and now it can do it's do get, grab the value and change it to grey, and then it will signal back to our producer, which will then wake up again and then do another film. And so on and so forth, right, so basically we can chain these actions together infinitely. Um, why is it useful? Um, mostly because you have situations like this where you have multiple producers or multiple consumers. So you can imagine a situation like there's, there's a whole bunch of use cases, you know, design patterns where this occurs, but a really simple one is if you can imagine like, um, if you have a process where like uh consuming data takes a long time, so basically, The data comes in and you've got a whole bunch of computers that can do some data processing, but you've got like this interim spot where the data comes in, right, like it's like a, I don't know, like a mailroom or uh uh what's a good example? I don't know. I'm trying to think of one. Sorry, I'm running out of a bit of brain brain juice at the moment. Um, but yeah, you have a situation where like you've got a lot of people who can do the jobs, right, so when something comes in, you want to be able to give it to one of a bunch of people, so it just sits on a buffer and whenever someone's finished with something they they can just pull it, immediately pull it off, right? And it maybe that takes quite a while for them to process it, so someone else can come to this same buffer and be like, oh, I can pull something off and start working on it, right? So you have a situation where stuff coming in is a lot like the producer is a lot faster than the consumer, so you have one producer but many consumers, right? Um, I think the example that they give is the example that you give is a dining room, right? Like so basically the kitchen produces lots of meals and then people can just come and grab meals whenever they want. Um. You can have it, the other situation as well where you've got like multiple uh producers, which is the one on the board, um, and that's like when you receive letters, you know, people can put letters in your mailbox, and maybe your consumer is really, really slow but it can grab a whole bunch of them at once, you know what I mean? um. Is another example, um. Yeah. You can have situations where you have multiple producers or multiple multiple consumers working off the same buffer. Um, We can get back to this idea of fairness as well with the condition variables, like, like, clearly we've got another problem here, it's like, OK, so we've got to be a bit more, you know, specific about like the weight and the signal functions, so when you signal who wakes up, like which which of the processes that are waiting. So if you had like lots of consumers and they've all got the same condition, and now you call signal, who wakes up, right? Um, there are, um, methods to signal literally everybody, it's a little bit crazy, but it wakes all the threads, um, and then we've got the regular one which just wakes one thread. Um, most of these systems do have like an implicit cue, right, that is to say it's like a first in, first out, whoever waited first, um. Yeah, um. One of the other things um that's kind of, um, you. A lot of these problems, I don't know, I, I mean I set an assignment like this for assistance programming in first semester, assignment 4, right, um, some students try and do it with one lock and one condition variable, and in a lot of cases you can, but most of the time it's just easier to have multiple locks and model condition variables, right? So like, it makes a lot more sense for debugging purposes and just for the sake of like, I don't know, making it intelligible to humans. If you have one condition which is signalling when it's empty, and one that's signalling it when it's full, it's much less likely to have a, Uh, like a thread that basically wakes itself up and be like, oh, it's full, so I'll send a signal, which is the signal that says it's empty to myself, yeah, right, like that's much less likely to happen. So you can see here that I've got a conned weight full, and then a signal full and I've got conned weight empty and signal empty, right? Um. So yeah, um, the main thing to remember is that when you do the weight and the signal, you always have to be holding a mutex because if you're not holding a mutex then the ordering can stuff up, um. Condition, yeah, I One of the things that a condition variable has, it has a concept of like, I don't know, state, like the, the, the, like a state flow diagram, like which states you can go to from which other states you can go to, um, And yeah, whenever a thread wakes, always make sure you recheck, right, like you always want to have it so that it like goes through a loop of some kind and then checks again. As you can see here, like if you have it as a wild loop, you can see it's a wild loop. So even if it gets woken up as the conned weight, it will check to see if it's full anyway, right? And it shouldn't be full, but there is a chance that maybe some other thread has managed to like fill it up or something. Um, having, uh, like conditions where you double check before you go and do something stupid always makes sense. Um. Cool, and I think we're at the end. Um, concurrency locks, locks implementations and conditional barrels. Yeah, excellent, alright, are there any questions? That was a long one. There are no immediate questions, I will pause the recording and then I'll get some other questions later. Thank you everyone for coming. Have a good weekend.

SPEAKER 0
Has like smashed their face against grade scope and basically ended up losing their mind. I explained this problem to my 12 year old son and he, he just did the, oh my God, what the hell, right? Um, he's learning how to programme in C++ at the moment, and he was just like, I don't understand. And then I had to explain to him timing errors and all this kind of stuff. It was all, all great. Anyway, I think the lecture's technically started and I've already started rambling, so, um. Just housekeeping assignment one is due on Sunday night. Um, lots and lots of people have gotten full marks for this assignment so far, so um it's definitely not impossible, um. I have tried to make sure that any opportunity where the output could be construed in a way that is annoying in some way, um, that that doesn't, that that that is not the case, that is to say, like, try to make sure that you know like if there's weird spacing errors and stuff like that, um, I will continue to see, depending on, you know, like if I get a student's answer, which I think is right, um, I'll, uh, what do you call it, update the marking script in order to try and accommodate those answers, um. Uh, other than that, um, again, uh, we have another tutorial next week, right, or workshop, whatever you call it. Um, so the workshop prep should be due this Sunday. Um, and then the last thing is that I've released assignment to. Now I haven't looked at assignment two in detail, but it is up on grey scope, and I am making the the faulty assumption that the person who designed this assignment that has pre-existed, um, that it still works, right, like as in the scripts work. I've not actually tested anyone's solution for it yet, but I'm gonna do that next week when I have more time. Um, basically assignment 2 is there's a, a very, very simple way of describing it. It is a group assignment, there is a coding component with a grade scope submission, there is a non-coding component which is a report. The idea is that you're just going to implement some of the page replacement algorithms which we're learning about today, um, and then you're going to basically try and summarise in a sort of scientific way. Which ones work better for these particular bits of data that are available? Um, I said, I think I need to make sure that the data is actually available. That's one thing I haven't done yet. Um. I'll, I'll make sure that gets done this afternoon actually. um, and yeah, I think that's where we're at at the moment. The other thing is that the first, Assessed quiz is going to be on Friday. I'm going to just set it open, it runs for half an hour, you can do it anytime during the day, it's online. Yeah, if that is impossible for you because you don't have half an hour on a Friday at some point in time, um, that is possible, like if you're in the hospital or I guess in transit to an interview that's on the other side of the world or something. Um, please send me an email just to say, you know, like I cannot do it at this specific time, but it's any time on a Friday. Um, and then it should be uh visible now, I think, um, uh, are there any other questions housekeeping wise, before I get started? It appears not. OK, let's begin. Alright, so what are we looking at today? Last week, what do we let's start with last week. Last week we were looking at segmentation and paging. Um, this week we are looking at translation, look aside, buffers. What does that mean? A little bit about page tables and swapping. I don't think this is accurate. I think we covered, we covered that last week, right, surely we we covered this last week. There we go. Here we go, we covered this last week. So, I, I, I produced this from, uh, material from last year, um, and I think, Uh, the lecturer last year spoke really slowly, and I do not, and so he never finished paging in the week 3, so he did paging in week 4, so I put paging in week 4 and then realised I could do it all in week 3. Alright, um. So segmentation and paging. All right, so what are we doing today? um. We think about where we were last week, it seems like a long time ago, um, last week we were talking about paging, right, and the idea or the the conceit of paging was that if all of the memory is divided into equal sized chunks, I never need to worry about the chunks. I just, I, I know I either have a space for a chunk or I don't have a space for a chunk. If everything is 4 kilobytes big, I can just be like, I need 4 kilobytes, I'll know and can easily track where the holes in my memory are, and I can just chunk, you know, throw in a 4 kilobyte chunk or a page wherever I want in memory, wherever there's space, and I never have to think about do I have a big enough. Space, right? I never have to think about cutting anything into bits other than everything now needs to be cut into pages, um, but I don't need to go, oh wait, this gap is 2 kilobytes, but I have a 3 kilobyte file or this gap is 1 kilobyte, this one is 1.8 kilobytes. I don't have to do that. I just go, where is the 1st 4 kilobytes? And if there are none, I know there are none, right, because I've used all the pages in my memory, right? So I've simplified the concept of external fragmentation. Um, So yeah, easy to manage swapping is easy. OK, it's very easy to swap one page for another, you can move the pages around really, really easily. The concern that we had last week is that if we recall, the way that this works is we have a page table, and the page table is in memory, and in order to work out where your pages are, you go to your page table by accessing memory, and then you look down your page table and you find out where the page you are looking for is, and then you go back to memory and you find that page. And if you have a multi-level page table to avoid the page tables being too big. What you end up doing is you end up doing this multiple times. So you go, oh, I'll go to where my, you know, my page index table of contents, I can't remember the name of it. Um, basically a page table that has pointers to other page tables, right? So you go into that and you're like, oh, OK, so where's my page? And I go into another one and I pointed to another page table. That's not really pointed to another page. Well, it is kind of pointed to another page table. Um, and then you work your way. Down that chain and in so doing you've referenced memory 3 or maybe even more times, so you've gone go to memory to get the table of contents, go to memory to get the chapter, go to memory to get the um individual page, right? And you're like, OK, I've had to access memory 3 times, like whose crazy idea is of a solution, this doesn't sound like a solution, it sounds like a problem, right? um. Now, we are ostensibly doing this to avoid segmentation, because segmentation is slow and trying to find where in memory you can put things is slow, so we have solved that problem. So, That's where we're at, right? Um, if we think about it in terms of the actual steps, you know, you get your virtual page number, which is a fast thing, that's fine, it's from a register. You calculate the address of page table entry, that's fast. You read the page table entry from memory, which is slow, you extract the page frame which is fast, you build the physical address, which is also fast, but then you have to read the contents of the physical address which is also slow. So that's for a single level page table, right? There are two accesses, one is to read the page table and one is to read the memory. Because the page tables in there, OK, I've laboured that way too much, OK. So, what we're gonna do now is we're going to consider, Let's assume that we have to use paging, right? What about paging do we know? Well, one of the things that we tend to know in any given programme, right, so if I write this programme here into a some one to end, right, and I'm just summing a whole bunch of numbers, really boring, um. I'm gonna assume that inside this array, right, that those elements in the array are kind of close in memory, right? Like it kind of makes sense that they would be, right, it's an array, right, so like, I don't know if I asked for a memory dress 1,028, then I would expect the next element to be 1,029, and the one after that to be 1030, right? So what we're going to do is we're going to talk about this idea of locality. So, and there are two kinds of locality, one is this sort of spatial locality, that is to say that if I'm writing a programme, on average, it's not accessing random bits of memory, it's accessing bits of memory that kind of makes sense to somebody, right? And and things that kind of make sense to somebody usually have bits that are all together, right? So that's one of the things that I'm going to do, I'm gonna be like, oh OK, cool, right? Maybe I'm going to be accessing the same page over and over and over again, right, that like this is, this is like the main takeaway, right? So even though it looks like a scattered mess here on the right, the reality is that like, you know, I might just be accessing that second one from the top over and over and over again, right? And so it gives me this incentive to say, well, I'm not gonna just constantly be grabbing pages, maybe I can like cash that or cache that or I don't know how you pronounce that word. Um, Yeah. Right, so this is, this is what's going on, right, that we look at visually, you can see here that you know A0 A1, A2 and A3 might all be on the same page all next to each other. So once I've loaded that page, I have an incentive to keep it, right? And so, Today's lecture is basically about, OK, what are the strategies about keeping these pages, right, that that's the big summary, so it's not super complicated one, And the question here is, yeah, why would I keep loading the same page of memory over and over again? That seems like a silly thing to do, because in reality, if you didn't do any anything smart here, what you would do is you'd go, I would like a 0, and then you go to the page table and then you find the thing, and then you'd load a 0, and then you would start afresh and you'd be like, oh, now I need A1. I'll go to the page table, and then I'll go to memory. And then I would load that bit of memory. So now I've done what, 4 memory loads to get two pieces of information that are literally next to each other in memory. That seems like an insane proposition. OK, so we have spatial locality and we have temporal locality, right, and, and we're going to explore, you know, sort of exploit both of them, like one of them is memory accesses. In logically written programmes do not access arbitrary parts of memory. I've already said this, right? We also have temporality, so temporal locality is basically the idea that a programme is going to probably reference the same stuff over and over again, right, like, like that seems kind of logical, you know, like, let's say I am, Making Mario, right? It would make sense that I'm gonna keep drawing Mario on the screen, right, repeatedly, right? Um, so if Mario's image is stored in memory somewhere, I, I mean, I don't know if this is the best analogy, I'm gonna be using it over and over and over again, right, and maybe I'm gonna use some new things, but on any given, you know, little small segment of code, I'm basically gonna be using the same memory over and over again, right, and that's basically what temporal locality means, right, so. You know, like, here, if I draw, I point at the little empty box, that's probably if I access X, I'm probably more likely to access empty box there, and if I access X here, I'm probably going to access that same bit of memory again in the future. OK. So basically the idea here is we go, OK, I have these pages, I don't want to load them ever because every time I load them I have to go to the page table and I have to go to the subpage table and I have to go to the page. Um, what I want to do is I want to store it in cash, right, so we cache it, right? OK, that seems reasonable. And the thing we do to call this is a translation look aside buffer, TLB, right, so if you ever see the word TLB, it stands for translation, look aside. Buffer, right? This is not the most explicitly obvious set of words to describe what we're talking about. It's a cache, right? It's a memory address cache, that's what it is, but it's a translation look aside buffer. Why are we calling it that? Well, because we're doing memory translations because we're doing page tables. Um, and it's a look aside buffer, right, so, so it's like instead of looking where we're meant to go, I guess we look to the side and it's a buffer, right? OK, there we go. I don't know. I labour the point enough. Translation look aside buffer TLE. All right, what do we do next? Oh yeah, yeah, yeah, yeah, so the, the logic here is that we're just gonna go, OK, instead of actually loading the page, if I store it in cache, then I can just load the cache, and if the cache is nice and small and fast, then everything in the universe is good and I am happy, right? Um, Yeah, sorry, this is just the um what's gonna be inside my TLB by the way, like it's just gonna have like the. Uh, the virtual page number and then the physical page number and then some tags, right? So like basically if we think about what the TLB is going to be, it's just gonna be a series of like essentially pages, right? Um, it doesn't actually contain the memory by the way, right, um, it's just going to tell me where stuff is, right, um. Cool, alright, so. Right. Content oh content addressable memory. Yeah, this is the part that is quite hilarious, right, like so, so, If you think about it, right, we're gonna build this cache and inside the cache it's gonna have a whole bunch of memory addresses, and what we wanna know is if our memory address is already in the cache, that's what we wanna know, right? Is our memory address in the cache. So how would you do this, right? A normal person, sorry, a normal person, like a boring normal person would be like, well, I guess if there's 8 entries, what I would do is I'd go to the first entry and I'd be like, Hey, are you my memory? And they'd be like, no, and I'd go to the second entry. Hey, are you my memory? No, this is slow, right? And so what they did instead is they literally, oh sorry, what they do instead is they basically go, What is in this address, right, this is where we would do it for normal memory, but in this context, content addressable memory which is implemented in the translation look aside buffer, what we do is we basically look at every location simultaneously and say is this, is this right? Yeah? So we just go, is, is it X, yes or no, right? And so we can immediately know if it's in the table, right? So we're simultaneously in hardware parallel checking all of these entries in the translation look aside buffer in order to instantly know it's there. OK, so we've sped that part up so we can immediately know, OK, so that's cool, it's very easy to tell if, and we're gonna use the word hit and miss a lot coming forward, like if it's a hit, right, like it's the page that I want in my translation look aside buffer. Yeah. OK, so what it's gonna look like in practise is we're gonna have a code and, You know, we're gonna store some address or whatever, right, we're gonna go ding and then we're gonna go page location ding and then we're gonna go um offset ding and then, so the three things we're gonna do our two memory accesses, right, but, Now we can just go straight to memory, right, like because if we're gonna load this, we already, we've already got it loaded, right? We've already, we already know what it is, it's in our translation look aside buffer, right, so we don't actually need to go to the page table anymore or do any of that nonsense anymore. So we've saved ourselves some time, right, so this is good, right? Um, and again, etc. etc. etc. and now we're just like, I've already got this page. It's all cool, I've got my page. Once I've loaded it, I'm gonna keep this page. OK. Alright, so let's have a look. Page table is huge, OK, so right right right. So we use a multi-level tree like page table, that was our, our thing, right? Problem number 2 is each level of the tree makes memory access slower. If we cage the commonly used address lookups, we're gonna be fine, right, like that's the, the kind of logic. Um, So, This is what it's gonna do, I have my TLB, the thing is there, is it a hit, the thing is not there, is it a miss. So when I'm talking about whether it's good or bad, kind of unsurprisingly, the metric is gonna be, The miss rate, so how many times does it stuff up, right, so my goal is gonna make my mis rate 0, right, like I, I wanna have it so that every time I access memory, This is the magic number that I'm gonna calculate to tell me good translation looks like buffer strategy versus bad translation looks aside buffer strategy. Um, not super complicated, um. So, An example, right, like say we're gonna loop over this massive number of numbers, right, and we're gonna say our page is like this big, right, um. So we have 2000 and 1,024 ins per page, right? Um, if N is 2000, yeah. Then it's 2000. I'm gonna have to loop. Above 1,0024, which means I'm gonna need at least 2 pages, which means I'm going to load two pages from memory, so ideally what I might have is a mis rate of 2 in 2000, which is about 0.1%, right? I'm going to access it 2000 times. I'm only going to have to deal with missed pages twice, once for the 1st 1000 numbers, well, 1,0024 numbers, and then the next one for the next 976 numbers, right? Fairly straightforward. OK. Any questions so far? We've got the general idea, right, well we're basically just in this thing where we've got a buffer of things, and we're gonna give you some random numbers, and each of them is gonna have a random page number on it and we're gonna try and maximise the amount of times that we don't have to. Do something. OK. No questions so far, alright, cool. Idea number one. Idea number one is we could fiddle with the page size, right? Because if we're trying to change this number here, right, the miss rate, if my pages are big, it means I have less pages. If I have less pages, it means I miss less, right? Classic exam question, right? It's like, what, what would be the effect of having small page sizes on a translation looker side buffer? Well, if I have small pages, then each page is smaller and show the amount of pages I need to represent any arbitrary bit of memory is larger, which means I'm going to have more misses or I need a bigger translation look aci buffer. Um, more pages, less business, OK, yeah, cool. Option number 3 is that your translation look aside buffer is psychic, right? So these two are, these two come with their own drawbacks, like if you make the page sizes bigger, obviously there are going to be some other problems, right? Um. But, and you make your translation look a side buffer bigger, you're gonna make more problems, right? Like so you can make the the buffer basically I can hold 9000 pages, I'm a I'm a cache, right? But like sort of definitionally within hardware, the larger something is, the the slower it is because you need to like physically wire, you know, the buffer to all the bits of memory. Um, so there are limits to how big a TLB can be, right, in order to be fast, right, um, and so. There's going to be a limited number of pages that you can cache. Yeah, um, the one place where we can play a bit is, is deciding which pages to keep, right? So if your translation look aside buffer is psychic and it knows what you want in advance, right, um, then theoretically, It would be optimally performant, right? OK. So, um, because, like, just reiterating this point, because we hit the sum repetitively, right, um, It's going to be temporarily did I temporarily, there we go, temporarily similar, right? We're going to constantly going back to sum, right? So even if sum and A are on different pages, we probably want to keep the page of sum in memory because we're gonna use it over and over and over and over again, right? So what this is saying is that like if I have loaded a page into memory, there is some, You know, statistical likelihood that I will need that page again, right, so I want to hold on to it, right, and the more I use it, probably the more I want to hold on to it, right? um. And the values of A are spatially similar, so they're gonna be on the same page, the same thing's gonna happen, right, like, so if these two were on two different pages, I would want to hold onto those pages a lot, yeah. OK, so that seems reasonable. OK. Spanner in the works. The spanner in the works is, I'm running multiple processes on my computer. Right, I'm running multiple processors on my computer, um, and I have a translation look aside buffer that's looking at particular shortcuts for bits of memory for maybe one of my processes that are completely irrelevant to a second process that I have, right, so you can imagine that you've got like a second process that's running and it's looking at a different bit of memory, right, like, so we've got the blue and the green, and they, they don't care, you know, like we, we spent, An entire lecture explaining how to keep them separate, right, um, so now if I have a translation look aside buffer and I go to it and I'll be like, hey, hey, I want this bit of memory, can you check if it's there, right? Um, it's not gonna be particularly helpful, yeah, and to be honest, it's gonna tell you everything might be wrong, right, you know, like um. So option number one is that when you're running multiple processes, you could just get rid of everything in the translation look aside buffer. That seems like a really easy but stupid solution, um, because if you're sort of just losing everything, maybe maybe you have enough space that you can hold on to some important pages, um. The next one is that you basically tell the translation look aside buffer, what process are you associated with, right? That will solve more problems, right? Um, it is annoying, you need an extra comparison, but we're doing all this in hardware, so it's not too bad. Um, you could have really, really big TLBs I guess, or multiple, I mean, obviously that's good, but the physical constraints prevent this from happening. Again, you know, like there are limits to how big a translation looks like buffer can be, um. All right, right. So, What we have here is the AISA ID, so we've got all the normal stuff we like the virtual page number, we've got the page file number, uh the, Physical frame number, sorry, we have a valid bit to say whether this is the bit of memory that we want. We can have protected bits to say that we read, write, or execute to these pages, please don't execute this page or something like that. And then we have the ASID and the ASID is basically saying this one is for you, this is your, your, your entry in the translation look aside buffer. So again, not too bad, it tells you what you can do with the page. Um, This is interesting, right, like, so we can imagine a situation where actually we have two different processes, they're accessing the same physical frame number. Kind of makes sense, right? Like that that could happen. You could, you could design two processes, um, you could use the map or whatever it is to like literally assign them to look at the same bit of memory, um, and so you'd have two different processes but they'd have the same, you know, physical location, which would be kind of a convenient thing to do, but they'd be different pages within your translation look aside buffer. Which would be kind of annoying in a way, right, like cause part of you want, would want, you know, if I've got shared memory, I'd want one entry, now I need to have two, it's kind of annoying, um. In terms of sharing, we could have it so like there's a couple of ways of doing it, like this is where we start getting into really interesting points of view, like in terms of optimisation. So like, for example, um, if a page is read only, I don't have to think about it too much, right? Because it, if it's read only, and I know only people can read it, if I have to get rid of it, I don't have to do anything with it, right? Like as in I can just get rid of it, right, because no one's modified it. If it's right, yeah. And then I'm just loaded into like memory, and then, then I've just got it and I'm I'm like, cool, I've got my memory and someone's written to it, I'm like, ah, as soon as I want to throw it away, I'm gonna have to write it to disc, and that's annoying and slow, right? Um. So, you know, read-only is kind of cool. Two processors can read as much as they want from read-only memory, and nothing bad will ever happen. Um, if you have full, fully shared memory, right, and we'll go through this next week when we're talking about concurrency, um, that's where you get into all these issues of like, um, I change something just before you pull it out and read it and then everything goes to hell. Um, but it's very good for quick data sharing, so, um, that's cool. The other one which I think is really neat is copy on write, right, so. What operating systems can do is they can be like, oh, I've got two processors and they're looking at the same bit of memory. Ah, well, I can get them to just look at a copy of it, that's fine, so they're both looking at the same, Bit of memory, right? And so I don't need to load two different bits of memory, right? But as soon as one writes to this bit of memory to change it, then I'm like, ah, alright, I guess now I will make an actual copy of it, right? Like as in like, so up until the point that one of them changes the memory, I can pretend that they're looking at different bits of memory and they don't know, right? So, um, there's a couple of ways that we can worry about this. Um, OK. In terms of the implementation of the translation look aside buffer, oh my god, I'm gonna get so tired of saying the TLB right? um. We have two basic ways of doing it. One is it's all done by hardware and the other one is it's all done by the operating system, right? Um, and depending on the system that you're using, so you can see here X86 and ARM, they use hardware and then MPs and Spark and Risk, use software managed, right? So essentially, What happens is. What happens essentially is when you, when you call the translation look at side buffer, it's managed by hardware, they'll basically go oh it's not there, alright, I will load it again and then, Rerun the instruction, be like, go get the memory again, right, so I'm like, so you're like, I want this bit of memory, and it'll be like, oh, the memory's not there, oh crap, um, load memory in and then tell you to ask again, right? And then they can be like, is it there yet? And you'd be like, yes, it is there, right? Um, so it's a little bit different. Um, there are some issues here about, you know, like, um, you know, the ordering of the instructions, so you can imagine that you could stuff this up, right, like you ask for memory, you have to repeat that old instruction, right, um, otherwise the whole thing is gonna stuff up, um. But yeah, um, if it's software managed, what is this, the OS trap, yeah, it it just uses the trap system. Um, and then you resume the original instruction, hooray, OK, not that interesting. Um. If it's hardware managed faster, so this is the the logic of it, it's like basically if it's hardware managed, you basically are, uh, you know, forced to use the hardware, right, you can't change the hardware, but if it's software managed then you can be like, oh well, I will, I will fiddle with the algorithm slightly in my own way, right? um that allows you to deal with it in a slightly different way, so it's more flexible but like it's going to be slower, right? Um, and it's all to do with, you know, like if it's hardware managed then the translation look aside buffer is like a physical piece of hardware, so all of the bits are fixed, right, so like it works in one particular way, um, and so you have to conform to its structure, um. OK. Its waste. Yeah. OK, I memory. I feel like this, I did not cut and paste this pre uh correctly out of the the previous one. Where's the memory that we aren't using? Where's the memory we aren't using. That's waste, yeah, that's fine. Uh, basic programming. Sorry, I'm just, I don't remember writing these slides, which I definitely did within the last week. Good data, heap, stack, OK, yeah, that's fine, and then we have a library. Oh, OK, yeah, this is just about like, OK, um. So this is basically saying one of the other things that you have um um in in programmes is if, um, and it's just this is just another sort of like little aside on shared memory, is that like if you have a programme, so if I go back here, right, and I go, this is my programme, right, in virtual memory I have Lib A and lib B, right, so I've linked it all together, right? So I've got libraries, yeah, in my code, right, so my code is importing libraries and now I have libraries. So one of the things that we might want to have to think about as well, just when we're in this memory space is that like, Do I want to have two copies of this, or do I want to extract it out? So like, maybe I want to have a a spot in memory which can loads these libraries so that if I have multiple programmes, they can all point at the memory of the library to get the instructions, right, that's basically what this is about, right? um. I can have two programmes and they're both pointing at physical memory, right, um, where I have loaded the library into, right, so it's another way of doing shared data essentially, but this time it's for code, right, like, so like we talked about shared data before for like programmes, this is shared data for code. Oh right, um. How are we thinking about memory? OK. So, The goal is that from the perspective of the programme, we are always in the CPU. The programme never thinks that we're not in the CPU, right, like that's, that's what we're trying to do, and at any given point in time, we kind of want to be in the CPU as much as we can possibly be, right? Um, when we can't be in the CPU, we will be in the cache, and the cache is bigger than CPU, right? So the advantage of the cache is it's bigger, and likewise, the advantage to the main memory is it's bigger than the cache and the advantage of storage is bigger than the main memory. But like the distance represents how how fast or slow the operations are sort of. Abstractly, right? Um. So, one of the things that we want to have is, we want to know, um, are, are pages in main memory or are they not, right, like as in like, have, have we already loaded them from disc into main memory and if we haven't, then we need to set a bit to 0 and if we have, we need to set a bit to 1. Yeah, um. We also have a valid bit, right, so, so that's the present bit. So if anyone ever asks you what a present bit means, it basically means it's in main memory. We also have a valid bit and the valid bit is basically a way of saying, well actually I don't give a crap about this bit of memory, it doesn't point anywhere useful, right? Um, and so you can just say this, this bit of memory is not valid, right? So like basically if someone tries to access part of your programme and it gives an instruction to your to your for a memory address and it's in your page table and your page table can point and go, no, that's not actually part of my programme. Um, because just the nature of the way that you can, you can sort of put an address that doesn't exist in a page table. Um, we also have a dirty bit, right, so a dirty bit is basically a way of saying if the dirty bit is zero, the page has not been modified and so if it's in cash, I can just pretend it didn't exist, right? So basically the idea is I've got a page, I've read from it, I have not written to it and so um. When I write, no, when I, when I want to get rid of this page, this um this page, if the dirty bit is zero, I can just get rid of it. But if the dirty bit is 1, someone's modified it, and when I'm putting it back into disc storage, I would have to actually write it. So it also gives me a different like an incentive in terms of designing my algorithms around this idea of dirtiness, right, like cos I could like, well I could get rid of all the clean pages first, right? Because I don't have to do anything if I get rid of a clean page. If I have to get rid of a dirty page, I have to actually store the the changes to the data. Um, All right. So, are we going for time? Oh no, we're good. Alright, so. Today we're gonna be doing, what is this, demand paging, right, like so the logic of demand paging is I'm only ever going to load a page when I need it, right? Yeah. The advantage of this approach basically comes down to the idea of what's the alternative? Well, the alternative would be I have a really big programme and I load all of the memory of that programme into memory when I load my programme, right, so I basically go, oh, my programme is gigabytes of memory or something like that, and I will load all gigabytes of memory into my into my um memory, and then um I will wait until I've loaded literally every part of the programme into memory um before I do anything useful. The alternative is demand paging, which is basically like I will load into memory stuff when I need it, right, like as soon as I need it, it's there. Um, I'm going to load it, but that what that means is that if there are parts of the code that you never use or you use rarely or you don't use it immediately, you can just ignore them, right? Like so you basically can save memory and make everything really fast. The problem is that every time you get a new page, it is slow. OK, alright, I feel like I'm going round in circles here, um. So the basic definition, and this is what I mean by page fault, a page fault is when we have a a miss, right? So the translation look aside buffer is asked, hey, do you have this page and it says, no, I don't have this page, that's a miss. If the page is there, it's it's it, right? In essence, basically what we do is we go present is equal to zero, the page table entry is present, not present, right? Um, or it's protection violation, which means that like basically you asked for something that you're not allowed to have, and there's a couple of other ways that can stuff up. All right. What do we do? We hit a page fault, right, we need memory. OK, so we have to save the instruction that we just ran, right, because basically we've gotta say, hey programme, stop, stop, stop, stop, stop, stop. I need to load that memory in before you do what you were just about to do, right? And that's actually not a trivial thing to do, right, like as in like um this this is kind of difficult, but you know, not, not something that we have to worry about here. Um, then I run my page fault handler, right, in the OS and the IOS will do, you know, do something in order to work out how to handle the page fault, right? So, um, these are the different options. I'm sure everyone has done this at some point in time, you've asked for a page that doesn't exist, right, you've basically asked for an address that doesn't exist, everyone has actually done this, you get a six egg volt, right? Um. Yeah, if the page is missing, one of the things we can do is we can just find the page, read the page into the frame and update the PTE to valid, right? So say it's a valid page now, yay, page table entry. Um, and the other one is copy and write, um, which is basically where we ask for a page and we've written something to a page and we were previously looking at a page we already had loaded into memory, now we need to put a new page into memory with the new version of it. Um, copy and I page to the new page and update the page table entry, so you have two page table entries. OK, cool, um. There's also this idea of pre-agging, so the idea is that like, OK, if I load page one, maybe I could load page two. The page right next to it, because maybe they have some kind of, you know, logic to it. So that is one of the other things that we might consider. Alright, now to the meat of the lecture, alright. Who to keep, who to lose, alright, so. It's actually really kind of basic problem, right, which is, You've got a translation look aside buffer, and it's got 4 spots and you get regular requests from different parts of memory. What is the strategy, who do you keep and who do you lose? Right, this is like the entire point of this lecture. Number one, Right, this is literally a thing, right? So basically the idea is, um, in order to be able to evaluate whether an algorithm is good or an algorithm is bad, I need to know what good means, right? And so what we do is we have the optimal page replacement algorithm, right, and the optimal page replacement algorithm can see the future, right, like it literally knows what the correct answer is. So we will sometimes ask you to do this. We're like, what would it be if you ran the optimal page replacement algorithm? And you'd be like, oh well, I can see that in 12 page frames. I'll need page 5, so I'll store page 5 here, right, so that I, you know, in 12 page frames I'll need it, right? Um, it's perfect. Um, and yeah, as I said, the cons are being psychic is difficult, but basically, With every page uh page replacement algorithm, what we're gonna do is compare it to Optimal because Optimal is basically as good as we're gonna get, um, and then work out how good we are compared to Optimal. Um, yeah, OK, so, optimal, what does this look like? OK, so this is a classic thing that you're gonna see a lot of, or, You will see this in an exam, I'm, I'm not kidding, um, and probably in a tutorial, like the next tutorial, and probably in assignment too. OK, so what are we actually doing here? At the top here, we have a series of requests for pages, right? You'll notice that there are 5 numbers here, right? Um, and we've only got 4 spots, right, so the way that we traditionally, I don't know, this is just the easiest way for you to like physically draw it to explain the logic of what you're thinking. Yeah. So basically what we do is we say, oh I need number one, right, and so I put number one in my translation look aside buffer, and so this is time, yeah. Then someone requests 5, right, and so I can keep 1, and I can keep 5, I've got 4 spots, that makes sense, and I can keep 4 and I can keep 3. So far so good, right? Now someone requests 2, yeah. And the question is, Question for the audience, maybe we'll get a little bit more energy going. OK. Which page would we replace? Live Why would we replace 5? Right, 5 is at the very end, right? So if I ask for new pages, right, I'm gonna need to get 2 again, I'm gonna need 1 again, 2 again, 4 again, 3 again, right? And so 1234 are all there. I will need 12 and 34 before I'll need 5, right, so the answer is correct, right? So it is, you replace what you replace. Which one you replace not 1, not 2. All 3. Or not 4, not 3. 5. Yeah. So, right, OK, that's the optimal algorithm. If you understood that example, here's another version of it, with me doing what's right, um. Basically what you can see here is that like we can go all the way to the end now, we can go 1234124313421, and in all of these we don't actually need to do anything because they're all already there, right? And the first thing that we're gonna have to do is 5, right? And so the way that we would describe this if we were doing it in a, I don't know, in an exam, right, it would look like this, right, it would be like we have 5 misses, right, because each time we're changing one of the numbers, then we'd have a lot of hits cos we're not changing any of the numbers in any of these, um, and you'll notice that like, Here that this is the same as that, that's how we know that this is a hit. So basically everything where it's the same as the previous column is a hit, right, and then we have one miss. And so if we were mathematicians we'd be like 6 divided by 15 is 40%, yeah, wait, right. Any questions about the ultiable yes. Sorry what? It doesn't, right? Like, as in like, this is, this, this can't actually happen. You can't implement this algorithm. Like it's, it's, it's literally can see the future, right? Um, the only reason that we use this algorithm is basically to, you know, like, go, OK, well, how good could it be if I was psychic? Right? Um, and then I can be like, well, I'm pretty close to being psychic, that's pretty good, right? Um, but yeah, you're right, like this is not implementable, like, yes. It's, uh, you basically, the way the optimal algorithm works it works is it just says, I will get the page I need the furthest into the future, right? And that's 100%, like it just works out that that is the correct answer to the, um, what do you call it? Yeah, I know what you mean. Like sometimes you're like, hey, is there an edge case where this is not true? We'll see, we'll see some fun edge cases in a minute, um, of where things that feel like they don't make sense will actually happen, um, but yeah. Uh, in this case we're all good. OK. Next one is least recently used page, right? I wonder what it means. I think it means that we get rid of the least recently used page. OK, cool. So what does that look like? OK, the first part is gonna be really, really boring. I have different numbers here just for the sake of it, right, so you can see here that we've got like a miss, miss, and because then we're getting a 1 again, it's a hit, and then we get a 3 and it's a miss and another one which is a a miss, right? I did this just because, you know, like to, like, I don't know, uh, make sure that no one's just getting confused at the very, very start. Yes, you can miss things, or you can hit things, um, and not actually have to update the table, even if the table's not full. OK. So, now we're, we need a 4. Any guesses? which one we're gonna get rid of? It least recently, yes. Is it? Is it 5? The least recently used 10, it's not 2, it's not 3, it's not, it's not 10, it's 5. Alright, yeah, we got rid of 5. Well done. Alright, uh, now we're gonna put in ironically, a 5. So what one is gonna go now? One, right? I just got rid of 5, right? It's gonna be uh the 1, right, because it can't be the 4, the 2, or the 3, right? Um, this one's gonna be a hit, that's fine because the 4 is already there, 3's already there, now we're gonna need a 1. it's not gonna be a 5, or 4 or a 3, right? least recently used, he's gonna be 2, so we'll get rid of the 25431. And then we're good, we're good. Uh, can't be a 1 or 4 or a 3. Do. And then we're all good. And now, last guess is 5, which one, let's see, audience. Alright, let's let's let's get mass audience participation. Who thinks it is one? Come on, 16, OK, what about 2? Or Is it 3? Yeah, alright, alright. I don't need to be here anymore. Everyone understands basic English and what those words mean. I'm done. Alright, um. And so, right, this is where Optimal comes into place, right, so we did the optimal, I did, well I think I I it might be a slightly different example. No, it's the same example just I did with Optimal, right? And we can see here that the optimal algorithm has less misses than the least recently used, right? So we have something we can directly compare it to, right, so we can be like how good is least recently used and we'll be like eh. Uh Fine. OK. Now we've gotta sort of sit here and go, hang on a second, hang on a second, how do we make this? Right, how do we implement it, right, because, like, calculating the least recently used is a bit non-trivial, right? Like it's because we're like, oh, OK, now I've got to have a memory of all the things that have been used in the past. Yeah, that's kind of annoying. Um, we could use a time stamp, that feels really strange, um, we could use an access bit. Right, and this is what ends up happening in practise is that we're gonna, we're gonna, we're gonna use some hacks to try and approximate this rather than like, you know, like what we say is that when we, when we get a page, we go, hey you tag, and it'll be like, I have an access bit, my access bit is 1, ya, right? Um, and then occasionally we'll be like slap your access bit is 0, right? And then when we go to a page we'll be like, have you been accessed recently and it'll be like, I, I had, but then you slapped me and now it's 0, so it is not, so you can kick me out, right? Or another one could be like, ha ha, I've not been slapped yet. Yes, I can stay, right? Um, and that approximates LREU, right, so we can, you know, like we we're again we're in this position where we're like, oh OK, so the optimal solution, this is not the optimal solution, but like the, the idealistic LRU is too much of a pain to actually implement in practise, so we'll we'll come up with an approximation for LRU. Alright, um. So one of the solutions to this is this idea of the um the two-handed clock, right? Um, and so basically what you do is you go, oh, OK, you know, if we access you you're gonna become active, and then we've got this thing that goes around in like in a clock and it goes, you know, it'll go, it'll go around the pages and it'll be like, you, you are now inactive, you are an inactive, aha, you are already inactive, kick you out, right? um. All right. Any guesses what this stands for? Fly in, fly out. That's right. This is the miner's algorithm invented by miners in the 90, sorry, um, when to fly, that was in like the 1990s is when fly-in flayout became popular. Um, first in, first out, yeah, we've done this a million times. What do we think first in, first out means? Does it mean that the first one in is the first, OK, alright, let's see if I, I'm not even gonna explain how this works. Let's see if you guys can work out what the answer is, and then uh we'll we'll we'll, we'll run along with it. Alright, OK, so. 15. 13. You like the colouring, by the way, right? It's just a hint So which one do you think? It's gonna go next when the 4 goes in. Who votes 42? Ah Hey jokers, who votes for 5? Who votes for 3? Who votes for 2? I nearly got you. There is no 22 doesn't kick. Who votes for 1? Yeah, alright, we're done. OK, cool, lecture. No, um. So we replaced the one, right, and then again, which one are we gonna replace? Well, we don't have to replace anything, we don't have to replace anything, don't have to replace anything, don't ooh, ooh, it doesn't have a one. Which one are we gonna replace now? He votes for 2. And he votes for 3, and he votes for 5. Oh, see, I, I nearly tricked you into being waiting for me to say it at the end. I was gonna say 4, but I didn't. OK, so we're gonna get rid of 5. And then, I mean, Uh We got rid of 5. Such a bad idea. Which one are we gonna get rid of now? 3 Really? OK, cool. You calculator hits and misses. 6 on 15, 0 yeah, it's not bad, 0.4, um. All right. Why is this bad? Right, so I've written up here, it needs more memory, so let's have a, let's interrogate that a bit, like why do we need more memory? It means that once something's in, I have to pay attention to it, and I like go win, OK, OK. You, you, I know you are the first in, I have to tell everyone that you're the first in. I have to keep them in order. I have to like, um. One of the other problems is, and I like, I think, I think the easiest thought experiment to run with a FIFO and to to realise why it's bad. Imagine I have 4 pages and I'm running in a loop, and I ask for 123451234512. OK, so what's that gonna look like? OK, right. I'm just going 1 to 5 over and over again. Well, I'll I'll do 1234, and then I'll be like, oh, which one should I kick out? I'll kick out 1 and then put in 5. What do I need now? I need 1 and I'll kick out 2. What do I need now? 2, and I'll kick out 3, right? So basically I'm constantly kicking out the one that I will need next, right? And if you can imagine from a programming perspective, Accessing the same set of pages in a cycle feels pretty likely because most programmes that run for a long period of time kind of end up in these long cycles, right? So if they're using slightly more pages than you have TLB and you use FIFO, you will have like categorically bad performance, right? um. Oh, that's not this one, OK, so, yes. Alright, um, one last thing that I wanted to like um talk about um is what's called, Bela is anomaly. Right, OK, so what's the anoma any guesses what the anomaly is. Like, what is, what what do we like what is bad about this picture, yes. Yeah, exactly, right, so the answer is, the TLB is smaller, I'm using less, I'm storing less pages, and yet somehow, I'm actually, Getting more hits with less pages, right? um. So Belay's anomaly is basically it's a sort of like a consequence of this where like basically, as I said, for a FIFO buffer, I think it's just for FIFO, right, um. You end up with a situation where it is possible by giving yourself more entries you end up with actually worse performance, which feels very counterintuitive. With the optimal algorithm, this would never happen, right? With the optimal algorithm it will use every entry to its maximum capability. With the least recently used it will generally improve as well. um, but with the. So there are situations where actually having more memory is bad for you, right? And you can sort of see, you know, in this example what happens, right, it's like, like it's a very confected example because you can see here it's like, OK, so it's 32153, right, so we're like trying to get 3 into the memory, right, so we kicked out 2, that, you know, like we've kicked out 2, and then we kick out we no so we hold on to 2 and then we kick out 5, right? Um. And what that means is we get like two goes in a row where it's OK and then we got another one where it's a row, like in a and it's uh in a row and it's OK. And, and basically the idea is because we're using FIFO, we can sort of like manipulate which ones are gonna be kicked out, right, by by making them the first in, first out. Um, all right. Do I even need to explain how this algorithm works, right, like again. Right? We got, what, what do we think it means? We have some pages in memory and one of them disappears, right, and we don't care which one, right? Um, it's actually not bad, right, like a random page replacement. Um, algorithm, it's not bad, and there should be a reasonable intuition behind why this is the case, right? It's because sort of statistically, the longer you're in the queue, the more likely you are to get kicked out. Yeah, right, so it's gonna have some kind of like interesting um. Uh, what do you call it, uh, properties, there's this random one, but I, I, yeah, I don't think I actually have a, a, a, a example of the random one, cause I don't know how I would ask that in an exam. Other than giving you a series of the answer, right, you know what I mean I'll be like, OK, here's a series of random digits of ones to kick out, you kick out, like I mean a child could do that exercise. Um, OK, so this is, you know, we have optimal, it's impossible, we random, it's cheap and it's kind of OK, um, LIU, which is pretty good and you can approximate it, um, so that's not actually expensive to do, and FIFO, which is easy but can be really, really bad. So. One of the things that we need to understand is again we're in this operating system space which means that you're like, even if you have a theoretically good solution, if your theoretically good solution takes physical time to calculate, then it's, it becomes bad, right, even if you know what the right answer is, and you can calculate the right answer. Sometimes it is just faster to guess, you know what I mean, if you're if you're gonna guess like, 75% of the time, right, you'd be like uh 12, right, you know what I mean, um, and that gets you right 75% of the time because the answer's 12, 75% of the time or something like that. That algorithm where you just guess is actually faster than one where you actually try and calculate the real answer, um. There are a couple of other ways that we can look at these algorithms, like if we want to make them more and more sophisticated, so one of them is that, you know, like you can, you know, With the frequency of use, so the idea is they're like, oh OK, OK, OK, I'm not just gonna kick you out, you're not least recently used, or you're not, you know, like, you're not a good page if I've only used you once. If I've used you many times, then I'm gonna hold on to you, right? And so you basically sort of track how often you're using pages. Now the problem with this, obviously, is that now you're adding all this extra sort of like thinking into your process for kicking out pages and and that could be costly itself. Um, so we have LRRU2, right, and basically that means that it's the least recently twice used. Right? Um, One of the ones that I think is kind of interesting, right, like you, you can track multiple victims and whatever there's there's one of the one of the things that you can do as well is you can sort of like kick them out to a, a list of of ones you're like, I kicked you out, but it's only like a fake kick out, right? I might need to get you back just in case, right? um. And the example for this is that like, imagine now that you've uh you know, someone's modified the page and now you have to write it out to discs. So you put it on a list and you're like I'm gonna have to write this out to disc, what a pain. Oh wait, they need the page again. I don't have to write it out to disc, yay. Um, so that's the kind of, um, logic, and yeah, there's demand zero list which is you keep a short list of empty pages to avoid security problems, right, like, um, and the, the idea there is that like um. One of the problems that you can have in memory. Because now we've got, now now we've got to think about this from a security point of view. I have a translation look aside buffer and someone's like, hey, could I have, I could I have a page? Can I, can I just read your old page, right? Um. Sometimes, you know, like you have to zero things out um before or after, depending on where you read them, right, um, to prevent someone trying to access that same bit of memory. So like you can imagine that you have a programme, it's like it loads a bit of memory in and then you've got a new programme, it's like, can I have that memory, and then it starts reading that memory and you're no longer secure, right? Um, and so, you know, occasionally you're gonna have to have like some 0 pages. Like one of the things is you have some zero pages so you can be like, hey, that's the zero page and I'm gonna zero these other pages out while you're not watching. Um, so that's another way that you can do it, um, and yes, uh, the last one is evicting non-dirty pages, which I talked about before, right? So the idea is that if you have a page that hasn't been written to, You can kick that out of a translation look aside buffer with no consequences, but if it has been written to, then when you kick it out, you're gonna have to write that information back into disc, right, so you can just have a priority for ones where it's easy to kick, you know, kick them out. Um, Yeah, um, you're putting them together, there's a couple of ways that you can do it, you know, you can have it have rules for them only being able to kick out their own pages, so like if each in your translation look aside buffer, you've got a bunch of processes and they have access to pages and they sort of have a page, like pages they're allowed to have, um, you can have it so that the, you know, when you kick out a page, you're kicking out your own page rather than kicking out someone else's page, um. And then, yeah, like, variable space algorithms. It says what it's um basically what it's gonna do, but like the idea is that, Sorry, there's a typo in there. I have to fix it. Um, Yeah, the processor can grow and shrink depending on the number of pages, right, like as in so you'd be like, oh, OK, so you can have more or less um space. Um, the problem with that kind of thing is that you can have one person's like, I would like all of the pages. Yes, I would have all of the memory, right, um, so you've gotta, you gotta be a little bit careful. Um, Much do we have left? We don't have much left, I don't think after this. There's a lot of slides, but it doesn't take long to get through. Um, it takes a long time to make. Alright, alright, so. We took, I think we're basically done, right, like as in, so as long as you understand LIU and FIFO and Optimal and what random is and potentially we're willing to think about maybe some other algorithms yourself, right, that is to say, the kind of thing that I might do as a complete jerk, would be like, imagine an algorithm that just page replacement like this. Why would it be good or why would it be bad, right? Um, then, uh, you should be fine, uh, for assignment 2 and for work going forward. OK, so let's talk about thrashing. What is thrashing? Thrashing, um, that's what happens when someone's beating you up, no, um. Oh I don't know. I'm sure there's some, mm. I'll say for work, um, let me think, alright, so, right, OK, so the problem is, the problem is, the logic is, OK, I have memory, right, and I have a lot of processors and they all want pages of memory, right? OK, so what's gonna happen? Well, How do we know this is happening? What happens is, it says low user CPU usage. OK, so what does that mean? So basically, You have all these processes and none of them are getting anything done, right, but you have high CPU usage, right, which means the OS is doing a lot of work. So what does this actually mean? What does thrashing mean? OK, imagine that I'm asking for a page and then I page roll, and the OS is like, oh, I must get another page, yeah, and another person goes, I want a page, and then it page rolls. Right, and then another person goes, I want a page, and the page folds. And you've got like hundreds of processors and they're all page faulting, and the OS is like, alright, all I'm going to do for the next 10 minutes is change one page for another page and change one page for another page and one page for another page and 1 page for another page, right? um. Now we have a situation where all the CPU is doing is servicing page holds. That's all it's doing, right, it's literally doing nothing else, it's just doing this um, Um, thing and so what you see with multi-programming, right, and it's just the way that it works is like in terms of performance, like you can imagine I've divided my programme into threads and I'm, I'm running more programmes, I'm running more programmes. I'm I I've multiprocessed it, right, and I get more and more done and then at some point in time I'm using just enough of the resource that the performance goes to hell. Has anyone here done CNA before? 12, OK. In CNA there's a similar thing for Ethernet, right? So when you, when you, when you send messages out on ethernet, right? Um, people use the ethernet in the day, right? Basically the way Ethernet says is go, I'm gonna send a message, and I don't care. I'm I'm just gonna send messages and if there's a problem then I'll send it again. I'll wait a random time and send it again. And that works really, really well until you get to high utilisation, right, and OS is not the same, basically it's a similar kind of idea. Once you get to high utilisation, there's so much crap going on that managing the crap becomes the the job of the OS that's all it's doing, it's just managing crap, um, and it's not actually doing any real work and other processes are actually running. um. Yeah, I already have page fold exceptions, it's not mapped, it's not resin and access violation, you can look that up in your own time, um. Yeah. Yeah, this is the bit where we get to the, the, the, the fun part. um, in assignment one, in assignment one, part one, we were redirecting some signals, right? Mm mm mm mm mm. There are other signals, who here has done signal processing? No signal processing, sorry, uh, systems programming. Yeah, cool. Alright, which, which, which, which signal do you never redirect? And he Eg fault, right? Never redirect the fault, you can, right? So. In practise, right, when there is a page fault, right, one of the page faults that you can get is a seg fault, right, and, and that's basically go I asked for a page that's not in the memory I'm allowed to get to have, right? And so some of these are actually some of these, you know, page faults are handled by the OS, so I go, excuse me, I'm going to try and get a page, the, the, the what do you call it, the paging system goes, that page is uh not uh what is it? It's an invalid page, you're not allowed to have an invalid page, and so it sends a signal back to the process. Standard C would say that the first thing that you do when you receive that programme, or you know like the C compiler when it compiles a programme, its default behaviour. What it does naturally when it receives that signal is to just kill the programme, right? But you can actually write, you know, signal handlers like we have for assignment one where when you receive the fault you do something first, right, you're like, oh, my programme sucks, but I'm going to save up some data first before, you know, like something really bad happens, right, or some kind of log or some. You know, like you, you, you change it in some way that you can be like, oh, I just did something wrong, I'm not gonna do that thing I just did again, um. But it's, that's very I estimate. Um, The other sort of random, I don't know, it didn't really fit in with the flow of the rest of the lecture, so this is this idea of the guard page, right, and this is actually really common in like operating systems that you have a guard page, um, that you basically say that 0, The 0 page isn't a legal page, right? And what that does is it means that like if someone hasn't defined memory properly and they go, oh, could I please have zero, which is actually a pretty common thing to do, it goes to a page that is restricted, that's basically you can't write to this page, the one page you can't write to. And this is where sort of like the null point exception kind of idea comes from, right? Like it's basically you have pointed to nothing, right, cos null is zero, right? So it's going, you have tried to point to 0 and 0 is a kind, a very special kind of guard page within my operating system, and so, um, you know, uh, you can get around it, right, and so you can, Basically, if you have a null reference, you can be like, oh actually that's just a page fault because you tried to reference the null page. um. But you can, you can play tricks with it because the first page might be 496 bytes big, so if you access number 496, you might get the next page, like the first page after the null page or something like that. So it's not foolproof, so you have to do some other uh checks as well. Um. One of the other ways that guard pages are used are in threads, right? So the idea is that like basically what you do is that you've got a process and in between where the threads are of the different stacks, right, because this is what a multi-threaded programming looked like in memory space. Um, there is a 00 guard page, right, and so if you try and access that guard page it stuffs up. And so as it tries to grow, if the stack tries to grow into a place that's not meant to grow, then it hits this guard rather than hitting the other programme, right, which is, I think, probably better, right, like as in you hit this thing and it says exception you've stuffed up rather than I'm just going to overwrite random parts of the stack in the other part of the programme, um, and then it tries to return in that stack and then does all kinds of nonsense. um. But what's really cool about this is it means that you don't even need to have, it's not like you have a Xero page or anything like that, you just have a page table entry that says uh this is a fake page, right? Um, so it's a kind of cool way of dealing with threads, um. Yeah, the other way is that you can have watch points, right, like so you can set certain pages so that like basically they're like, hey, I'm a special page, and if someone tries to access the page then you're like, hey, stop, right, so you can use them as a kind of like um. It triggers an exception and then from the exception because they're trying to access a page that is protected, you can then pay attention to it and then write some code around it and be like, hey, something happened and pause your programme, so you can sort of debug it a bit. Um, Oh yeah, no, and then there's this other idea of like file mapping, right, like this is where you start getting really, really, So like hacky, I guess. I don't know, like, I've never done anything like this and I don't know why I would ever do anything like this. So, you know, I'm letting you know that this exists, but I, I don't know what it's for. Um, basically the idea is what you could do is you could like map a file into RAM as a page, right? So you'd be like, hey, I have a, I have a file, and now I'm going to like just map it into my memory and say you're a page of memory now, right? Um, and, and so now you can like, um. I don't know, do some shared memory stuff. Like it sounds like, I don't know. This is something that you should Google in your spare time. I don't think this is a major outcome of operating systems. It just exists, right? So basically, but basically the idea is that like, if you want to have to process that sharing a file really efficiently, you can pretend that the file's not a file anymore and load it in as like a bit of memory into a page and then both have them read from that same page and then pray that your programme doesn't just immediately die. Um, Yeah, and there are other ones as well, yeah, so this is an example of like a snapshot, which is where you like you you basically map a bit of memory. To a file Oh yeah, yeah, so the idea is what you do is you, you like, if you can imagine if you've got a process, you sort of like take all the memory of the process and then you copy it into a file and then that will allow you to reload from the file that process halfway through running, right? Um, at least if you've got all the important parts of it, right, like there's a little bit more to it than just that. But basically the idea is that it's just a way of like saying, instead of loading it into memory, I could load it to disc essentially or I could load it into a file, right? Um, and so what you could do is you sort of like, like if you can imagine you've got like some thing that runs and. and runs and runs and runs and runs and runs and it's meant to run for like weeks and then you every so often you take a snapshot, you save that snapshot to a file so when you have a power outage, you can just reload from that snapshot and start your simulation halfway through, right, so it's a, it's a bit like that, um. Oh yeah, and there was this thing about persistent programming. So, there are some like, OK, this is, this is, this is like, Really out there, so um I was listening to the lecturer from last year, right, and I don't know if this is actually part of OS or he's just talking random stuff, but I thought it was kind of interesting, um, Basically, there's this thing called persistent programming, right, so all of us do not exist in this world, right, like as in every single one of us has a very basic expectation that when you turn the computer off it basically dies and memory disappears, right? There are different research folks whose interest are in programming and computers where everything is constantly stored forever, so that you can like, Basically, once something is into CPU it would never get cleaned for any reason, it would never like, you know, like, um, and this has become I think more and more like viable because originally part of the reason that you have things like RAM is because RAM if you leave RAM for long enough, it would just flip bits for no reason, um. Nowadays I think that that's no longer as much of an issue as it used to be, and so like you could have these sort of like programmes that sort of assume that the computer never resets anything, which is kind of an interesting sort of um thing to have a look into. Um, Yeah, and this is it, this is like orthogonal persistence, right, like, you know, like what if you just wanna keep stuff around, right? Like you just like your heap objects don't disappear at process completion, right? Which is real like it's a really, really kind of weird way of thinking about it. You're like, oh, OK, so I have this programme and it's running on the stack, but when I allocate something to the heap, it stays in the heat, and then I have another programme that knows that it stays in the heap and asks for that same page and then does stuff with that memory, right? Like. I'm sure you might be able to do something with it, like it's like for those of the 99% of you who are gonna go off and programme in Java, C, and Python and C++ and maybe JS or something, I don't know what else, um, it's probably not relevant, but it might be something that you might be interested in. Don't know. Um, but it allows you to like, you can do it for loading huge data sets and you can make it so that your programme is, um, like crash resistant, um. Uh. Yeah, and this is, oh yeah, alright, OK, so again more an advanced topic but something to think about if this is something that piques your interest. Basically the idea is, OK, so we've got this idea of translation look aside buffers and pages and dirty pages and all this stuff. What happens when we have multiple cores, right? And you're like, oh, OK, right. If I have multiple cores and I've loaded something into memory and in my core I write something to a page, then I have to tell everyone else. Right, that that page has been violated, right by me, right? So there's a lot of, you know, like we've everything that we've done so far kind of like it it kind of assumes a bit of multi-programming but not like really deep multi-program. It doesn't really assume that we, you know, like we're, we've got multiple cores with multiple translation look aside buffers, right, because that's what we would have, we have multiple cores with multiple translation look aside buffers and if they're, You know, if you have two cores and they've loaded a process, and both of those processes are looking at the same bit of memory, those. Cas don't talk to each other. Sorry, but those, oh, is that the last page? No. Um, but those cores don't talk to each other, then I'm gonna have like a lot of like hassles and so there are, when you have multiple cores there are further optimisations that you could run on top of what we've looked at today, um, but yeah, I think that's it. I think we're at the end. Wow, that didn't take too long. Um, we have translation look aside buffers, right, everyone remembers TLB, right, um, we have, you know, some issues about size and, you know, the hardware and the operating system implementation, um, but mostly. No, this is, uh, we did not talk about these things. We did not, sorry, I'm gonna reupload these lecture slides. Alright, there we go. We talked about page replacement algorithms, alright, alright, maybe what I'll do is I'll just for the sake it I'll be like, hey, what did we do? Which ones? Optimal, yeah. At least recently. U LRU. And the last one was. First test, first in, first out, first in, first out. There we go. Interactive lecture, there we go, you get to write them as you go, and oh yeah, and random. Yeah, OK, cool, um, any questions? Excellent. All right, um, have a good weekend, everyone. Um, I'm going to be doing. Consultation for the next 2.5 hours, I guess, um, just in case anyone, I, I really shouldn't have done it on a Friday afternoon now that I think about it, I should have had my consultation earlier in the week, yes. How do I know the size of the page I'll set? So, so the page offset is gonna be what's left over, right, like, so essentially like, Every, every, every page address is gonna come, you get two numbers, right? So one number is gonna be like which page is it within my page table, right, and then that's gonna, that's basically gonna replace that part with another number, yeah, and then the offset is just gonna be the end of your original memory address. So like, um, the example would be like, let's say you've got like 32 bits and it's a multi-level page table and the 1st 10 bits are, you know, page table 1 and the next 10 bits are page table two, and the last 12 bits are the offset, right? Yeah, so that that the 12 bits at the very end of the string will tell you within the page, how how how deep you have to go, and that just coincides with to the power of 12, which is 4,096, right? But Uh, it's always smaller than the virtual address. Well, yeah, I mean you basically come up to some mathematical constraints there, right, because like basically it will depend on your page size, right? So you can't have more offset, like your, the offset has to be how big the page is, right, so that you can access every bit of the page, otherwise you've got like dead memory or you're gonna access beyond the end of the page. Yeah. Right. I will pause the recording.

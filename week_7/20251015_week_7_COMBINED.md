<!-- 
Combined Study Guide
Generated by LectureQ
Date: 2025-10-15 23:43:39
Week/Module: Week 7
Files Combined: 3
Content Type: Mixed
-->

# Week 7 - Complete Study Guide

> **Auto-generated combined study guide**  
> Generated from 3 individual notes files  
> Date: October 15, 2025

## üìö Table of Contents

1. [Week 7: Lecture 7 - I/O Devices, Hard Drives, and Solid State Drives](#week-7-lecture-7---i/o-devices-hard-drives-and-solid-state-drives)
2. [Week 7: Textbook Devices - Comprehensive Study Notes](#week-7-textbook-devices---comprehensive-study-notes)
3. [Hierarchical headers for each major section](#hierarchical-headers-for-each-major-section)


---

# Week 7: Lecture 7 - I/O Devices, Hard Drives, and Solid State Drives

I need to create comprehensive lecture notes for a student who missed class on Week 7: Lecture 7 about I/O devices, hard drives, and solid state drives.

Let me analyze the requirements:

1. I must synthesize ALL information from both slides AND transcript
2. Include EVERY concept, example, and explanation
3. Rephrase naturally without copying verbatim
4. Make it educational and readable for a student to learn everything from these notes alone

I'll structure this as:
- Clear H1 title with week/lecture info
- Overview section after the title
- Hierarchical headers organizing content logically
- Key insights from transcript formatted in specific way
- Summary at the end with key takeaways

Let me identify all the important concepts and organize them:

From slides:
- Canonical Device (registers, status/command/data)
- Direct Memory Access (DMA) vs Programmed I/O
- Hard Drives (tracks, sectors, cylinders, seek/rotate/transfer times)
- Solid State Drives (SSD), NAND flash memory
- Device Drivers (top/bottom half)
- Lifecycle of an I/O request
- Everything is a file concept
- Timing paradigms (blocking/non-blocking/asynchronous)

From transcript:
- Historical context (physical switches, punch cards)
- Bus architecture (PCI, USB controllers)
- Interrupts vs polling
- Port-mapped vs memory-mapped I/O
- DMA controller explanation with analogy to graphics card
- Device driver complexity (70% of Linux code is drivers)
- Hard drive calculations and performance metrics
- SSD mechanics (pages, blocks, erase operations)
- Flash memory physics (floating gate transistors)
- Wear leveling concepts

Now I'll create a logical structure for the notes:


### Overview
This lecture covers fundamental I/O device concepts including canonical devices, direct memory access, hard drives, and solid state drives. We explore historical context of I/O systems, how devices communicate with the CPU through registers, the limitations of programmed I/O, DMA as a solution to these limitations, and detailed mechanics of both traditional hard disk drives (HDDs) and modern solid-state drives (SSDs). The lecture also covers device drivers, the "everything is a file" paradigm in Linux, timing paradigms for I/O operations, and how storage devices are scheduled.

Now let me organize all content into proper sections with natural flow:

1. Historical Context of Input/Output
2. Device Architecture & Communication
3. Canonical Devices (Registers)
4. Programmed I/O vs DMA
5. Interrupts vs Polling
6. Protocol Variants: Port-Mapped vs Memory-Mapped I/O
7. Device Drivers
8. Lifecycle of an I/O Request
9. Everything is a File Concept
10. Timing Paradigms for I/O Operations
11. Hard Disk Drives (HDDs)
    - Physical Structure & Mechanics
    - Performance Metrics (Seek, Rotate, Transfer)
    - Calculations Example
12. Solid State Drives (SSDs)
    - NAND Flash Memory Basics
    - Page/Block Architecture
    - Write/Erase Operations
    - Wear Leveling
    - Comparison to HDDs

I'll now create detailed notes for each section, synthesizing both slides and transcript while making it educational.

For the Key Insights from Transcript sections, I'll format them as specified with:
- The insight rephrased clearly
- *Instructor Emphasis* (what they stressed)
- *Student Question* if applicable
- *Practical Example* used

I need to be careful not to skip any algorithms, examples, calculations or key insights.

Let me start drafting the notes...
</think>

# Week 7: Lecture 7 - I/O Devices, Hard Drives, and Solid State Drives

### Overview
This lecture explores fundamental concepts of input/output (I/O) systems in operating systems, focusing on how devices communicate with the CPU, the mechanics of storage devices, and modern implementations. We'll cover historical context for I/O systems, canonical device interfaces, direct memory access (DMA), interrupt handling, hard disk drives (HDDs), solid-state drives (SSDs), and the "everything is a file" paradigm in Linux. The lecture emphasizes practical considerations like performance metrics, scheduling algorithms, and how modern storage devices overcome physical limitations.

---

### Historical Context of I/O Systems

#### Early Input/Output Methods
The earliest computers used **physical switches** for input and **lights** for output‚Äîeach switch represented binary values (0 or 1), allowing users to directly program the machine by flipping switches. This was extremely labor-intensive, as each instruction required manual configuration.

By the 1970s, **punch cards** became common: holes in cards represented instructions (e.g., a card with specific patterns of holes encoded programs). A physical reader would detect these holes to input data into the computer, and output could be printed onto new cards. This was still slow but more efficient than manual switch configuration.

#### Modern I/O Architecture
Today's systems use **buses**‚Äîphysical pathways connecting components like CPU, memory, storage controllers, and peripherals (keyboard, mouse, monitor). Key bus types include:
- **PCI Express**: High-speed interconnect for graphics cards, SSDs, etc.
- **USB**: Universal Serial Bus for peripheral devices.
- **SATA**: Storage interface for hard drives.

Modern systems feature a **centralized architecture** where the CPU communicates with device controllers (e.g., disk controller) via standardized registers. This abstraction allows different devices to operate through consistent interfaces despite physical differences.

---

### Device Communication: Canonical Devices

#### The Core Concept
All I/O devices communicate using three fundamental **registers**:
- **Status Register**: Indicates whether a device is ready (`BUSY` or `READY`)
- **Command Register**: Specifies actions (e.g., "read," "write")
- **Data Register**: Holds actual data being transferred

This model abstracts all devices into a consistent interface, regardless of physical differences.

#### Example: Writing to a Device
```plaintext
while (STATUS == BUSY) ; // Wait until device is ready
Write data to DATA register   // Load the data to send
Write command to COMMAND register  // Tell device what to do
while (STATUS == BUSY) ; // Wait for completion
```

This process‚Äîcalled **polling**‚Äîis inefficient because it wastes CPU cycles while waiting. The CPU spins in a loop checking `STATUS` until the device is ready.

#### Key Insight from Transcript
> "Polling is like standing at a restaurant counter, repeatedly asking 'Is my food ready yet?' instead of sitting down and waiting for someone to call your name."

*Instructor Emphasis*: "Polling wastes CPU time. Interrupts are better because they let the CPU do other work while waiting."

*Student Question*: "Why not just have the device tell us when it's done?"

*Practical Example*: Imagine a waiter taking orders at a restaurant:  
- **Polling**: You keep asking, "Is my food ready?" every 30 seconds.  
- **Interrupts**: The waiter signals you when your meal is ready (no wasted time).

---

### Programmed I/O vs Direct Memory Access (DMA)

#### Problem with Programmed I/O
In traditional **programmed I/O**, the CPU directly copies data between memory and devices:
```plaintext
CPU: Copy byte from device ‚Üí RAM
CPU: Copy next byte from device ‚Üí RAM
...
```
This is inefficient because the CPU must manage every single byte transfer, wasting time on low-level tasks.

#### DMA Solution
**Direct Memory Access (DMA)** offloads this work to a dedicated **DMA controller**, which handles data transfers directly between devices and memory. The CPU only needs to:
1. Specify source/destination addresses
2. Indicate how much data to move
3. Wait for the DMA controller to signal completion

The process looks like:
```plaintext
CPU: Set up DMA (source, destination, size)
DMA Controller: Copies data directly between device and RAM
DMA Controller: Interrupts CPU when done
```

#### Key Insight from Transcript
> "Think of DMA as a specialized courier service. The CPU says 'deliver this package from point A to point B,' then goes back to work while the courier handles the delivery."

*Instructor Emphasis*: "DMA is critical for performance because it frees up the CPU to do other tasks during data transfers."

#### Practical Considerations for DMA
- **Setup overhead**: The CPU still has to program source, destination, and length registers‚Äîusually a few microseconds.
- **Tiny transfers**: Moving a handful of bytes may complete faster with programmed I/O than paying the DMA setup cost plus the interrupt on completion.
- **Break-even size**: Typically 64‚Äì256 bytes depending on hardware. Above that threshold the CPU saves time by letting DMA stream the data.
- **Channel contention**: Legacy buses had limited DMA channels; modern PCIe devices arbitrate for bandwidth but can still throttle one another under heavy load.

> üìè **Guideline**: Use programmed I/O for register pokes, DMA for bulk transfers like disk blocks, network packets, or video frames.

---

### Interrupts vs Polling

| **Polling** | **Interrupts** |
|-------------|----------------|
| CPU repeatedly checks device status (wastes cycles) | Device signals CPU when ready (no wasted time) |
| Simple but inefficient for slow devices | Efficient but requires hardware support |
| Good for very fast devices with minimal wait times | Standard approach for most modern systems |

*Instructor Insight*: "Interrupts are the foundation of multitasking in operating systems. Without them, your computer would be constantly checking if a key was pressed or a mouse moved."

---

### Protocol Variants: Port-Mapped vs Memory-Mapped I/O

#### Port-Mapped I/O
- Devices have dedicated **I/O ports** (addresses separate from memory)
- CPU uses special instructions (`IN`, `OUT`) to access them
- Simpler hardware design but less flexible

#### Memory-Mapped I/O
- Device registers are treated as part of the **memory address space**
- CPU accesses devices using standard memory read/write operations
- More flexible and allows for better integration with existing systems

*Instructor Insight*: "Memory-mapped I/O is why Linux treats everything like a file‚Äîyou can `read()` from a device just like you'd read from a file."

---

### Device Drivers: The Bridge Between OS and Hardware

#### Top/Bottom Half Architecture
Device drivers are split into two parts:
- **Top Half**: Handles high-level operations (e.g., reading/writing files)
- **Bottom Half**: Manages low-level hardware interactions (registers, interrupts)

*Instructor Emphasis*: "70% of Linux kernel code is device drivers. This shows how critical they are to system functionality."

#### Key Insight from Transcript
> "Device drivers are the most complex part of an operating system because they must handle both high-level abstractions and low-level hardware quirks."

---

### Lifecycle of an I/O Request

1. **Request**: Application asks for data (e.g., `read()` syscall)
2. **Queueing**: OS queues request to device driver
3. **Setup**: Driver configures DMA controller with source/destination
4. **Execution**: DMA transfers data directly between memory and device
5. **Completion**: Device interrupts CPU; driver signals application

This process ensures the CPU is never idle during I/O operations.

---

### Everything Is a File: The Linux Paradigm

Linux treats all resources as files:
- Hard drives ‚Üí `/dev/sda`
- Network interfaces ‚Üí `/sys/class/net/eth0`
- Input devices ‚Üí `/dev/input/mouse0`

*Instructor Insight*: "This is why you can `cat` a device file to see its contents‚Äîit's literally just another file."

---

### Timing Paradigms for I/O Operations

| **Paradigm** | **Description** | **Use Case** |
|--------------|-----------------|--------------|
| **Blocking I/O** | CPU waits until operation completes (e.g., `read()` blocks) | Simple, predictable behavior |
| **Non-blocking I/O** | CPU continues execution while waiting for completion | High-performance servers |
| **Asynchronous I/O** | CPU registers callback to handle completion later | Modern web applications |

*Instructor Insight*: "Blocking I/O is like ordering food at a restaurant‚Äîyou wait until it's ready. Non-blocking is like getting a text when your order is done."

---

### Hard Disk Drives (HDDs)

#### Physical Structure
- **Platters**: Circular disks coated with magnetic material
- **Read/Write Heads**: Move across platter surface to access data
- **Tracks**: Concentric circles on the platter
- **Sectors**: Divisions of tracks (typically 512 bytes each)
- **Cylinders**: Sets of tracks at same position across all platters

#### Performance Metrics: Seek, Rotate, Transfer

| **Metric** | **Description** | **Typical Time** |
|------------|-----------------|------------------|
| **Seek Time** | Time for head to move to correct track | 4‚Äì10 ms (average) |
| **Rotational Latency** | Time for desired sector to rotate under head | ~2‚Äì5 ms (for 7,200 RPM drives) |
| **Transfer Rate** | Speed data moves from disk to memory | 100‚Äì200 MB/s |

#### Performance Calculation Example
*Problem*: How long does it take to read a 16 KB file from a "Cheetah" drive with:
- Average seek time: 4 ms
- RPM: 15,000
- Max transfer rate: 125 MB/s

**Solution**:
1. **Seek Time**: 4 ms (given)
2. **Rotational Latency**:  
   $ \frac{60\text{ seconds}}{15,000\text{ RPM}} = 4\text{ ms per rotation} $  
   Average wait time: $ \frac{4}{2} = 2\text{ ms} $
3. **Transfer Time**:  
   $ \frac{16\text{ KB}}{125\text{ MB/s}} = \frac{0.016\text{ MB}}{125\text{ MB/s}} = 0.000128\text{ seconds} = 0.128\text{ ms} $

**Total Time**: $ 4 + 2 + 0.128 = 6.128\text{ ms} $

*Instructor Insight*: "This is why SSDs are so much faster‚Äîthey eliminate seek time and rotational latency."

---

### Solid State Drives (SSDs)

#### NAND Flash Memory Basics
- **No moving parts**: Uses flash memory instead of spinning platters
- **Data stored in pages** (typically 4 KB)
- **Pages grouped into blocks**
- **Blocks erased before new data can be written**

*Instructor Insight*: "SSDs are like a digital sticky note pad: you can write to blank sections, but to overwrite existing content, you must erase the entire block first."

#### Write/Erase Operations
| **Operation** | **Description** | **Time** |
|---------------|-----------------|----------|
| **Read Page** | Read an entire page (4 KB) | ~20‚Äì100 Œºs |
| **Write Page** | Write to a blank page | ~200 Œºs ‚Äì 1.7 ms |
| **Erase Block** | Erase an entire block before writing new data | ~1.5 ms |

*Instructor Insight*: "SSDs are faster than HDDs because they eliminate mechanical movement, but their write operations require careful management to avoid wear."

#### Wear Leveling
- SSDs have a limited number of erase cycles (typically 3,000‚Äì100,000)
- **Wear leveling** distributes writes across the drive to prevent premature failure

**How the Flash Translation Layer Spreads Wear**
- Tracks erase/program counts for each block so hot data (logs, swap) is rotated across the device.
- Remaps logical block numbers to new physical pages on every rewrite, keeping the OS oblivious to the movement.
- Performs background garbage collection to relocate cold data onto older blocks, preserving fresher cells for future writes.

**Extra Capacity for Durability**
- SSDs include spare flash (over-provisioning) beyond the advertised size to replace failing blocks and give the FTL room to juggle data.

**Why Writes Lag Reads**
- Reads simply sense charge (~25 Œºs), whereas writes must erase an entire block (~1.5 ms) before programming (~200 Œºs), so small random writes are the worst case without buffering and coalescing.

*Instructor Insight*: "Without wear leveling, an SSD would fail after just one year of normal use. Wear leveling ensures even distribution of writes."

#### Flash Memory Physics
SSDs use transistors with a **floating gate**:
- **Electrons in floating gate**: Represents `0` (no current flow)
- **No electrons**: Represents `1` (current flows)

*Instructor Insight*: "Writing is easy‚Äîjust add electrons. Erasing requires high voltage to remove all electrons, which wears out the device."

---

### SSD vs HDD: Performance Comparison

| **Metric** | **HDD** | **SSD** |
|------------|---------|---------|
| **Seek Time** | 4‚Äì10 ms | ~0 ms (no movement) |
| **Rotational Latency** | 2‚Äì5 ms | N/A |
| **Transfer Rate** | 100‚Äì200 MB/s | 500‚Äì3,500 MB/s |
| **Power Usage** | Higher (spinning parts) | Lower |
| **Durability** | Fragile when moving | Shock-resistant |

*Instructor Insight*: "SSDs are faster because they eliminate mechanical movement. This is why your laptop boots instantly compared to an older HDD-based system."

---

### Scheduling Algorithms for Storage Devices

#### First-Come, First-Serve (FCFS)
- Simplest algorithm: Process requests in order received
- **Problem**: Poor performance with random access patterns

#### Shortest Seek Time First (SSTF)
- Prioritize requests closest to current head position
- **Advantage**: Reduces average seek time
- **Disadvantage**: Starves distant requests

*Instructor Insight*: "Imagine a delivery driver who always takes the nearest package first. They'll deliver many packages quickly but might never reach the farthest one."

#### SCAN (Elevator Algorithm)
- Move head in one direction until end of disk, then reverse
- **Advantage**: Balanced performance for all requests

#### C-SCAN (Circular SCAN)
- Similar to SCAN but return to start without servicing requests on way back
- **Advantage**: More consistent response times

*Instructor Insight*: "SCAN is like an elevator: it goes up serving floors, then comes down. C-SCAN is like a circular elevator that only serves one direction."

---

### Key Takeaways and Study Questions

#### Summary of Key Concepts
1. All I/O devices communicate through **status**, **command**, and **data registers**.
2. **DMA** offloads data transfers from the CPU to dedicated hardware, improving efficiency.
3. **Interrupts** are superior to polling for waiting on device completion.
4. HDD performance depends on **seek time**, **rotational latency**, and **transfer rate**.
5. SSDs use **NAND flash memory** with page/block architecture requiring careful management of writes/erases.
6. **Wear leveling** distributes write operations across the drive to prevent premature failure.

#### Study Questions
1. Why is polling inefficient for I/O devices?
2. How does DMA improve system performance compared to programmed I/O?
3. Calculate the total time to read a 32 KB file from an HDD with:
   - Average seek time: 5 ms
   - RPM: 7,200
   - Max transfer rate: 150 MB/s
4. Why can't SSDs overwrite data directly? Explain using page/block architecture.
5. What is wear leveling and why is it necessary for SSDs?
6. Compare the physical structure of HDDs vs SSDs in terms of moving parts, performance characteristics, and durability.
7. How does memory-mapped I/O differ from port-mapped I/O?
8. Why do modern operating systems treat all devices as files?

#### Practical Exercise
Imagine you're designing a file system for an SSD-based storage device:
1. What challenges would arise from the page/block architecture?
2. How would you implement wear leveling to maximize drive lifespan?
3. What scheduling algorithm would work best for sequential vs random access patterns? Why?

---

### Real-World Applications

#### Modern Storage Interfaces
- **NVMe**: High-speed protocol for SSDs (7‚Äì14 GB/s)
- **SATA III**: Older standard for HDD/SSD (6 Gb/s)

*Instructor Insight*: "NVMe is why your SSD feels so fast‚Äîit's designed specifically for the parallel nature of flash memory."

#### Why SSDs Dominated Over HDDs
1. Eliminated mechanical movement ‚Üí faster access times
2. Lower power consumption ‚Üí better battery life in laptops
3. Higher durability ‚Üí survives drops and shocks

*Instructor Insight*: "SSDs solved the biggest bottleneck in computing: slow storage. This is why modern computers boot instantly compared to older systems."


---

# Week 7: Textbook Devices - Comprehensive Study Notes

We are creating comprehensive study notes based on the provided textbook slides (Week 7: Textbook Devices). The goal is to cover ALL content from the chapter with extreme detail.

Let's break down each slide and extract every concept, theory, principle, example, case study, etc. We'll organize by sections as they appear in the slides.

We note that there are 14 slides (from Slide 1 to Slide 14). However, we must be cautious: the provided text includes some repeated headers and footers (like "OPERATING SYSTEMS [VERSION 1.10] WWW.OSTEP.ORG"). We'll focus on the content.

We structure the notes as follows:


### Introduction
- Briefly state the importance of I/O devices and the core problem (CRUX) for integration.

Then, we go through each section in order.

Let's extract every piece of information from the slides.

Slide 1:
- Introduces I/O devices: input/output critical because without input a program produces same result; without output what was purpose?
- CRUX: HOW TO INTEGRATE I/O INTO SYSTEMS? (How should I/O be integrated into systems? What are general mechanisms? How to make efficient?)

36.1 System Architecture
- Classical diagram: CPU attached to main memory via memory bus.
- Devices connected via:
  - General I/O Bus (e.g., PCI) for higher-performance devices (graphics).
  - Peripheral Bus (e.g., SCSI, SATA, USB) for slow devices (disks, mice, keyboards).
- Why hierarchical? Physics and cost: faster buses must be shorter; high performance is costly. So, high-performance components near CPU, low-performance further away.
- Benefits of peripheral bus: can connect many devices.

Slide 2:
- Figure 36.1 shows the classical system architecture (as described above).
- Modern systems use specialized chips and point-to-point interconnects (Figure 36.2).
- Intel Z270 Chipset example:
  - CPU connected to memory via high-speed bus, also has direct connection to graphics card.
  - CPU connects to I/O chip via DMI (Direct Media Interface).
  - Devices connect to I/O chip via various interconnects: eSATA for disks, USB for keyboard/mouse, PCIe for network interface and NVMe storage.

Slide 3:
- Figure 36.2 shows modern architecture.
- Canonical device description (Figure 36.3):
  - Hardware interface: how the system software controls it (like a software API).
  - Internal structure: implementation-specific; may include micro-controller, memory, other chips.

Slide 4:
- The canonical protocol for device interaction (using three registers: status, command, data).
- Protocol steps:
  1. Wait until STATUS != BUSY (polling)
  2. Write data to DATA register
  3. Write command to COMMAND register (starts the device)
  4. Wait until STATUS == READY (not busy) again.
- This is called **programmed I/O (PIO)** because CPU moves data by hand.

Slide 5:
- CRUX: HOW TO AVOID THE COSTS OF POLLING?
- Solution: Use interrupts instead of polling.
  - OS issues request, puts process to sleep, context switches.
  - When device finishes, raises interrupt -> jumps to ISR (interrupt service routine).
  - ISR completes the request and wakes up the waiting process.

Timeline comparison:
- Without interrupts: CPU spins while waiting for disk (CPU busy with I/O wait).
- With interrupts: OS runs another process during I/O time.

TIP: Interrupts not always better than polling.
- For fast devices, polling is cheaper because interrupt handling cost outweighs benefit.
- Also, if device is very fast (first poll finds it done), then using an interrupt would be worse.
- Hybrid approach: poll for a short while and then switch to interrupts.

Another issue with interrupts in networks:
- If there's a flood of packets each generating an interrupt -> livelock (OS only processes interrupts).
- Solution: occasionally use polling to control the system better (e.g., web server on Hacker News).

Interrupt coalescing: device waits for a bit before delivering interrupt, allowing multiple requests to be completed and then one interrupt is delivered.

Slide 6:
- CRUX: HOW TO LOWER PIO OVERHEADS?
- Problem with PIO: CPU spends time moving data by hand (e.g., copying memory to disk word-by-word).
- Timeline without DMA: 
   - Process runs on CPU, initiates I/O -> copies data one word at a time (marked 'c') while waiting for device.
   - Then disk does the work.

Solution: **Direct Memory Access (DMA)**.
  - OS programs DMA engine with:
      - Source memory address
      - Destination (device)
      - Length of transfer
  - DMA controller handles data movement without CPU intervention.
  - When done, raises interrupt to notify OS.

Timeline with DMA:
   - Process runs on CPU -> initiates I/O and then does other work (Process 2) while DMA copies the data in background.
   - Then when DMA finishes, interrupt wakes up process.

Slide 7:
- CRUX: HOW TO COMMUNICATE WITH DEVICES?
- Two methods for device communication:

1. **Explicit I/O instructions** (e.g., x86 `in` and `out`):
   - Special CPU instructions to read/write specific device registers.
   - Privileged instruction: only OS can use them (to prevent user programs from directly accessing devices).
   - Example: on x86, `in al, 0x3F6` reads from port 0x3F6.

2. **Memory-mapped I/O**:
   - Device registers are mapped into the memory address space.
   - OS uses normal load/store instructions to access them (e.g., read/write a specific memory address).
   - No new instructions needed; both methods still in use today.

Slide 8:
- CRUX: HOW TO BUILD A DEVICE-NEUTRAL OS?
- Problem: How to hide device-specific details from major OS subsystems (like file system) so that the same file system can work on different devices.
- Solution: **Abstraction via Device Drivers**.
  - At lowest level, a software module (device driver) knows how to interact with a specific device.
  - Encapsulates all device interaction specifics within the driver.

Example: Linux File System Stack (Figure 36.4):
   Application -> POSIX API (open, read, write, close)
        |
        v
   File System
        |
        v
   Generic Block Layer (issues block reads/writes to generic interface)
        |
        v
   Device Driver [SCSI, ATA, etc.] (handles device-specific protocol)

Also note: Raw interface for low-level tools (like file-system checker) that bypass the file system.

Downside of abstraction:
- Special capabilities of a device may be hidden from higher levels.
  - Example: SCSI devices have rich error reporting but Linux only returns generic EIO because other block devices (ATA/IDE) are simpler. So extra details lost to file system.

Device drivers as code percentage:
- Studies show over 70% of OS kernel code is in device drivers (for Linux).
- This means when we say "OS has millions of lines", it's mostly driver code.
- Drivers often written by amateurs -> more bugs and primary cause of kernel crashes.

Slide 9:
- Case Study: IDE Disk Driver
  - Interface registers for IDE disk (Figure 36.5):
      * Control Register at 0x3F6
      * Command Block Registers (from 0x1F0 to 0x1F7)
          - Data Port (0x1F0): where data is read/written
          - Error (0x1F1), Sector Count (0x1F2), LBA low, mid, hi bytes (0x1F3-0x1F5), Drive select (0x1F6)
          - Command/Status (0x1F7)

  - Status Register bits: BUSY, READY, FAULT, SEEK, DRQ, CORR, IDDEX, ERROR
  - Error Register bits: BBK, UNC, MC, IDNF, MCR, ABRT, T0NF, AMNF

- Protocol for IDE disk (after initialization):
   1. Wait until drive is ready (read status register until READY and not BUSY).
   2. Write parameters to command registers (sector count, LBA address, drive number).
   3. Start I/O by writing READ or WRITE command to command register.
   4. For writes: wait for DRQ (drive request) then write data to data port.
   5. Handle interrupts (for each sector or batched).
   6. Error handling: after operation, check status and error registers.

Slide 10:
- The xv6 IDE driver code (Figure 36.6):
   - Functions in the driver:
        * `ide_wait_ready()`: polls until drive is ready.
        * `ide_start_request(struct buf *b)`: sends request to disk using I/O instructions; for write, also transfers data via `outsl`.
        * `ide_rw(struct buf *b)`: queues a request and waits for completion (using sleep/wakeup).
        * `ide_intr()`: interrupt handler that reads data on read requests, wakes process, starts next request.

Slide 11:
- Historical notes: 
   - Interrupts existed in early machines (UNIVAC in the 1950s?).
   - DMA origins debated (DYSEAC vs. IBM SAGE; mid-1950s).

Note on history: It's hard to trace because inventions tied to obscure machines.

Slide 12:
- The xv6 IDE driver code is provided as a simplified example (Figure 36.6). We have the code in Slide 12 and 13? Actually, Slide 12 has the code for Figure 36.6.

But note: the slide numbers might be off because of how they were presented. The content we extracted from slides is consistent.

Slide 13:
- Summary: 
   - OS interacts with devices using interrupts (for efficiency) and DMA (to avoid CPU data movement).
   - Two approaches for device register access: explicit I/O instructions or memory-mapped I/O.
   - Device drivers encapsulate low-level details, making the rest of the OS device-neutral.

Slide 14:
- References list. We don't need to include all references in notes (but we can note that they are provided).

Now, let's create comprehensive study notes with:

- Hierarchical headers
- Bold key terms and concepts
- Tables for comparisons (e.g., between polling vs interrupts, or two communication methods)
- All examples explained
- Comprehensive summary

We'll also include 15-20 study questions.

Let's structure the note accordingly.
</think>

# Week 7: Textbook Devices - Comprehensive Study Notes  
*Based on OSTEP Chapter "I/O Devices" (Slides 1-14)*  

---

### **Introduction to I/O Systems**  
#### Core Problem Statement  
> **CRUX**: *HOW TO INTEGRATE I/O INTO SYSTEMS?*  
> - Input/output is critical: A program with no input produces identical results every run; a program with no output has no purpose.  
> - Key questions:  
>   1. What are the general mechanisms for integrating I/O?  
>   2. How can we make these mechanisms efficient?  

---

### **36.1 System Architecture**  
#### Hierarchical Bus Structure (Figure 36.1)  
| **Bus Type**          | **Purpose**                                  | **Example Devices**               | **Performance Priority** |
|-----------------------|---------------------------------------------|----------------------------------|--------------------------|
| **Memory Bus**        | Connects CPU to main memory                 | DRAM, SRAM                       | Highest (shortest distance) |
| **General I/O Bus**   | High-performance devices near CPU           | Graphics cards, SSD controllers  | Medium-High              |
| **Peripheral Bus**    | Low-speed peripheral devices                | Disks, mice, keyboards, USB      | Lowest                   |

#### Why Hierarchical?  
- **Physics**: Faster buses must be shorter (reducing signal degradation).  
- **Cost**: High-performance interconnects are expensive to engineer.  
- **Design Trade-off**: Place high-demand components closer to CPU; connect low-demand devices via slower, cheaper peripheral buses.  

> üí° *Example*: Modern systems use PCIe for NVMe SSDs (high performance) and USB for keyboards/mice (low performance).  

---

### **36.2 Canonical Device Architecture**  
#### Two Critical Components of a Device (Figure 36.3)  
| **Component**         | **Description**                                                                 |
|-----------------------|-----------------------------------------------------------------------------|
| **Hardware Interface** | Standardized protocol/registers for system software to control the device (e.g., status/command/data registers). |
| **Internal Structure** | Implementation-specific hardware: <br> - Simple devices: Single chip.<br> - Complex devices: Micro-controller + memory + specialized chips (e.g., RAID controllers with firmware). |

> üìå *Key Insight*: Device drivers abstract the internal structure, hiding complexity from higher OS layers.  

---

### **36.3 Canonical I/O Protocol**  
#### Programmed I/O (PIO) Workflow  
```plaintext
While (STATUS == BUSY);          // Polling: Wait for device to be ready
Write data to DATA register;     // Transfer data via CPU
Write command to COMMAND reg;   // Start operation
While (STATUS == BUSY);         // Poll until done
```

#### Critical Flaws of PIO  
1. **CPU Overhead**: Wasting cycles polling a slow device (e.g., disk).  
2. **Inefficiency**: CPU cannot execute other tasks while waiting for I/O completion.  

> üí° *Example*: Reading 4KB from a disk via PIO would require the CPU to copy each word individually, blocking all other processes during transfer.

---

### **36.4 Interrupts: Avoiding Polling Costs**  
#### How Interrupts Work (Timeline Comparison)  
| **Without Interrupts** | **With Interrupts** |
|------------------------|---------------------|
| ```CPU: 1111111111ppppp```<br>```Disk: ppppp11111``` | ```CPU: 111111111122222```<br>```Disk: pp111111111``` |
| *Process 1 spins waiting for disk* | *OS runs Process 2 while disk works on Process 1's request* |

#### When to Use Interrupts vs. Polling  
| **Scenario**                | **Preferred Method** | **Reason**                                                                 |
|-----------------------------|----------------------|----------------------------------------------------------------------------|
| Slow devices (disk, network) | Interrupts           | CPU can execute other tasks during I/O wait time.                          |
| Fast devices (e.g., RAM cache) | Polling              | Interrupt handling cost > benefit of overlap; first poll usually finds device ready. |
| Network floods (e.g., Hacker News traffic) | Hybrid polling       | Prevent livelock from excessive interrupts ([MR96]).                      |

#### **Interrupt Coalescing**  
- Device waits before raising interrupt to batch multiple requests into one interrupt.  
- Trade-off: Reduces interrupt overhead but increases latency if waiting too long.  

> üìå *Case Study*: Web servers on Hacker News experience traffic spikes ‚Üí use polling periodically to avoid livelock ([MR96]).

---

### **36.5 DMA: Eliminating PIO Data Movement**  
#### Problem with PIO for Large Transfers  
- CPU must manually copy data (e.g., 4KB disk block) word-by-word, wasting cycles.  

#### Direct Memory Access (DMA) Workflow  
1. OS programs DMA engine with:  
   - Source memory address  
   - Destination device  
   - Transfer length  
2. DMA controller handles data movement *without CPU intervention*.  
3. On completion, DMA raises interrupt to notify OS.  

##### Timeline Comparison  
| **PIO**                          | **DMA**                            |
|----------------------------------|------------------------------------|
| ```CPU: 1111111111ccc22222```<br>```Disk: ppccccccppp``` | ```CPU: 111111111122222222```<br>```DMA: ccc``` |
| *CPU copies data (c) while waiting* | *DMA moves data; CPU runs Process 2 during transfer* |

---

### **36.6 Device Communication Methods**  
#### Two Primary Approaches to Access Devices  

| **Method**                | **Mechanism**                                                                 | **Pros/Cons**                                      |
|---------------------------|-----------------------------------------------------------------------------|--------------------------------------------------|
| **Explicit I/O Instructions** | CPU instructions (e.g., `in`/`out` on x86) read/write device registers.      | ‚úÖ Simple<br>‚ùå Requires privileged instructions; only OS can use them. |
| **Memory-Mapped I/O**     | Device registers appear as memory addresses; normal load/store access used.  | ‚úÖ No new CPU instructions needed<br>‚ùå Less intuitive for some hardware designs. |

#### Critical Security Note  
- Explicit I/O instructions are *privileged* (only OS can use them).  
- **Why?**: Prevent user programs from directly accessing devices (e.g., a malicious app reading/writing disk blocks ‚Üí system chaos).

---

### **36.7 Device Drivers: Building a Device-Neutral OS**  
#### The Abstraction Problem  
> **CRUX**: *HOW TO BUILD A DEVICE-NEUTRAL OS?*  
> - File systems should work on SCSI, IDE, USB drives without knowing device specifics.  

##### Solution: **Device Driver Layer** (Figure 36.4)  
```plaintext
Application ‚Üí POSIX API (open/read/write/close)
              ‚Üì
          File System
              ‚Üì
        Generic Block Layer (block read/write interface)
              ‚Üì
      Device Driver [SCSI, ATA, USB] ‚Üê Specific device protocol
```

#### Key Benefits of Abstraction  
- **Device Neutrality**: Higher layers (e.g., file system) interact only with generic block layer.  
- **Simplified OS Development**: No need to rewrite core subsystems for new devices.  

##### Downsides of Abstraction  
1. **Lost Capabilities**: Special device features may be hidden from higher levels.  
   > *Example*: SCSI has rich error reporting, but Linux returns only `EIO` (generic I/O error) because other drives (ATA/IDE) lack detail ‚Üí file system ignores extra details ([G08]).  

2. **Code Bloat**: Device drivers dominate kernel code:  
   - **Linux**: >70% of kernel lines are device driver code ([C01]).  
   - **Consequence**: Drivers often written by "amateurs" (not core developers) ‚Üí higher bug rate and primary cause of crashes ([S03]).

---

### **36.8 Case Study: IDE Disk Driver**  
#### IDE Interface Registers (Figure 36.5)  
| **Register Address** | **Purpose**                          | **Example Value/Bit Meaning**       |
|----------------------|-------------------------------------|-----------------------------------|
| `0x1F7`              | Command/status register             | Bit 0: READY, Bit 7: BUSY         |
| `0x1F6`              | Drive select (LBA mode)             | B=1 for LBA; D=drive number       |
| `0x1F2`-`0x1F5`      | Logical Block Address (LBA) bytes   | 4-byte address of sector to access  |
| `0x1F1`              | Error register                      | Bit 7: UNC = Uncorrectable error  |

#### IDE Protocol Workflow  
```plaintext
Wait for READY & not BUSY (read status reg at 0x1F7)
Write parameters to command registers:
   - Sector count (0x1F2)
   - LBA address bytes (0x1F3-0x1F5)
   - Drive select (0x1F6: master=0, slave=1)
Start I/O by writing READ/WRITE cmd to 0x1F7
For WRITEs: Wait for DRQ ‚Üí write data to port 0x1F0
Handle interrupt when done; check error reg if needed.
```

#### xv6 IDE Driver Code (Simplified)  
```c
// ide_wait_ready(): Poll until drive is ready
static int ide_wait_ready() {
    while (((int r = inb(0x1f7)) & IDE_BSY) || !(r & IDE_DRDY));
}

// ide_start_request(): Send request to disk via I/O instructions
static void ide_start_request(struct buf *b) {
    outb(0x3F6, 0); // Enable interrupts
    outb(0x1f2, 1); // Sector count = 1
    outb(0x1f3, b->sector & 0xff);
    ... // Write LBA bytes (0x1F4-0x1F5)
    if(b->flags & B_DIRTY) {
        outb(0x1f7, IDE_CMD_WRITE); 
        outsl(0x1f0, b->data, 512/4); // DMA-like data transfer
    }
}

// ide_intr(): Interrupt handler for I/O completion
void ide_intr() {
    if (!(b->flags & B_DIRTY)) insl(0x1f0, b->data, 512/4);
    wakeup(b); 
    if (ide_queue) ide_start_request(ide_queue);
}
```

> üí° *Key Insight*: `outsl()` copies data to device port without CPU intervention during transfer ‚Üí mimics DMA behavior.

---

### **36.9 Historical Notes**  
- **Interrupts**: Exist since early machines (e.g., UNIVAC in 1950s), but origins are debated ([S08]).  
- **DMA**: First implemented by mid-1950s; disputed between DYSEAC and IBM SAGE systems.  
> üìå *Why History Matters*: These ideas were obvious solutions to the CPU-speed vs. device-speed mismatch ‚Üí no "Einsteinian leap" required.

---

### **36.10 Summary**  
| **Concept**               | **Purpose**                                      | **Key Insight**                                  |
|---------------------------|-------------------------------------------------|------------------------------------------------|
| **Interrupts**            | Replace polling for slow devices                | Overlap computation and I/O; avoid CPU waste   |
| **DMA**                   | Offload data movement from CPU                  | DMA controller handles transfers directly      |
| **Memory-Mapped I/O**     | Access device registers via memory addresses    | No new instructions needed                     |
| **Device Drivers**        | Abstract hardware details for OS layers         | >70% of kernel code is drivers ‚Üí high bug rate  |

---

### **Comprehensive Study Questions (15-20)**  
#### Core Concepts & Definitions  
1. Define *programmed I/O (PIO)* and explain why it wastes CPU cycles during disk reads.  
2. Contrast *polling* vs. *interrupts*: When is each optimal? Provide a real-world example for both cases.  
3. What is **DMA**? How does it improve efficiency over PIO in data transfers?  

#### Architecture & Design  
4. Why do modern systems use hierarchical buses (memory bus ‚Üí general I/O bus ‚Üí peripheral bus)? Explain using physics and cost principles.  
5. Describe the two methods for device communication (`in`/`out` instructions vs. memory-mapped I/O). What is a key advantage of each?  

#### Device Drivers & Abstraction  
6. How do **device drivers** enable *device-neutral* OS design (e.g., file systems working across SCSI, IDE, USB)? Illustrate with the Linux stack diagram.  
7. Why does Linux return only `EIO` for disk errors instead of detailed error codes? What is this phenomenon called in [G08]?  

#### Case Study & Code Analysis  
8. Explain how the xv6 IDE driver uses **interrupts** and **DMA-like transfers** (`outsl()`) to avoid CPU blocking during I/O.  
9. In Figure 36.5, what does `IDE_BSY` in the status register indicate? How is it used in `ide_wait_ready()`?  

#### Critical Thinking & Applications  
10. A web server experiences a traffic spike from Hacker News (HackerRank). Why might *polling* be better than interrupts for network I/O here? Cite [MR96].  
11. If 75% of kernel bugs come from device drivers ([C01]), why is this problematic for OS reliability? How does [S03] address it?  

#### Historical & Theoretical  
12. Why are the origins of interrupts/DMA difficult to trace historically? What evidence supports early implementations (e.g., UNIVAC)?  
13. Explain **interrupt coalescing** and its trade-off between latency and overhead using a network packet example.  

#### Advanced Analysis  
14. How would you modify `ide_start_request()` to support *batched* sector transfers instead of one-sector-at-a-time? What changes are needed in the protocol?  
15. Why is memory-mapped I/O more common than explicit instructions in modern systems (e.g., PCIe devices)?  

---

### **Key Terms & Definitions**  
| **Term**               | **Definition**                                                                 |
|------------------------|-----------------------------------------------------------------------------|
| **Peripheral Bus**     | Low-speed bus for slow devices (USB, SATA); connects many devices cost-effectively. |
| **Programmed I/O (PIO)** | CPU manually moves data between memory and device via register writes/reads. |
| **Interrupt Coalescing** | Device delays interrupt delivery to batch multiple requests into one event.   |
| **Livelock**           | System spends all time handling interrupts, never executing user processes ([MR96]). |
| **Device-Neutral OS**  | Core subsystems (e.g., file system) unaware of device specifics via abstraction layers. |

---

> üí° **Exam Tip**: Focus on *why* each mechanism exists (CPU vs. I/O speed mismatch), not just how it works. Always link concepts to efficiency trade-offs (cost, latency, overhead).


---

# Hierarchical headers for each major section

Okay, I need to create comprehensive study notes based on a textbook chapter about hard disk drives. The user has provided all the slides from Week 7 of their operating systems course. Let me analyze this carefully.

First, I notice this is for an OS class focusing on storage devices - specifically HDDs (hard disk drives). The material covers everything from basic geometry to scheduling algorithms. Since it's a textbook chapter with detailed content including examples and formulas, my notes need to be extremely thorough.

Hmm... the user emphasized "DETAILED" study notes covering ALL content. That means I can't skip anything - not even those small footnotes about dimensional analysis or that slide on track skew. Every concept must be extracted and explained properly.

Let me mentally organize this by section:
- The interface (Slide 1)
- Basic geometry terms like platters, tracks etc. (Slides 2-3)
- Latency components: rotational delay, seek time (Slides 3-4)
- I/O timing math with formulas (Slide 6-8)
- Disk scheduling algorithms (SSTF, SCAN variants) (Slides 10-12)
- The "unwritten contract" concept (Slide 2)

I should watch out for tricky parts like:
- That dimensional analysis example in Slide 6 - need to explain it step-by-step
- Why average seek time is ~1/3 of full seek (Slide 9's math)
- The difference between SSTF and SPTF scheduling

The references at the end are important too - I'll include those as "Further Reading" but not in main notes since they're just citations.

For structure, I'm thinking:
**Bold key terms** like "rotational delay"
Tables comparing disk specs (like Cheetah vs Barracuda)
All formulas with explanations

The user wants 15-20 study questions at the end - that's crucial. Not just simple recall but application questions testing deep understanding, like "Why does track skew exist?" or "When would SATF outperform SSTF?"

*checking slide count again*
Slide 16 has homework problems... I should convert those into practice questions rather than listing them verbatim.

Biggest challenge will be making the physics of disk operation clear without getting too technical. For example, explaining why outer tracks have more sectors (geometry) but not diving into magnetic properties.

Also must clarify common confusions:
- "Block" vs "sector" terminology
- Why write-back caching is dangerous for file systems
- How rotational delay differs from seek time

*noticing the slide about SSDs in references*
Important to note this chapter focuses on HDDs, though it mentions SSD research. Shouldn't include that in notes since not covered.

For examples:
- The 3-track diagram (Slide 4) needs detailed walkthrough
- Must explain Figure 37.8 showing SSTF's limitation

*checking slide 12*
Ah! That "IT ALWAYS DEPENDS" tip is gold - should emphasize it as a key engineering principle beyond just disks.

Okay, time to structure this systematically starting from the interface definition through all components...
</think>

# Comprehensive Study Notes: Hard Disk Drives (Week 7)

---

### **I. Core Concepts & Interface**

#### **1. Fundamental Interface**
- **Disk as an Array of Sectors**:  
  Modern HDDs store data in fixed-size blocks called *sectors* (typically **512 bytes**). Each sector has a unique address from `0` to `n‚àí1` on a disk with `n` sectors.
  - **Atomicity Guarantee**: Manufacturers guarantee that a single 512-byte write is atomic (completes entirely or not at all). Larger writes may suffer *torn writes* during power loss.

- **The "Unwritten Contract"** ([SG04]):  
  Assumptions clients make but aren‚Äôt part of the interface:
  - Accessing nearby blocks (`block` and `sector` used interchangeably) is faster than distant ones.
  - Sequential access (contiguous blocks) is significantly faster than random access.

---

### **II. Physical Geometry & Components**

#### **1. Key Terminology**
| Term          | Definition                                                                 |
|---------------|----------------------------------------------------------------------------|
| **Platter**   | Circular hard surface coated with magnetic layer for persistent storage.    |
| **Surface**   | One side of a platter (each platter has 2 surfaces).                       |
| **Spindle**   | Central axis to which all platters are attached; spins at fixed RPM rate.   |
| **Track**     | Concentric circle on a surface where data is stored.                        |
| **Disk Head** | Reads/writes magnetic patterns (one per surface); mounted on disk arm.      |
| **Disk Arm**  | Moves head across tracks to position over desired track.                    |

#### **2. Geometry Details**
- **RPM & Rotation Time**:  
  - Typical modern HDDs: **7,200‚Äì15,000 RPM**.
  - *Example*: 10,000 RPM ‚Üí rotation time = `60 seconds / 10,000 rotations` = **6 ms/rotation**.
- **Track Density**:  
  Tracks are tightly packed (hundreds fit in a human hair‚Äôs width). Outer tracks have more sectors than inner tracks due to geometry (**multi-zoned drives**).
- **Track Skew**:  
  Sectors on adjacent tracks are offset by `2 blocks` (e.g., Figure 37.4) to avoid waiting for the next sector during track switches.

#### **3. Physical Operation Diagrams**
- **Single Track Example** (Figure 37.1 & 37.2):  
  - Disk rotates counter-clockwise; head positioned over sector `6`.
  - *Rotational delay*: Time to wait for desired sector under head (`R/2` on average).
    > *Example*: To access sector `0` from position `6`, worst-case rotation = **full rotation** (if starting at `5`).

- **Multi-Track Example** (Figure 37.3):  
  - Head moves to outer track via *seek* ‚Üí waits for rotational delay (`~3 sectors`) ‚Üí transfers data.
  - **Seek phases**: Acceleration ‚Üí coasting ‚Üí deceleration ‚Üí settling (**0.5‚Äì2 ms settling time**).

---

### **III. I/O Performance Components**

#### **1. I/O Time Formula**
$$T_{I/O} = T_{\text{seek}} + T_{\text{rotation}} + T_{\text{transfer}} \quad (37.1)$$

| Component          | Description                                                                 | Typical Value       |
|--------------------|-----------------------------------------------------------------------------|---------------------|
| **Seek Time**      | Time to move head to correct track (`T_seek`).                               | 4‚Äì9 ms              |
| **Rotational Delay** | Average time for sector to rotate under head (`R/2` where `R = rotation period`). | 1‚Äì2 ms (at 7,500 RPM) |
| **Transfer Time**   | Data transfer rate from disk surface.                                       | ~30 Œºs per 4KB block |

#### **2. I/O Rate Formula**
$$R_{I/O} = \frac{\text{Size}_{\text{transfer}}}{T_{I/O}} \quad (37.2)$$

---

### **IV. Performance Analysis: Random vs. Sequential Workloads**

#### **1. Case Study: Cheetah 15K.5 vs. Barracuda**
| Parameter          | Cheetah 15K.5 (SCSI)       | Barracuda (SATA)         |
|--------------------|----------------------------|--------------------------|
| Capacity           | 300 GB                     | 1 TB                     |
| RPM                | 15,000                     | 7,200                    |
| Avg. Seek Time     | **4 ms**                   | **9 ms**                 |
| Max Transfer Rate  | 125 MB/s                   | 105 MB/s                 |

#### **2. Random Workload Calculation (4KB reads)**
- *Cheetah*:  
  $T_{I/O} = \underbrace{4\text{ms}}_{\text{seek}} + \underbrace{2\text{ms}}_{\text{rotation}} + \underbrace{0.03\text{ms}}_{\text{transfer}} = \mathbf{6\ ms}$  
  $R_{I/O} = \frac{4\text{KB}}{6\text{ms}} = \mathbf{0.66\ MB/s}$
- *Barracuda*:  
  $T_{I/O} = 9 + 2 + 0.03 = \mathbf{11.03\ ms}$ ‚Üí **$R_{I/O} = 0.31\ MB/s$**

#### **3. Sequential Workload Calculation (100MB transfer)**
- *Cheetah*: $T_{I/O} ‚âà 4 + 2 + \frac{100,000}{125,000} = \mathbf{800\ ms}$ ‚Üí **$R_{I/O} ‚âà 125\ MB/s$**
- *Barracuda*: $T_{I/O} ‚âà 9 + 2 + \frac{100,000}{105,000} = \mathbf{950\ ms}$ ‚Üí **$R_{I/O} ‚âà 105\ MB/s$**

#### **4. Key Insight: Random vs. Sequential Performance Gap**
| Drive       | Random I/O Rate | Sequential I/O Rate | Ratio (Seq/Random) |
|-------------|-----------------|---------------------|--------------------|
| Cheetah     | 0.66 MB/s       | 125 MB/s            | **~190√ó**          |
| Barracuda   | 0.31 MB/s       | 105 MB/s            | **~340√ó**          |

> üí° **Design Principle**: *Always transfer data sequentially or in large chunks* (Slide 8 tip).

---

### **V. Disk Scheduling Algorithms**

#### **1. Shortest Seek Time First (SSTF)**
- **Mechanism**: Service requests on the nearest track first.
- **Example** (Figure 37.7):  
  Head at inner track ‚Üí Requests for sectors `21` (middle) and `2` (outer). SSTF services `21` before `2`.
- **Problem: Starvation**  
  If constant requests arrive on the current track, distant tracks are ignored forever.

#### **2. Elevator Algorithm (SCAN/C-SCAN)**
| Type       | Mechanism                                                                 |
|------------|---------------------------------------------------------------------------|
| **SCAN**   | Head moves back-and-forth across disk; services all pending requests in order during a sweep. |
| **C-SCAN** | Circular scan: Moves from outer to inner, then resets at outer track (more fair). |

> ‚ö†Ô∏è *Why SCAN avoids starvation*: Late arrivals on distant tracks are queued until the next sweep.

#### **3. Shortest Positioning Time First (SPTF/SATF)**
- **Mechanism**: Considers both seek time AND rotational delay (`T_seek + T_rotation`).
  - *When to use SPTF?* When seek and rotation times are comparable.
    > üí° *"It always depends"* (**Livny‚Äôs Law** ‚Äì Slide 13).

- **Example Failure of SSTF** (Figure 37.8):  
  Head at inner track (`sector 30`). Requests: `sector 16` (middle) vs. `sector 8` (outer).  
  - *SSTF*: Chooses closer middle track ‚Üí waits for full rotation to access sector `16`.  
  - *SPTF*: Prefers outer track if seek time is low relative to rotational delay.

#### **4. Modern Disk Scheduling**
- **Internal vs. OS Scheduler**:  
  | Approach          | How It Works                                                                 |
  |-------------------|-----------------------------------------------------------------------------|
  | Older Systems     | OS handles all scheduling (simple disks).                                   |
  | Modern Systems    | Disk controller has internal scheduler (implements SPTF using exact head position). |
- **I/O Merging**: Combine adjacent requests into single I/O to reduce overhead.
- **Anticipatory Scheduling** ([ID01]):  
  *Non-work-conserving*: Wait briefly for better incoming requests ‚Üí improves efficiency.

---

### **VI. Critical Concepts & Formulas**

#### **1. Average Seek Time Derivation**
$$\text{Average seek distance} = \frac{\text{Full seek distance}}{3}$$
- **Why?**  
  For tracks `0` to `N`, average absolute difference between any two tracks is $\frac{1}{3}N$ (calculated via double integral, Slide 9).

#### **2. Dimensional Analysis Examples**
| Problem                                      | Calculation                                                                 |
|----------------------------------------------|-----------------------------------------------------------------------------|
| *Rotation time for 15K RPM*                  | $ \frac{60\text{s}}{15,\!000} = 4\text{ms/rotation}$                      |
| *Transfer time for 4KB at 125 MB/s*          | $\frac{4\text{KB}}{125,000\text{KB/s}} \times 1000 = \mathbf{32\ Œºs}$     |

---

### **VII. Summary of Key Insights**

| Concept                          | Why It Matters                                                                 |
|----------------------------------|--------------------------------------------------------------------------------|
| **Unwritten Contract**           | Sequential access > random; nearby blocks faster ‚Üí drives optimized for this.   |
| **Rotational Delay Dominance**   | At 7,200 RPM: `R = 8.3 ms` ‚Üí avg. delay = `4.15 ms`; dominates small I/Os.    |
| **SPTF vs. SSTF**                | SPTF better when seek time ‚âà rotation (modern drives); hard to implement in OS.|
| **Multi-Zoned Drives**           | Outer tracks have more sectors ‚Üí higher data density at edges.                 |
| **Write-Back Caching Risk**      | Data may not be written to disk immediately; can cause corruption without journaling. |

---

### **VIII. Study Questions (15+ Exam-Focused)**

1. Explain why a 4KB random read on the Barracuda drive takes ~13ms, while sequential reads achieve near-peak transfer rates.
2. Calculate rotational delay for an HDD spinning at 10,000 RPM. Show dimensional analysis steps.
3. Why does track skew exist? How would *not* having it affect performance during cross-track reads?
4. Derive the average seek time formula (Slide 9) using integrals and explain why it‚Äôs `~1/3` of full seek.
5. Compare SSTF, SCAN, and SPTF in terms of starvation avoidance and implementation complexity.
6. A disk has a transfer rate of 80 MB/s. How long does it take to read a 2MB file sequentially? Randomly?
7. Why is write-back caching dangerous for database systems without journaling? (Hint: torn writes)
8. Explain the "unwritten contract" and give two examples of how OSs exploit it.
9. If seek time = rotation delay, which scheduling algorithm should be used? Justify using SPTF formula.
10. Calculate I/O rate for a disk with `T_seek=5ms`, `T_rotation=3ms`, `T_transfer=0.2ms` on 8KB requests.
11. Why do outer tracks have more sectors than inner tracks? Sketch the geometry to explain.
12. Describe how anticipatory scheduling improves efficiency over work-conserving approaches (Slide 14).
13. In a multi-zoned drive, why is bandwidth higher for outer zones?
14. How does I/O merging reduce overhead in disk operations? Provide an example.
15. Why can‚Äôt the OS implement SPTF effectively on modern disks? What *can* it do instead?
16. A request stream: `[7, 30, 8]`. Show SSTF vs. SATF order and calculate total time for each (assume `T_seek=4ms`, `R=5ms`).
17. How does C-SCAN differ from SCAN in fairness to inner/outer tracks? Sketch both.
18. Why is the "random workload" performance gap larger on Barracuda than Cheetah?
19. Explain why a disk‚Äôs cache (track buffer) improves sequential read performance but not random reads.
20. A drive has `T_seek=3ms`, `R=4ms`. What should be prioritized: reducing seek time or rotation delay? Justify.

---

### **IX. References for Further Study**
- [RW92]: Ruemmler & Wilkes, *An Introduction to Disk Drive Modeling* (1992) ‚Äì Foundational.
- [SG04]: Schlosser & Ganger, *The Unwritten Contract of Disks* (2004).
- [HK+17]: He et al., *Unwritten Contract for SSDs* (EuroSys 2017) ‚Äì Modern extension.
- [ID01]: Iyer & Druschel, *Anticipatory Scheduling* (SOSP 2001).

> üí° **Final Exam Tip**: Always calculate `T_I/O = T_seek + R/2 + transfer_time` for performance questions. Prioritize sequential access in design!

---

## üìù Study Guide Summary

This combined study guide contains all notes for **Week 7**, automatically generated from 3 individual files.

### Individual Files Included:

1. `20251015_lecture_7.md`
2. `20251015_textbook_devices.md`
3. `20251015_textbook_disks.md`

### Usage Tips:
- Use the table of contents above to navigate quickly
- Each section is separated by horizontal rules (`---`)
- Individual files are still available for focused study
- This combined file is perfect for comprehensive review

---
*Generated by [LectureQ](https://github.com/yourusername/lectureq) - Your AI-powered study companion* ü§ñ

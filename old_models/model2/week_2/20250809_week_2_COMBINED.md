<!-- 
Combined Study Guide
Generated by LectureQ
Date: 2025-08-09 23:27:08
Week/Module: Week 2
Files Combined: 6
Content Type: Mixed
-->

# Week 2 - Complete Study Guide

> **Auto-generated combined study guide**  
> Generated from 6 individual notes files  
> Date: August 09, 2025

## üìö Table of Contents

1. [Week 2: Lecture 2 - Operating Systems Scheduling](#week-2-lecture-2---operating-systems-scheduling)
2. [CPU Scheduling: Core Concepts](#cpu-scheduling-core-concepts)
3. [Week 2: Proportional-Share CPU Scheduling](#week-2-proportional-share-cpu-scheduling)
4. [üìå MLFQ Scheduling: Core Concepts](#üìå-mlfq-scheduling-core-concepts)
5. [Week 2 Study Notes: Memory Abstraction & Virtual Address Spaces](#week-2-study-notes-memory-abstraction-&-virtual-address-spaces)
6. [Week 2: Virtual Memory Mechanism (Condensed Notes)](#week-2-virtual-memory-mechanism-(condensed-notes))


---

# Week 2: Lecture 2 - Operating Systems Scheduling

<think>
I'm being asked to create comprehensive lecture notes for a student who missed class on "Operating Systems: Scheduling" from Week 2, Lecture 2. I need to synthesize all the information from both slides and transcript into natural, flowing educational notes.

Let me analyze what's required:

1. I must write natural, flowing notes that synthesize ALL information
2. Rephrase, reword, and improve explanations for clarity
3. Don't copy text verbatim - make it educational and readable
4. Synthesize information from both slides and transcript intelligently
5. A student should be able to learn everything from these notes alone

I'll structure the notes with:
- H1 title with week/lecture info
- Overview section after title
- Hierarchical headers (#, ##, ###)
- Key insights formatted as specified
- Tables for metrics/comparisons
- Clear explanations of concepts
- Improved analogies and examples

Let me go through all the content to identify key points:

**Key Concepts from Slides:**
- Scheduling vs Dispatching (Scheduler = "when", Dispatcher = "how")
- Process states: Running, Ready, Blocked
- Workload definition
- Key metrics: Turnaround Time, Response Time, Wait Time, Throughput, Resource Utilization, Overhead, Fairness
- Scheduling algorithms:
  - FCFS (First Come First Serve)
  - SJF (Shortest Job First)
  - STCF (Shortest Time to Completion First)
  - RR (Round Robin)
  - MLFQ (Multi-Level Feedback Queue)
- Gantt charts for visualizing scheduling
- Assumptions about jobs:
  1. Jobs run for fixed time
  2. All jobs arrive simultaneously
  3. All jobs are CPU-only
  4. Run-time of each job is known

**Key Concepts from Transcript:**
- The "convoy effect" with FCFS (long process blocking short processes)
- Preemptive vs non-preemptive scheduling
- I/O Blocking and how it affects scheduling
- Priority-based scheduling challenges
- Multi-Level Feedback Queue design principles
- Completely Fair Scheduler (CFS) in Linux

Now, let's structure the notes:

1. Start with H1 title: "Week 2: Lecture 2 - Operating Systems Scheduling"
2. Add an Overview section after title
3. Organize content logically using hierarchical headers
4. Include Key Insights from Transcript as specified
5. Create tables for metrics and algorithm comparisons
6. Explain concepts clearly, not just copy slides

I'll make sure to:
- Rephrase all technical terms in clearer language
- Add context from the transcript that explains why certain concepts matter
- Improve analogies (like the hospital example)
- Remove redundant information while preserving key points
- Create a natural flow of ideas rather than listing slide content

Let me create these notes now.
</think>


### Overview

This lecture explores **scheduling**‚Äîthe core mechanism by which operating systems manage and allocate CPU resources among competing processes. We'll examine how scheduling algorithms impact system performance, understand key metrics for evaluating scheduling effectiveness, and analyze common scheduling strategies from simple to complex. Unlike last week's focus on virtualization (where each process believes it has its own computer), this lecture dives into the *how* of managing multiple processes efficiently.

---

### Scheduling Fundamentals

#### Core Concepts & Terminology

Before diving into algorithms, we need clear definitions:

- **Workload**: The collection of jobs requiring CPU time
- **Job**: A unit of work (typically a single CPU burst for a process)
- **Process**: Alternates between *CPU bursts* and *I/O operations*
- **Scheduler**: Logic determining which job to execute next *(conceptual "when")*
- **Dispatcher**: Mechanism executing the switch between processes *(mechanical "how")*

> **Key Insight from Transcript**  
> > "The scheduler determines *which process to dispatch*, while the dispatcher handles *how* to switch between them. The scheduler is conceptual (the 'when'), and the dispatcher is mechanical (the 'how')."

#### Process States & Scheduling

Processes exist in three primary states:

| State     | Description                                                                 |
|-----------|-----------------------------------------------------------------------------|
| **Running** | Currently executing on CPU                                                  |
| **Ready**   | Waiting for CPU, but ready to run immediately                               |
| **Blocked** | Waiting for I/O (e.g., keyboard input, disk read) and cannot proceed        |

> **Key Insight from Transcript**  
> > "When a process needs I/O (like waiting for keyboard input), it *blocks*‚Äîit tells the OS 'I'm waiting' rather than wasting CPU time. This is why we must consider I/O operations in scheduling."

---

### Evaluating Scheduling Effectiveness

To determine good vs bad scheduling, we need measurable metrics:

| Metric                | Definition                                      | Why It Matters                                  |
|-----------------------|-------------------------------------------------|------------------------------------------------|
| **Turnaround Time**   | Completion time - Arrival time                  | Total time from request to completion           |
| **Response Time**     | Start time - Arrival time                       | How quickly system responds to a request        |
| **Wait Time**         | Time spent waiting in ready queue               | Time not actively using CPU                     |
| **Throughput**        | Number of jobs completed per unit time          | System's overall productivity                   |
| **Resource Utilization** | Percentage of time CPU is busy                  | Avoiding idle CPU (wasted resources)            |
| **Overhead**          | Cost of context switching between processes     | Too many switches hurt performance              |
| **Fairness**          | Equal access to CPU for all processes           | Preventing starvation of certain jobs           |

> **Key Insight from Transcript**  
> > "Imagine running a hospital with one operating room. Good scheduling means minimizing dead patients (turnaround time), reducing wait times, maximizing throughput, and ensuring fairness‚Äîso no patient gets neglected."

---

### Scheduling Algorithms & Analysis

#### Gantt Charts: Visualizing Scheduling
Gantt charts help visualize how processes are scheduled over time:

```
Time: 0   5   10   15   20
Task A: ====
Task B:     ====
Task C:         ====
```

This shows Task A runs from 0-5, Task B from 5-10, and Task C from 10-15.

---

#### Common Scheduling Algorithms

##### First Come, First Serve (FCFS)
*Simplest algorithm‚Äîprocesses run in arrival order*

**Example:**
| Job | Arrival Time | Burst Time | Turnaround Time |
|-----|--------------|------------|----------------|
| A   | 0            | 5          | 5              |
| B   | 0            | 5          | 10             |
| C   | 0            | 5          | 15             |

**Average Turnaround Time:** (5 + 10 + 15) / 3 = **10**

> **Key Insight from Transcript**  
> > "FCFS is simple to implement but suffers from the *convoy effect*‚Äîa long process blocks many short processes behind it, like a slow truck creating traffic in front of faster cars."

**Problem Example:**
| Job | Arrival Time | Burst Time | Turnaround Time |
|-----|--------------|------------|----------------|
| A   | 0            | 20         | 20             |
| B   | 0            | 4          | 24             |
| C   | 0            | 4          | 28             |

**Average Turnaround Time:** (20 + 24 + 28) / 3 = **24**

---

##### Shortest Job First (SJF)
*Runs the shortest job next to minimize average turnaround time*

**Example:**
| Job | Arrival Time | Burst Time | Turnaround Time |
|-----|--------------|------------|----------------|
| A   | 0            | 5          | 5              |
| B   | 0            | 4          | 9              |
| C   | 0            | 3          | 12             |

**Average Turnaround Time:** (5 + 9 + 12) / 3 = **8.67**

> **Key Insight from Transcript**  
> > "SJF is optimal for minimizing average turnaround time *only if all jobs arrive simultaneously*. If they don't, SJF can be problematic‚Äîlike having a tutorial arrive while you're working on a big assignment."

---

##### Shortest Time to Completion First (STCF)
*Preemptive version of SJF: Always runs the job with shortest remaining time*

**Example:**
| Job | Arrival Time | Burst Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|---------------|
| A   | 0            | 10         | 10              | 0             |
| B   | 2            | 10         | 18              | 8             |
| C   | 4            | 10         | 26              | 16            |

**Average Response Time:** (0 + 8 + 16) / 3 = **8**

> **Key Insight from Transcript**  
> > "STCF is like a student who pauses their big assignment to do a quick tutorial‚Äî'I'll get back to my big job later.' It minimizes response time but can lead to longer turnaround times for some jobs."

---

##### Round Robin (RR)
*Each process gets a fixed time slice before being preempted*

**Example:**
| Job | Arrival Time | Burst Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|---------------|
| A   | 0            | 10         | 26              | 0             |
| B   | 2            | 10         | 26              | 0             |
| C   | 4            | 10         | 26              | 0             |

**Average Response Time:** (0 + 0 + 0) / 3 = **0**

> **Key Insight from Transcript**  
> > "RR is fairer than FCFS but can be inefficient if the time slice is too short. Too many context switches create overhead, while a long time slice makes response times poor."

---

#### I/O Blocking & Scheduling

Processes often alternate between CPU and I/O:

```
Time: 0   2   4   6   8
Task A: ==== (CPU)    ==== (I/O)
        |      |
        |      +----> Task B starts at time 2
Task B:     ==== (CPU)
```

**Key Insight from Transcript**  
> "Don't waste CPU! When a process blocks for I/O, the OS should immediately switch to another ready process. This is why interactive processes (like keyboard input) need quick response times."

---

### Advanced Scheduling: Multi-Level Feedback Queue (MLFQ)

#### Why MLFQ?
Real-world scheduling requires handling:
- Jobs of varying lengths
- Jobs arriving at different times
- Unknown runtimes
- Different goals (interactive vs batch jobs)

> **Key Insight from Transcript**  
> > "Interactive jobs need fast response time; batch jobs care about turnaround time. We can't ask processes how long they'll take‚Äîthey'd lie to get more CPU!"

#### MLFQ Design Principles

1. **Run high-priority jobs first** (using round-robin within priority levels)
2. **Start all jobs at highest priority**
3. **If a job runs too long, lower its priority**
4. **Periodically reset priorities** (to prevent starvation)

```
Queue 0: Highest Priority
Queue 1: Medium Priority
Queue 2: Lowest Priority
```

> **Key Insight from Transcript**  
> > "MLFQ is like the 'evil' solution to scheduling‚Äîmalicious processes can exploit it by running for just under their time slice, then sleeping. But MLFQ counters this with periodic priority resets."

#### How MLFQ Prevents Starvation
- Long-running jobs get demoted over time (Queue 0 ‚Üí Queue 1 ‚Üí Queue 2)
- **Periodic reset**: All jobs return to highest priority after a set interval
- Ensures even long jobs eventually complete

> **Key Insight from Transcript**  
> > "If you're a big job, MLFQ will demote you until you're in the lowest queue. But every so often, it resets priorities‚Äîso you'll get back to the top and finish eventually."

---

### Modern Scheduling: Completely Fair Scheduler (CFS)

#### How CFS Works
- Tracks **vruntime** (virtual runtime) for each process
- Runs processes with *smallest vruntime* first
- Each task runs for a target latency period
- Uses **red-black trees** to efficiently find next process

> **Key Insight from Transcript**  
> > "CFS is used in Linux since 2007. It's called 'completely fair' because every process gets a proportional share of CPU time, preventing starvation."

#### CFS Weighting & `nice` Values
- Processes can adjust their *weight* (priority)
- Lower `nice` value = higher priority (more CPU time)
- Example: `nice -5` gives a process 2x more CPU than default

```
Process Type       | Typical nice Value | Priority Level
-------------------|--------------------|---------------
Video Rendering    | +10                | Low
Word Processor     | 0                  | Medium
System Daemon      | -5                 | High
```

> **Key Insight from Transcript**  
> > "The `nice` command is like a soft priority‚Äîit's not absolute, but it changes how much CPU time your process gets relative to others."

---

### Summary & Key Takeaways

#### Core Scheduling Concepts
- **Scheduler vs Dispatcher**: Scheduler decides *when* (conceptual), dispatcher handles *how* (mechanical)
- **Process States**: Running ‚Üí Ready ‚Üí Blocked (I/O) ‚Üí Ready
- **Key Metrics**: Turnaround time, response time, wait time, throughput

#### Algorithm Comparison
| Algorithm | Best For               | Worst Case          | Key Limitation              |
|-----------|------------------------|---------------------|----------------------------|
| FCFS      | Simple systems         | Long jobs block short | Convoy effect              |
| SJF       | Minimizing average turnaround | Jobs arrive late  | Requires knowing job length |
| STCF      | Minimizing response time | High overhead        | Preemption cost            |
| RR        | Fairness               | Too many context switches | Poor throughput for long jobs |

#### Advanced Scheduling
- **MLFQ**: Handles varying job lengths and priorities; prevents starvation with priority resets
- **CFS (Linux)**: Uses vruntime to ensure fair CPU allocation across processes

> **Key Insight from Transcript**  
> > "Scheduling is about trade-offs. There's no perfect algorithm‚Äîjust the best one for your specific system needs."

---

### Why This Matters in Practice

Modern operating systems use sophisticated scheduling because:
1. Interactive applications (browsers, editors) need fast response times
2. Batch processing (simulations, data analysis) cares more about throughput
3. Fairness prevents any single process from monopolizing CPU
4. Efficient scheduling maximizes resource utilization and minimizes wasted time

> **Key Insight from Transcript**  
> > "If you're a student working on assignments, think of your schedule as a mini-scheduler: You might do short tasks first (like tutorials), then return to big projects later‚Äîjust like SJF or STCF algorithms."

---

### Further Exploration
- Try running `top` in Linux and observe process priorities
- Experiment with the `nice` command to adjust process priority
- Research how modern CPUs handle multi-core scheduling challenges

> **Key Insight from Transcript**  
> > "Scheduling is where theory meets practice. It's not just about algorithms‚Äîit's about balancing competing needs efficiently."


---

# CPU Scheduling: Core Concepts

<think>
Okay, the user wants me to create concise study notes from a textbook chapter about CPU scheduling. They've provided detailed slides with explanations of various scheduling algorithms and concepts.

Looking at this material, I need to extract only the most essential information for exam preparation. The key is to focus on core concepts like FIFO, SJF, STCF, RR, and their trade-offs - while eliminating all redundant examples and verbose text.

Hmm... the textbook covers several important points:
- Workload assumptions (though unrealistic)
- Scheduling metrics: turnaround time vs response time
- Algorithms: FIFO ‚Üí SJF/STCF ‚Üí Round Robin
- The critical trade-off between throughput and interactivity

I should organize this into clear categories. Let me identify exactly what students need to know for exams:

First, the two main scheduling goals:
1) Minimize average **turnaround time** (total job completion time)
2) Minimize **response time** (time until first CPU allocation)

Then the algorithms that optimize each:
- For turnaround: **SJF/STCF** (shortest jobs first/preemptive version)
- For response: **Round Robin** with small time slices

Must include the critical trade-off concept - you can't maximize both simultaneously. The textbook uses great examples like grocery store lines to illustrate this.

Also need to cover:
- Why FIFO fails with long jobs (convoy effect example)
- How STCF handles late arrivals better than SJF
- Impact of I/O on scheduling (CPU bursts vs waiting)

The references and homework are less important for quick review, so I'll skip those. The "easy pieces" tips can be condensed into key takeaways.

For structure:
- Start with core metrics table (turnaround vs response)
- Then algorithm comparison in a simple format
- Highlight the unavoidable trade-off as main takeaway

Must keep it ultra-concise - target 30% of original length. Will avoid all examples except one killer illustration: the "100s job blocking two 10s jobs" for FIFO failure.

Also need to define key terms simply:
- Turnaround time = completion - arrival
- Response time = first run start - arrival

The STCF vs SJF distinction is crucial (preemptive vs non-preemptive), and the textbook emphasizes that STCF is optimal when arrivals are staggered.

For RR, must mention the time slice trade-off: too short ‚Üí high context switch cost; too long ‚Üí poor response times. The amortization tip about making slices longer to reduce switching overhead is worth including briefly.

I'll skip all historical references (like Cobham 1954) and focus only on concepts applicable to modern OS scheduling.

The summary box at the end should capture everything in one glance - that's what students will actually use for last-minute review.
</think>

*Focused study notes for exam preparation*

---

### üîë **Key Metrics**  
| Metric             | Definition                                  | Goal                          |
|--------------------|---------------------------------------------|-------------------------------|
| **Turnaround Time** | `Completion time - Arrival time`            | Minimize (total job completion) |
| **Response Time**   | `First CPU run start - Arrival time`        | Minimize (interactivity)       |

> üí° *Trade-off: Optimizing one metric worsens the other. Fairness often conflicts with performance.*

---

### ‚öôÔ∏è **Core Scheduling Algorithms**

#### 1. **FIFO / FCFS**  
- **How it works**: Run jobs in arrival order (non-preemptive).  
- ‚úÖ *Pros*: Simple, easy to implement.  
- ‚ùå *Cons*: **Convoy effect** with long jobs blocking short ones:  
  > Example: Long job (100s) blocks two short jobs (10s each) ‚Üí Avg turnaround = `110s` vs optimal `50s`.  

---

#### 2. **SJF / STCF**  
| Type       | Preemptive? | Optimizes      | Best For               |
|------------|-------------|----------------|------------------------|
| **SJF**    | ‚ùå No        | Turnaround     | Batch systems (known job lengths) |
| **STCF**   | ‚úÖ Yes       | Turnaround*    | Dynamic arrivals (optimal if known runtimes) |

> üí° *Why STCF > SJF?*: Handles late-arriving short jobs by preempting long ones.  
> Example: Jobs A(100s), B/C(10s each, arrive at t=10):  
> - **STCF**: Runs B/C first ‚Üí Avg turnaround = `50s` (vs SJF‚Äôs `103.3s`).  

---

#### 3. **Round Robin (RR)**  
- **How it works**: Time-slice (`quantum`) per job; cycles through ready queue.  
- ‚úÖ *Pros*: Excellent for **response time** (fair, interactive systems).  
- ‚ùå *Cons*: Poor for **turnaround time** (stretches jobs unnecessarily).  

> ‚öñÔ∏è **Time Slice Trade-off**:  
> - Too short ‚Üí High context-switch cost (`10%` overhead at 10ms slice).  
> - Too long ‚Üí Degraded response time.  
> *Optimal*: Balance to amortize switch costs (e.g., `10-100ms`).  

---

### üîÑ **Critical Trade-offs & Realism**  
| Goal                | Algorithm   | Assumption Needed      | Reality Check               |
|---------------------|-------------|------------------------|----------------------------|
| Minimize turnaround | SJF/STCF    | Known job lengths      | ‚ùå Unachievable (OS can‚Äôt predict) |
| Minimize response   | RR          | Short time slices      | ‚úÖ Practical for interactive systems |

> üí° **I/O Handling**:  
> - Treat CPU bursts as independent jobs ‚Üí Overlap I/O with other work.  
> *Example*: Job A (CPU 10ms + I/O 10ms) runs while B uses CPU ‚Üí Better utilization.

---

### üìå **Exam Essentials**  
1. **STCF is optimal for turnaround time** under dynamic arrivals (*if runtimes known*).  
2. **RR minimizes response time but maximizes turnaround** (worst-case: `N √ó quantum`).  
3. **Convoy effect**: Long jobs block short ones ‚Üí Avoid with SJF/STCF.  
4. **I/O-bound vs CPU-bound**: Schedulers must prioritize I/O completion to avoid idle CPU.  

> üö´ *Never assume*: "Job runtimes are known" (real OSes use prediction, e.g., multi-level feedback queues).

---

### ‚úÖ **Summary Box: The Core Conflict**  
| Metric          | Best Algorithm | Why?                          |
|-----------------|----------------|-------------------------------|
| Turnaround Time | STCF           | Runs shortest jobs first      |
| Response Time   | RR             | Short time slices for fairness |

> üí° *Remember*: "You can‚Äôt have your cake and eat it too" ‚Üí **Optimize one metric, sacrifice the other.**  
> *(e.g., SJF = fast batch processing; RR = responsive terminals)*


---

# Week 2: Proportional-Share CPU Scheduling

<think>
Okay, the user wants me to create concise study notes from a textbook chapter about CPU scheduling with proportional share concepts. They've provided detailed slides covering lottery scheduling, stride scheduling, and Linux CFS.

First I need to understand what's most important for exam purposes. The key is identifying 5-10 core concepts that would likely be tested. From the slides, these stand out:
- Lottery scheduling using tickets (probabilistic)
- Ticket mechanisms like currency/transfer/inflation
- Stride scheduling as deterministic alternative
- CFS implementation with vruntime and red-black trees
- The fairness tradeoffs between approaches

I notice students often get confused about how lottery vs stride actually works in practice. I should clarify that lottery uses randomness (so proportions are probabilistic over time) while stride achieves exact ratios deterministically.

For the notes, I'll avoid all examples except one perfect illustration - like the 75/25 ticket split for A/B processes. The implementation code snippets can be cut down to just the key idea: "Traverse list adding tickets until counter > random number."

The CFS section has important details about schedlatency and mingranularity parameters that affect time slices. Must include those formulas but simplify them - maybe show only the weighted slice calculation.

Hmm... should I mention all references? Definitely not for study notes. But the key papers (WW94, W95) are worth naming since they're foundational.

Biggest trap to avoid: getting bogged down in CFS's red-black tree details. Just need to say "uses balanced trees for O(log n) scheduling decisions" without explaining tree structures.

For fairness metrics - that Figure 9.2 graph is important but I can summarize it as "Fairness approaches 1 only with long job durations."

Wait, the user emphasized eliminating redundant explanations. So no need to explain why computers count from zero (that Dijkstra note in Slide 13). Cut all tangents.

Let me structure this:
- Core concept bullets first
- Then ticket mechanisms table (currency/transfer/inflation)
- CFS key parameters as a mini-table
- Final comparison of approaches

Must keep it tight. Original slides are ~20 pages, notes should be under 1 page A4 equivalent. Targeting 35% length reduction.

*checks slide count* Slide 9 has that nice weight table - but for study notes I'll just say "weights map to priority (e.g., nice=-5 ‚Üí weight=3121)" without listing all values.

Ah! The homework questions are actually great exam material. But since they're not part of the chapter content, I won't include them in notes unless directly relevant - like question 4 about quantum size affecting fairness.

*mental note*: Emphasize that lottery's randomness causes short-term unfairness but averages out (per Slide 5 graph). That's a common exam point.
</think>

*(Condensed for Exam Focus)*  

---

### üîë **Core Concepts**  
1. **Proportional Share Goal**: Guarantee processes receive *specific CPU percentages* (e.g., A gets 75%, B gets 25%), not just min response time.  
2. **Lottery Scheduling**: Uses **tickets** to probabilistically allocate shares:  
   - Each process has tickets; winner = random ticket draw from total pool.  
   - *Example*: Process A (75 tickets), B (25 tickets) ‚Üí A runs ~75% of time (probabilistic, not guaranteed).  
3. **Stride Scheduling**: Deterministic alternative to lottery:  
   - Each process has a `stride` = constant √∑ tickets (e.g., 10,000 √∑ 100 tickets = stride=100 for A).  
   - Scheduler picks lowest `pass value` (updated by stride after each run) ‚Üí *exact* proportional shares.  
4. **CFS (Linux)**: Efficient implementation of fair-share scheduling using:  
   - **Virtual runtime (`vruntime`)**: Accumulates inversely to process weight/nice level.  
   - **Schedlatency**: Target time for all processes to run once (e.g., 48ms). Time slice = `schedlatency / n`.  
   - **Weighting via nice levels** (-20=high priority, +19=low): Weights map to priorities (see table below).  

---

### ‚öôÔ∏è **Key Mechanisms & Formulas**  
| Concept          | Description                                                                 | Formula/Example                                  |
|------------------|-----------------------------------------------------------------------------|-------------------------------------------------|
| **Tickets**      | Represent CPU share; % of tickets = target CPU %.                           | A: 75 tickets, B: 25 ‚Üí A gets ~75% CPU.         |
| **Ticket Currency** | User allocates tickets *within their own jobs* (system converts to global). | User A gives job1/2: 500 tickets each ‚Üí Global = 50 each. |
| **CFS Weighting** | `weight` from nice level; time slice ‚àù weight / total weights.              | Nice=-5 (`weight=3121`) vs. default (nice=0, `weight=1024`).<br>A‚Äôs slice = 3√óB‚Äôs slice (‚âà36ms vs. 12ms @ schedlatency=48ms). |
| **CFS vruntime** | Adjusts for priority: `vruntime += (1024 / weight) √ó runtime`.              | A (`weight=3121`) accumulates vruntime at *‚Öì* rate of B (`weight=1024`). |

---

### üåê **Comparison: Lottery vs. Stride vs. CFS**  
| Approach       | Fairness (Short-Term) | Implementation Complexity | Key Advantage                          |
|----------------|------------------------|----------------------------|-----------------------------------------|
| **Lottery**    | Probabilistic (~75% A over time, not exact) | Very simple (`O(n)` list traversal) | No global state; easy to add new processes. |
| **Stride**     | Exact (deterministic)  | Moderate (track `pass`/`stride`) | Guarantees ratios *every cycle*.        |
| **CFS**        | Near-exact             | High (uses red-black trees for efficiency) | Scales to thousands of processes; efficient. |

> üí° **Why CFS?**: Uses balanced trees (`O(log n)` lookups vs. `O(n)` list scans), critical in datacenters with 1000s of processes.

---

### ‚ö†Ô∏è **Critical Exam Points**  
- **Lottery‚Äôs randomness is a feature, not a bug**: Avoids corner cases (e.g., LRU worst-case) and requires minimal state.  
- **CFS fairness tradeoff**: Shorter `schedlatency` ‚Üí better fairness but more context switches; longer ‚Üí worse fairness but higher performance.  
  - *Mingranularity* prevents tiny time slices (e.g., sets min slice to 6ms).  
- **Ticket assignment problem**: No universal solution ("how many tickets for browser?"). CFS uses `nice` levels as a practical workaround.  
- **I/O starvation risk in CFS**: Sleeping processes wake up with low vruntime ‚Üí *fixed by resetting vruntime to tree min value*.  

---

### üìä **Summary Box: When to Use What**  
| Scenario                          | Best Scheduler       |
|-----------------------------------|----------------------|
| Simple, probabilistic sharing (e.g., VMs) | Lottery              |
| Exact ratios needed (no randomness) | Stride               |
| Modern Linux systems / datacenters  | CFS                  |

> üí° **CFS is dominant**: Used in all modern Linux kernels. *Not* a panacea‚Äîstruggles with I/O-bound processes and requires manual priority tuning (`nice`).

---

### ‚ùå **What to Skip**  
- All code snippets (e.g., `Figure 9.1`).  
- Redundant examples (e.g., full ticket conversion steps).  
- Reference details (cite papers like [WW94] only if asked for *foundational work*).  
- Homework questions (focus on concepts, not simulations).  

> ‚úÖ **Final Tip**: For exams, focus on:  
> 1. How tickets ‚Üí CPU share in lottery.  
> 2. Why stride is deterministic vs. lottery‚Äôs randomness.  
> 3. CFS `vruntime` and weighting logic (`nice` ‚Üî weight ‚Üî slice time).


---

# üìå MLFQ Scheduling: Core Concepts

<think>
Okay, the user wants me to create concise study notes from a textbook chapter about CPU scheduling with MLFQ (Multi-Level Feedback Queue). They've provided detailed slides and specific formatting requirements.

First I need to understand what's essential here. The key points seem to be:
- MLFQ is designed for systems without prior knowledge of job lengths
- It uses feedback from process behavior to adjust priorities dynamically
- Core rules include priority-based scheduling, allotment system, and periodic boosts

Hmm the textbook has several attempts at refining MLFQ - I should focus on the final refined version since that's what would be tested. The starvation problem and gaming attacks are important exam concepts.

Let me identify the absolute must-know elements:
1) The 5 core rules (especially Rule 4 revised to prevent gaming)
2) How it approximates SJF without knowing job lengths
3) Priority boost mechanism for fairness
4) Why it's better than pure RR or SJF

I notice the textbook emphasizes practical implementation details like Solaris using MLFQ, but students probably just need to know it's widely used. The references and homework are less important for exam prep.

For structure:
- Start with a one-sentence definition of MLFQ
- List core rules as bullet points (not numbered since they're interdependent)
- Explain the allotment concept simply
- Contrast old vs new Rule 4 to show why gaming was fixed

Should avoid:
- All historical context about Corbato (though interesting, not testable)
- Specific examples like Figure 8.2 unless critical
- Parameter tuning details beyond "boost period S is configurable"

The summary box at the end should capture MLFQ's unique value: learning from behavior to balance interactive vs batch workloads.

Wait - need to check length. Original slides are about 10 pages, so notes should be ~3-4 paragraphs max. The user said target 30-40% of original content.

Biggest trap would be including too many implementation details (like Solaris table values). Focus on principles: "uses feedback from process behavior" not "Solaris uses 60 queues".

Also must include the security angle about scheduling being a vulnerability - that's mentioned in Slide 5 as an important tip. But keep it brief since it's just one line.

For key terms:
- **Allotment**: Time limit at current priority before demotion
- **Priority Boost**: Periodic reset to top queue to prevent starvation

The gaming example (Rule 4a/b abuse) is test-worthy so I'll summarize that as "gaming: short jobs could monopolize CPU by doing I/O early" but note the fix in Rule 4.

Final check against requirements:
- ‚úÖ Bullet points not paragraphs
- ‚úÖ Bolded key terms like **Allotment**
- ‚úÖ Only essential examples (the two-job scenario)
- ‚úÖ No redundant explanations

Okay, time to distill this into tight notes. The challenge is keeping it under 200 words while hitting all testable concepts.
</think>

*Condensed for exam-focused review*

---

### üîë **Key Principles**  
- Solves the *dual problem*: Minimize **turnaround time** (like SJF) + minimize **response time** (like RR), without prior job length knowledge.  
- Uses **feedback from process behavior** to dynamically adjust priorities ‚Üí learns "what type of job" it is as execution progresses.  

---

### ‚öôÔ∏è **Core Rules (Refined MLFQ)**  
| Rule | Action |
|------|--------|
| **Rule 1** | Higher priority job runs first (`Priority(A) > Priority(B)` ‚áí A runs). |
| **Rule 2** | Same-priority jobs run in **round-robin** (using queue‚Äôs time slice). |
| **Rule 3** | New jobs start at **highest priority** (topmost queue). |
| **Rule 4** | *After using full allotment* ‚Üí demote to next lower queue *(prevents gaming!)*.  
‚Üí *Fixes old Rule 4a/4b: I/O doesn‚Äôt reset allotment.* |
| **Rule 5** | **Periodic priority boost**: All jobs moved to top queue after time `S` (avoids starvation). |

> üí° **Why Rule 4 is critical:** Prevents "gaming" where a job does I/O *just before* its allotment ends to stay high-priority and monopolize CPU.

---

### üåü **How MLFQ Works**  
- **Short/interactive jobs**: Start at top queue ‚Üí run quickly (like SJF) ‚Üí complete fast.  
- **Long/CPU-bound jobs**: Use full allotments ‚Üí slowly demoted to lower queues ‚Üí get fair share via *priority boost*.  
- **Approximates SJF without knowing job lengths** by assuming all new jobs are short until proven otherwise.  

---

### ‚ö†Ô∏è **Critical Issues & Fixes**  
| Problem | Fix |
|---------|-----|
| **Starvation (long jobs never run)** | Rule 5: Periodic priority boost (`S` = configurable time). |
| **Gaming the scheduler** | Revised Rule 4: *Allotment used up* ‚Üí demotion, regardless of I/O. |
| **Behavior changes over time** | Priority boost ensures CPU-bound jobs become interactive (if they do) get higher priority. |

> ‚ö†Ô∏è `S` is a "voo-doo constant" ‚Äì too large = starvation; too small = poor responsiveness.

---

### üí° **Why MLFQ? Exam Takeaway**  
- **Best of both worlds**: Excellent for *interactive jobs* (low response time) + fair for *batch jobs* (progress without starvation).  
- **Widely used**: Basis in BSD, Solaris (`TS` class), Windows NT.  
- **Key insight**: *"Don‚Äôt assume job type ‚Äì learn from behavior."*  

> ‚úÖ **Test question tip:** If a process does I/O early to stay high-priority ‚Üí *this is gaming*, and Rule 4 prevents it.

---

### üìä Summary: MLFQ vs. Other Schedulers  
| Scheduler | Turnaround Time | Response Time | Requires Job Length? |
|-----------|-----------------|---------------|----------------------|
| **MLFQ**  | ‚úÖ Good (approximates SJF) | ‚úÖ Excellent for interactive jobs | ‚ùå No ‚Äì learns dynamically |
| RR        | ‚ùå Poor           | ‚úÖ Good         | ‚ùå No                |
| SJF       | ‚úÖ Best          | ‚ùå Poor         | ‚úÖ Yes               |

> üí° **MLFQ‚Äôs superpower**: Balances both goals *without prior knowledge*.


---

# Week 2 Study Notes: Memory Abstraction & Virtual Address Spaces

<think>
Okay, the user wants me to create concise study notes from a textbook chapter about memory abstraction in operating systems. They've provided detailed slides with content about early computer systems, multiprogramming, time sharing, and virtual memory concepts.

Looking at this material, I need to extract only the most essential information for exam preparation. The key themes are: how physical memory evolved into abstract address spaces, why virtualization became necessary, and the core goals of virtual memory (transparency, efficiency, protection).

I notice several important historical context points that students might get tested on - like early systems having no abstraction where OS sat at 0x0 and user programs started at 64KB. But I'll condense this to just the contrast between old vs new approaches.

The address space diagram (Slide 3) is crucial but I can summarize it as three components: code (static, top), heap (grows down from code), stack (grows up from end). The "free" area in middle isn't needed for exam focus.

For virtual memory goals, the three pillars are non-negotiable:
- Transparency (program unaware of abstraction)
- Efficiency (minimal performance hit)
- Protection (isolation between processes)

The "ASIDE: EVERY ADDRESS IS VIRTUAL" slide is gold - I'll make sure to emphasize that all addresses printed in C programs (like %p) are virtual, not physical. This trips up many students.

I should skip:
- All historical references (DV66 etc.)
- Homework details
- Full code examples from va.c
- Most of Slide 8's reference list

The summary box at the end will tie it all together with "Why VM matters" - because modern systems need to run multiple processes safely in shared memory.

For formatting, I'll use:
- Bold for key terms (Address Space, Virtual Memory)
- Bullet points for goals/concepts
- One table comparing old vs new approaches
- Short paragraphs max

Length check: Original slides are ~10 pages. Target is 3-4 concise bullets + summary box - about 25% of original length as requested.

Most importantly, I'm focusing on what students will actually be tested on: defining address space components, explaining why virtualization was needed (vs swapping), and listing the three VM goals with their meanings.
</think>


### Core Concepts  
#### **1. Early Systems vs. Modern Needs**  
- **Early systems**: No abstraction; OS sat at physical address `0`, user program started at `64KB`. Users expected minimal features ‚Üí *simple but inflexible*.  
- **Problem**: Machines were expensive (hundreds of thousands $) ‚Üí needed **multiprogramming** (run multiple processes) and later **time-sharing** for interactivity.  

> üí° *Key shift*: From "single user, single program" to *"many users, concurrent programs with timely responses."*  

---

#### **2. The Address Space Abstraction**  
The OS provides a **private memory view** (address space) per process:  
| Component | Purpose                          | Growth Direction |  
|-----------|----------------------------------|------------------|  
| **Code**  | Instructions (`main()`, functions) | Static (fixed at top) |  
| **Heap**  | Dynamically allocated data (`malloc()`) | Grows *down* from code |  
| **Stack** | Local variables, function calls   | Grows *up* from end of memory |  

> ‚ö†Ô∏è **Critical Insight**: Physical memory layout ‚â† address space. Example: Process A‚Äôs virtual `0x10` ‚Üí physical `320KB`.  

---

#### **3. Virtual Memory (VM) Goals**  
| Goal          | Meaning                                                                 | Why It Matters                          |  
|---------------|-------------------------------------------------------------------------|------------------------------------------|  
| **Transparency** | Program unaware memory is virtualized; runs as if own private RAM       | No code changes needed                   |  
| **Efficiency**  | Minimal performance hit (uses hardware like TLBs)                       | Avoids slow disk swaps (`Figure 13.2`)   |  
| **Protection**  | Isolates processes: *No process can access another‚Äôs memory*            | Prevents crashes/malware from harming OS or others |  

> üîë **Key Principle**: **Isolation** = Failure of one process doesn‚Äôt affect others (e.g., via `malloc` errors).  

---

#### **4. Why Virtualization? The Core Problem**  
- **Physical memory constraint**: All processes share *one* physical RAM ‚Üí must prevent interference.  
- **Old approach failed**: Swapping entire memory to disk (`Figure 13.1`) was too slow (memory size ‚Üë ‚Üí swap time ‚Üë).  
- **Modern solution**: Keep all processes in RAM; use hardware + OS to map virtual ‚Üî physical addresses *on-the-fly*.  

> üí° **Exam Tip**: All user-level pointers printed via `%p` (e.g., `printf("%p", &x)`) are **virtual addresses**. Physical locations known only by OS/hardware.  

---

### Summary Box: Virtual Memory in 30 Seconds  
> The OS creates a *private, large address space* for each process using virtual memory. It maps this to physical RAM via hardware (e.g., MMU), ensuring:  
> - **Transparency** (program thinks it has full RAM)  
> - **Efficiency** (minimal overhead with TLBs)  
> - **Protection** (no cross-process memory access).  
> *Without VM, modern systems couldn‚Äôt run multiple apps safely or efficiently.*  

---

### Essential Terms to Know  
| Term                | Simple Definition                                  |  
|---------------------|----------------------------------------------------|  
| **Address Space**   | Process‚Äôs private view of memory (code/stack/heap)  |  
| **Virtual Address** | User-level address; *not* physical location        |  
| **Physical Address**| Actual RAM location (known only to OS/hardware)    |  
| **Isolation**       | Processes cannot interfere with each other‚Äôs memory  |  

> ‚úÖ **Test Question**: *"Why can‚Äôt a process access another process‚Äôs stack?"* ‚Üí **Protection via VM ensures isolation.**


---

# Week 2: Virtual Memory Mechanism (Condensed Notes)

<think>
Okay, the user wants me to create concise study notes from a textbook chapter about virtual memory mechanisms. They've provided detailed slides covering address translation and base-and-bounds relocation.

First I need to understand what's essential here. The core concept is how hardware (with MMU) translates virtual addresses to physical ones using base + bounds registers, while the OS manages this through context switches and exception handling. 

Hmm... the key points seem to be:
- Limited direct execution principle (OS only intervenes at critical moments)
- Hardware-based address translation as the mechanism
- Base register for relocation offset
- Bounds register for protection against out-of-bounds access
- How this creates virtual memory illusion

I should eliminate all redundant examples like the detailed assembly code in Slide 3. The "x=3000" example is too verbose - just need to mention it's a simple case showing address translation.

Important terms to define simply:
- **Virtual Address**: What program thinks it's using
- **Physical Address**: Actual memory location
- **Base Register**: Offset added during translation (e.g., 32KB)
- **Bounds Register**: Maximum allowed virtual address

For the OS responsibilities, I'll condense Slide 10 into just three bullet points: allocation on process creation, deallocation on termination, and saving/restoring base/bounds during context switches.

The fragmentation issue in Slide 13 is worth mentioning but briefly - it's why we need better systems later (segmentation).

I notice the textbook emphasizes "transparency" as key - that programs don't know their addresses are translated. Must include that.

For formatting, I'll use:
- Bolded terms for quick scanning
- A minimal table comparing virtual vs physical address concepts
- Bullet points only for core mechanisms
- Skip all references and homework details (they're not exam-critical)

*Checking length*: Original slides total ~15 pages. Target should be 4-6 concise bullets + definitions. Will aim for under 200 words.

Biggest trap to avoid: Don't get stuck in the "why base+bounds was invented" history - focus on how it works today as a foundational concept.
</think>


### Core Concept
**Hardware-based address translation**: Hardware transforms *virtual addresses* (program's view) ‚Üí *physical addresses* (actual memory location), enabling **efficient virtualization of memory** while maintaining control.

---

#### Key Principles & Components
| Term                | Role                                                                 |
|---------------------|----------------------------------------------------------------------|
| **Virtual Address** | Logical address generated by program (e.g., `15KB` in code)          |
| **Physical Address**| Actual RAM location after translation (`32KB + 15KB = 47KB`)         |
| **Base Register**   | Offset added to virtual addr ‚Üí physical addr: <br> *phys = virt + base* |
| **Bounds Register** | Ensures `0 ‚â§ virt < bounds`; triggers exception if violated          |

---

#### How It Works (Simplified)
1. **Translation**:  
   Hardware automatically adds base register value to every memory reference.
   > *Example*: Base=32KB, Virtual addr=15KB ‚Üí Physical addr=47KB

2. **Protection**:  
   CPU checks if `virtual address < bounds` before translation.  
   ‚ùå Violation (e.g., virtual=4400 in 4KB space) ‚Üí **exception** ‚Üí OS terminates process.

3. **OS Responsibilities**:
   - Allocate memory on process creation (`free list` tracks available slots)
   - Save/restore base/bounds during context switches
   - Handle exceptions (terminate misbehaving processes)

---

#### Why This Matters for Exams
- **Transparency**: Program *never knows* addresses are translated ‚Üí illusion of private memory.
- **Efficiency**: Hardware does translation in <1 cycle; OS only intervenes on faults/context switches (*limited direct execution*).
- **Protection Guarantee**: Bounds register + hardware check prevent processes from accessing other apps/OS memory.

> üí° **Critical Insight**: Base-and-bounds is the *simplest virtualization mechanism*. It‚Äôs inefficient (causes internal fragmentation) but foundational for understanding modern paging.  
> *(Next: Segmentation ‚Üí Paging to solve fragmentation)*

---

#### Common Exam Questions
1. Given base=32KB, bounds=16KB, what happens at virtual addr=4096?  
   **Answer**: `phys = 32768 + 4096 = 36864` ‚Üí Valid (within bounds).
   
2. Why can‚Äôt user mode modify base/bounds registers?  
   **Answer**: Requires privileged instructions; otherwise, process could hijack memory.

> ‚ö†Ô∏è **Avoid This Mistake**: Base+bounds *does not* solve external fragmentation (wasted gaps between processes) ‚Äî that‚Äôs why paging is needed later.

---

## üìù Study Guide Summary

This combined study guide contains all notes for **Week 2**, automatically generated from 6 individual files.

### Individual Files Included:

1. `20250809_lecture_2.md`
2. `20250809_textbook_cpu_sched.md`
3. `20250809_textbook_cpu_sched_lottery.md`
4. `20250809_textbook_cpu_sched_mlfq.md`
5. `20250809_textbook_vm_intro.md`
6. `20250809_textbook_vm_mechanism.md`

### Usage Tips:
- Use the table of contents above to navigate quickly
- Each section is separated by horizontal rules (`---`)
- Individual files are still available for focused study
- This combined file is perfect for comprehensive review

---
*Generated by [LectureQ](https://github.com/yourusername/lectureq) - Your AI-powered study companion* ü§ñ

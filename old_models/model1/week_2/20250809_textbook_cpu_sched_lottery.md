<!-- 
Generated by LectureQ
Date: 2025-08-09 19:46:47
Model: qwen3:30b
-->

<think>
I need to create comprehensive study notes from the textbook chapter on "CPU Scheduling: Lottery" based on the provided slides. The instructions specify that I must include ALL information without summarizing, preserving every detail, example, and explanation.

Let me go through each slide and extract all the information in a comprehensive manner, following the required format.

Starting with Slide 1:

This slide introduces proportional-share scheduling, also known as fair-share scheduling. It explains that instead of optimizing for turnaround or response time, a scheduler might try to guarantee that each job obtains a certain percentage of CPU time.

It mentions lottery scheduling as an example, based on research by Waldspurger and Weihl [WW94], with the basic idea being to hold a lottery to determine which process should run next, with processes getting more chances to win if they should run more often.

The CRUX (core question) is: "HOW TO SHARE THE CPU PROPORTIONALLY?"

Then it moves to Section 9.1: Basic Concept: Tickets Represent Your Share

It explains that tickets are used to represent the share of a resource a process should receive. The percent of tickets a process has represents its share of the system resource.

Example: Two processes, A and B, with A having 75 tickets and B having 25 tickets. They want A to receive 75% of the CPU and B 25%.

Lottery scheduling achieves this probabilistically by holding a lottery every so often (e.g., every time slice). The scheduler knows the total number of tickets (100 in this example) and picks a winning ticket (a number from 0 to 99). If A holds tickets 0-74 and B holds 75-99, the winning ticket determines which process runs.

It provides an example output of winning tickets: "63 85 70 39 76 17 29 41 36 39 10 99 68 83 63 62 43 0 49 12" and the resulting schedule: "A A A A A A A A A A A A A A A A B B B B"

It notes that lottery scheduling leads to probabilistic correctness (not deterministic) - in the example, B got 4 out of 20 time slices (20%) instead of the desired 25%, but with longer competition, they're more likely to achieve the desired percentages.

Slide 2 continues with a tip about using randomness:

It explains three advantages of random approaches:
1. Avoids strange corner-case behaviors that traditional algorithms may have trouble handling
2. Is lightweight, requiring little state to track alternatives (vs. traditional fair-share scheduling which requires per-process accounting)
3. Can be quite fast (as long as generating a random number is quick)

It also provides a tip about using tickets to represent shares, noting that tickets can be applied broadly (e.g., for memory in virtual memory management).

Slide 3 covers Ticket Mechanisms:

It discusses three mechanisms:
1. Ticket currency: Allows a user with a set of tickets to allocate tickets among their own jobs in whatever currency they'd like; the system converts to global value.
   - Example: User A has 100 tickets, gives 500 each to A1 and A2 (in A's currency), which converts to 50 each in global currency. User B has 100 tickets, gives 10 to B1 (in B's currency), which converts to 100 in global currency. Total global tickets = 200.

2. Ticket transfer: A process can temporarily hand off its tickets to another process.
   - Useful in client/server settings: client passes tickets to server to maximize performance, then gets them back.

3. Ticket inflation: A process can temporarily raise or lower its number of tickets.
   - Useful in trusted environments (not competitive), where a process can boost its ticket value to reflect need without communicating with others.

Slide 4 provides implementation details:

It shows code for lottery scheduling decision (Figure 9.1):

```
// counter: used to track if we've found the winner yet
int counter = 0;

// winner: call some random number generator to
// get a value >= 0 and <= (totaltickets - 1)
int winner = getrandom(0, totaltickets);

// current: use this to walk through the list of jobs
node_t*current = head;
while (current) {
    counter = counter + current->tickets;
    if (counter > winner)
        break; // found the winner
    current = current->next;
}
// 'current' is the winner: schedule it...
```

It explains the example with three processes (A: 100 tickets, B: 50 tickets, C: 250 tickets) and total tickets = 400. If the winning ticket is 300:
- Counter starts at 0
- A adds 100 (counter=100 < 300)
- B adds 50 (counter=150 < 300)
- C adds 250 (counter=400 > 300) -> break, C is winner

It notes that organizing the list in sorted order (highest to lowest tickets) can make it more efficient, as it minimizes the number of list iterations.

Slide 5 covers an example with a fairness study:

It defines a fairness metric F = time first job completes / time second job completes. A perfectly fair scheduler would achieve F=1.

Figure 9.2 shows a graph plotting average fairness as job length (R) varies from 1 to 1000 over thirty trials. It notes that when job length is short, average fairness can be low; only as jobs run for significant time slices does the lottery scheduler approach the desired fair outcome.

Slide 6 introduces Stride Scheduling:

It explains that stride scheduling is a deterministic fair-share scheduler invented by Waldspurger [W95], addressing the issue that lottery scheduling doesn't deliver exact proportions over short time scales.

Stride scheduling:
- Each job has a stride, inverse in proportion to the number of tickets it has.
- Stride = large number / number of tickets (e.g., 10,000 / ticket value)
- Example: A (100 tickets) -> stride=100, B (50 tickets) -> stride=200, C (250 tickets) -> stride=40
- Each job has a pass value (initially 0), incremented by stride each time it runs
- Scheduler picks process with lowest pass value to run next

Pseudocode:
```
curr = remove_min(queue); // pick client with min pass
schedule(curr); // run for quantum
curr->pass += curr->stride; // update pass using stride
insert(queue, curr); // return curr to queue
```

It provides a trace (Figure 9.3) showing how the scheduler behaves over time with A, B, C having strides 100, 200, 40.

It notes that stride scheduling gets proportions exactly right at the end of each scheduling cycle, while lottery scheduling achieves proportions probabilistically over time.

It also explains why lottery scheduling is still used despite stride scheduling's precision: lottery has no global state per process, making it easier to incorporate new processes.

Slide 7 covers the Linux Completely Fair Scheduler (CFS):

It explains that CFS implements fair-share scheduling in a highly efficient and scalable manner.

Basic Operation:
- Goal: fairly divide CPU evenly among all competing processes
- Uses virtual runtime (vruntime) - each process accumulates vruntime as it runs
- When scheduling decision occurs, picks process with lowest vruntime to run next

It discusses the tension between fairness and performance:
- Switch too often: increased fairness but decreased performance (too much context switching)
- Switch less often: increased performance but decreased near-term fairness

CFS manages this with two parameters:
1. sched_latency: determines how long one process should run before considering a switch (typically 48 ms)
   - For n processes, time slice = sched_latency / n
   - Example: 4 processes -> 12 ms per process

2. min_granularity: ensures time slice isn't too small (typically 6 ms)
   - Example: 10 processes -> would be 4.8 ms without min_granularity, but set to 6 ms instead

It explains that CFS uses a periodic timer interrupt (e.g., every 1 ms), and tracks vruntime precisely.

Weighting (Nice):
- CFS uses nice level (from -20 to +19) to control priority
- Maps nice value to weight using a table:
```
static const int prio_to_weight[40] = {
/*-20*/ 88761, 71755, 56483, 46273, 36291,
/*-15*/ 29154, 23254, 18705, 14949, 11916,
/*-10*/ 9548, 7620, 6100, 4904, 3906,
/*-5*/ 3121, 2501, 1991, 1586, 1277,
/*0*/ 1024, 820, 655, 526, 423,
/*5*/ 335, 272, 215, 172, 137,
/*10*/ 110, 87, 70, 56, 45,
/*15*/ 36, 29, 23, 18, 15,
};
```

Time slice formula:
timeslice_k = (weight_k / sum_{i=0}^{n-1} weight_i) * sched_latency

Example: 
- A has nice value -5 (weight=3121), B has nice value 0 (weight=1024)
- A's time slice ≈ 3/4 of sched_latency (36 ms), B's ≈ 1/4 (12 ms)

vruntime formula:
vruntime_i = vruntime_i + (weight_0 / weight_i) * runtime_i

It explains that the table preserves CPU proportionality ratios when the difference in nice values is constant.

Slide 8 discusses using red-black trees for efficiency:

- CFS keeps processes in a red-black tree (instead of a simple list)
- Red-black trees maintain low depth, ensuring O(log n) operations instead of O(n) for lists
- Only running (runnable) processes are kept in the tree
- Processes that go to sleep are removed from the tree

It provides an example with vruntime values: 1, 5, 9, 10, 14, 18, 17, 21, 22, 24
- In a list, finding the next job is O(1) but insertion is O(n)
- In a red-black tree, both operations are O(log n)

Slide 9 covers dealing with I/O and sleeping processes:

- Problem: If a process sleeps for a long time, when it wakes up, its vruntime will be behind, causing it to monopolize CPU while catching up
- CFS handles this by setting the vruntime of a waking process to the minimum value in the tree (to avoid starvation)
- However, this means jobs that sleep for short periods frequently don't get their fair share

Slide 10 covers other CFS features and a tip about efficient data structures:

- CFS has many other features (cache performance heuristics, multi-CPU strategies, etc.)
- Tip: "Use efficient data structures when appropriate" - simple lists don't scale for thousands of processes

Slide 11 is a summary of the chapter:

- Introduces proportional-share scheduling with three approaches: lottery, stride, and CFS
- Lottery uses randomness to achieve proportional share; stride does so deterministically
- CFS is a weighted round-robin with dynamic time slices, built to scale and perform well under load
- Problems with fair-share schedulers: don't mesh well with I/O, ticket/priority assignment is hard
- Proportional-share schedulers are effective in domains like virtualized data centers

Slide 12 has references:
- [AC97] Extending Proportional-Share Scheduling to a Network of Workstations
- [B+18] The Battle of the Schedulers: FreeBSD ULE vs. Linux CFS
- [B72] Symmetric binary B-Trees
- [D82] Why Numbering Should Start At Zero
- [K+15] Profiling A Warehouse-scale Computer
- [J09] Inside The Linux 2.6 Completely Fair Scheduler
- [KL88] A Fair Share Scheduler
- [WW94] Lottery Scheduling: Flexible Proportional-Share Resource Management
- [W95] Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management
- [W02] Memory Resource Management in VMware ESX Server

Slide 13 has homework/simulation details:
- A program called lottery.py allows seeing how a lottery scheduler works
- Questions about simulations with different parameters

Now I'll create comprehensive study notes based on all this information, following the required format.
</think>

# Chapter 9: Scheduling - Proportional Share

## Learning Objectives
- Understand proportional-share scheduling concepts
- Learn about lottery scheduling and its implementation
- Explore ticket mechanisms (currency, transfer, inflation)
- Analyze stride scheduling as a deterministic alternative
- Study the Linux Completely Fair Scheduler (CFS) in detail
- Compare different fair-share scheduling approaches

## Core Concept: Proportional-Share Scheduling

**Definition**: Proportional-share scheduling is a type of scheduler that guarantees each job a certain percentage of CPU time, rather than optimizing for turnaround or response time.

**CRUX (Core Question)**: *HOW TO SHARE THE CPU PROPORTIONALLY?*

**Key Mechanism**: Instead of focusing on traditional metrics like turnaround time or response time, proportional-share scheduling ensures that each process receives a specific proportion of the CPU resources based on its assigned share.

**Historical Context**: 
- Early example: Waldspurger and Weihl [WW94] research
- Known as **lottery scheduling**
- The idea is older [KL88]

## 9.1 Basic Concept: Tickets Represent Your Share

**Definition**: Tickets are used to represent the share of a resource (like CPU time) that a process should receive. The percentage of tickets a process has represents its share of the system resource.

**Example**: 
- Two processes, A and B
- A has 75 tickets, B has 25 tickets
- Desired allocation: A gets 75% of CPU, B gets 25%

**Implementation**:
1. Total tickets = 100 (75 + 25)
2. Scheduler picks a winning ticket (random number from 0 to 99)
3. A holds tickets 0-74, B holds tickets 75-99
4. Winning ticket determines which process runs

**Example Output**:
- Winning tickets: `63 85 70 39 76 17 29 41 36 39 10 99 68 83 63 62 43 0 49 12`
- Resulting schedule: `A A A A A A A A A A A A A A A A B B B B`

**Key Insight**: 
- Lottery scheduling achieves proportional sharing **probabilistically**, not deterministically
- In the example, B received 4 out of 20 time slices (20%) instead of the desired 25%
- The longer the jobs compete, the more likely they are to achieve the desired percentages

## 9.2 Ticket Mechanisms

### Ticket Currency
**Definition**: A mechanism allowing a user with a set of tickets to allocate tickets among their own jobs in whatever currency they prefer; the system automatically converts this to global value.

**Example**:
- User A has 100 tickets
- User A runs two jobs (A1, A2) and gives each 500 tickets (in A's currency)
- User B has 100 tickets
- User B runs one job (B1) and gives it 10 tickets (in B's currency)

**Conversion**:
- A1: 500 (A's currency) → 50 (global currency)
- A2: 500 (A's currency) → 50 (global currency)
- B1: 10 (B's currency) → 100 (global currency)

**Global Ticket Total**: 200 (50 + 50 + 100)

### Ticket Transfer
**Definition**: A process can temporarily hand off its tickets to another process.

**Use Case**: Client/server setting
- Client process sends message to server asking it to do work on client's behalf
- Client passes tickets to server to maximize server performance while handling request
- Server transfers tickets back to client when finished

### Ticket Inflation
**Definition**: A process can temporarily raise or lower the number of tickets it owns.

**Use Case**: 
- In trusted environments (not competitive scenarios)
- A process that needs more CPU time can boost its ticket value to reflect that need
- No communication with other processes required

**Important Note**: 
- In competitive scenarios with untrusted processes, inflation makes little sense (a greedy process could take over the machine)
- In trusted environments (e.g., group of processes working together), inflation can be useful

## 9.3 Implementation

**Key Insight**: Lottery scheduling is remarkably simple to implement.

**Required Components**:
1. Good random number generator
2. Data structure to track processes (e.g., list)
3. Total number of tickets

**Example Implementation** (Figure 9.1):

```c
// counter: used to track if we've found the winner yet
int counter = 0;

// winner: call some random number generator to
// get a value >= 0 and <= (totaltickets - 1)
int winner = getrandom(0, totaltickets);

// current: use this to walk through the list of jobs
node_t*current = head;
while (current) {
    counter = counter + current->tickets;
    if (counter > winner)
        break; // found the winner
    current = current->next;
}
// 'current' is the winner: schedule it...
```

**Example with Three Processes**:
- Process A: 100 tickets
- Process B: 50 tickets
- Process C: 250 tickets
- Total tickets: 400

**Example Execution** (winning ticket = 300):
1. counter = 0 + 100 = 100 (A's tickets) → 100 < 300 → continue
2. counter = 100 + 50 = 150 (B's tickets) → 150 < 300 → continue
3. counter = 150 + 250 = 400 (C's tickets) → 400 > 300 → break
4. Current points to C (winner)

**Optimization Tip**: 
- Organize the list in sorted order (highest to lowest tickets)
- This minimizes the number of list iterations, especially when a few processes have most tickets
- Sorting does not affect correctness but improves efficiency

## 9.4 An Example: Fairness Study

**Fairness Metric (F)**:
- Defined as time first job completes divided by time second job completes
- F = 1 indicates perfect fairness (both jobs finish at same time)

**Example Calculation**:
- If R = 10, first job finishes at time 10, second job at time 20
- F = 10/20 = 0.5

**Figure 9.2**: Lottery Fairness Study
- Plots average fairness as job length (R) varies from 1 to 1000 over thirty trials
- When job length is short, average fairness can be low
- As jobs run for significant time slices, lottery scheduler approaches desired fair outcome

**Key Insight**: 
- Lottery scheduling requires longer job execution times to achieve near-perfect fairness
- Short jobs exhibit more variance in completion times

## 9.5 How To Assign Tickets?

**Core Problem**: The system behavior strongly depends on how tickets are allocated.

**Approach Considered (and Rejected)**:
- Assume users know best: each user is handed some number of tickets, can allocate to any jobs they run
- This solution doesn't provide guidance on what to do (non-solution)

**Open Problem**: 
- Given a set of jobs, the "ticket-assignment problem" remains open
- How to assign tickets to achieve desired behavior

## 9.6 Stride Scheduling

**Definition**: A deterministic fair-share scheduler invented by Waldspurger [W95], addressing the issue that lottery scheduling doesn't deliver exact proportions over short time scales.

**Key Concept**:
- Each job has a stride, which is the inverse of the number of tickets it has
- Stride = large number / number of tickets (e.g., 10,000 / ticket value)

**Example Calculation**:
- Process A: 100 tickets → stride = 10,000 / 100 = 100
- Process B: 50 tickets → stride = 10,000 / 50 = 200
- Process C: 250 tickets → stride = 10,000 / 250 = 40

**Scheduler Operation**:
- Each process has a pass value (initially 0)
- When a process runs, its pass value is incremented by its stride
- Scheduler picks process with lowest pass value to run next

**Pseudocode**:
```
curr = remove_min(queue); // pick client with min pass
schedule(curr); // run for quantum
curr->pass += curr->stride; // update pass using stride
insert(queue, curr); // return curr to queue
```

**Example Trace** (Figure 9.3):
| Pass(A) (stride=100) | Pass(B) (stride=200) | Pass(C) (stride=40) | Who Runs? |
|----------------------|----------------------|---------------------|-----------|
| 0                    | 0                    | 0                   | A         |
| 100                  | 0                    | 0                   | B         |
| 100                  | 200                  | 0                   | C         |
| 100                  | 200                  | 40                  | C         |
| 100                  | 200                  | 80                  | C         |
| 100                  | 200                  | 120                 | A         |
| 200                  | 200                  | 120                 | C         |
| 200                  | 200                  | 160                 | C         |
| 200                  | 200                  | 200                 | ...       |

**Key Insight**: 
- C ran 5 times, A twice, B once → exactly proportional to ticket values (250:100:50)
- Lottery scheduling achieves proportions probabilistically over time
- Stride scheduling gets exact proportions at the end of each scheduling cycle

**Why Use Lottery Scheduling Instead of Stride?**
- Lottery scheduling has no global state per process
- When a new job enters, lottery scheduling simply adds it with its tickets, updates total tickets
- With stride scheduling, determining the initial pass value for a new process is problematic (if set to 0, it monopolizes CPU)

## 9.7 The Linux Completely Fair Scheduler (CFS)

**Definition**: A fair-share scheduler implemented in Linux that achieves similar goals to proportional-share scheduling but in an efficient and scalable manner.

**Key Insight**: 
- CFS aims to spend very little time making scheduling decisions
- Recent studies show scheduling uses about 5% of overall datacenter CPU time [K+15]
- Reducing scheduling overhead is a key goal in modern scheduler architecture

### Basic Operation

**Virtual Runtime (vruntime)**:
- As each process runs, it accumulates vruntime
- In basic case, each process's vruntime increases at the same rate as physical time
- When scheduling decision occurs, CFS picks process with lowest vruntime to run next

**Scheduling Tension**:
- Switch too often: increased fairness but decreased performance (too much context switching)
- Switch less often: increased performance but decreased near-term fairness

**CFS Parameters**:
1. **sched_latency**: Determines how long one process should run before considering a switch (typically 48 ms)
   - For n processes, time slice = sched_latency / n
   - Example: 4 processes → 12 ms per process
2. **min_granularity**: Ensures time slice isn't too small (typically 6 ms)
   - Example: 10 processes would be 4.8 ms without min_granularity, but set to 6 ms

**Example Execution** (Figure 9.4):
- Four jobs (A, B, C, D) each run for two time slices
- C and D complete, leaving two remaining
- Remaining jobs run for 24 ms in round-robin fashion

**How CFS Handles Scheduling Decisions**:
- Uses periodic timer interrupt (e.g., every 1 ms)
- Tracks vruntime precisely
- Over long term, approximates ideal CPU sharing

### Weighting (Nice)

**Definition**: CFS enables controls over process priority using the classic UNIX "nice" level.

**Nice Value Range**: -20 to +19 (default = 0)
- Positive nice values → lower priority
- Negative nice values → higher priority
- "When you're too nice, you just don't get as much (scheduling) attention"

**Weight Mapping Table**:
```c
static const int prio_to_weight[40] = {
/*-20*/ 88761, 71755, 56483, 46273, 36291,
/*-15*/ 29154, 23254, 18705, 14949, 11916,
/*-10*/ 9548, 7620, 6100, 4904, 3906,
/*-5*/ 3121, 2501, 1991, 1586, 1277,
/*0*/ 1024, 820, 655, 526, 423,
/*5*/ 335, 272, 215, 172, 137,
/*10*/ 110, 87, 70, 56, 45,
/*15*/ 36, 29, 23, 18, 15,
};
```

**Time Slice Formula**:
```
timeslice_k = (weight_k / sum_{i=0}^{n-1} weight_i) * sched_latency
```

**Example Calculation**:
- Process A: nice value -5 → weight = 3121
- Process B: nice value 0 → weight = 1024
- A's time slice ≈ (3121 / (3121 + 1024)) * sched_latency ≈ 3/4 * sched_latency (36 ms)
- B's time slice ≈ (1024 / (3121 + 1024)) * sched_latency ≈ 1/4 * sched_latency (12 ms)

**vruntime Formula**:
```
vruntime_i = vruntime_i + (weight_0 / weight_i) * runtime_i
```

**Key Insight**: 
- The table preserves CPU proportionality ratios when the difference in nice values is constant
- Example: If A had nice value 5 (weight=335) and B had nice value 10 (weight=110), CFS would schedule them in exactly the same manner as before

### Using Red-Black Trees

**Why Red-Black Trees?**
- Simple lists don't scale for modern systems with thousands of processes
- Searching through a long list every few milliseconds wastes CPU cycles
- Red-black trees maintain low depth, ensuring O(log n) operations instead of O(n)

**CFS Implementation**:
- Keeps only running (runnable) processes in the red-black tree
- Processes that go to sleep (e.g., waiting for I/O) are removed from the tree
- When a process wakes up, it's reinserted into the tree

**Example with vruntime Values**:
- Values: 1, 5, 9, 10, 14, 18, 17, 21, 22, 24
- In a list: finding next job is O(1), but insertion is O(n)
- In red-black tree: both operations are O(log n)

**Figure 9.5**: CFS Red-Black Tree (depicts the tree structure with values ordered by vruntime)

### Dealing With I/O And Sleeping Processes

**Problem**: 
- If a process sleeps for a long time (e.g., 10 seconds), when it wakes up, its vruntime will be 10 seconds behind others
- Without handling, it would monopolize CPU while catching up, starving other processes

**CFS Solution**:
- When a process wakes up, CFS sets its vruntime to the minimum value found in the tree
- This avoids starvation but has a cost: jobs that sleep for short periods frequently don't get their fair share

**Key Insight**: 
- CFS handles this by altering vruntime of waking process to minimum value in tree
- This is a trade-off between avoiding starvation and ensuring fair sharing

## 9.8 Summary

**Three Approaches to Proportional-Share Scheduling**:
1. **Lottery Scheduling**: Uses randomness to achieve proportional share
   - Simple implementation
   - Probabilistic correctness (not deterministic)
   - No global state per process

2. **Stride Scheduling**: Deterministic fair-share scheduler
   - Achieves exact proportions at end of each scheduling cycle
   - Requires global state (pass values)
   - More complex to handle new processes

3. **Completely Fair Scheduler (CFS)**: Linux implementation
   - Weighted round-robin with dynamic time slices
   - Built to scale and perform well under load
   - Most widely used fair-share scheduler today

**Key Limitations of Fair-Share Schedulers**:
1. **I/O Problems**: Jobs performing I/O occasionally may not get their fair share of CPU [AC97]
2. **Ticket/Priority Assignment**: Hard problem of determining how many tickets or nice values to assign (e.g., how many tickets should a browser be allocated?)

**Effective Use Cases**:
- Virtualized data centers (cloud): Assign specific CPU percentages to VMs
- Example: Assign one-quarter of CPU cycles to Windows VM and rest to base Linux installation
- Can be extended to other resources (e.g., memory in VMWare ESX Server [W02])

## Key Terms and Definitions

| Term | Definition |
|------|------------|
| **Proportional-Share Scheduling** | A scheduling approach that guarantees each job a certain percentage of CPU time, rather than optimizing for turnaround or response time. |
| **Lottery Scheduling** | A proportional-share scheduling algorithm where processes are given tickets representing their share, and a lottery determines which process runs next. |
| **Tickets** | A mechanism to represent a process's share of the CPU (or other resources), where the percentage of tickets a process has represents its share of the resource. |
| **Ticket Currency** | A mechanism allowing a user to allocate tickets among their own jobs in a local currency, which the system converts to global value. |
| **Ticket Transfer** | A mechanism allowing a process to temporarily hand off its tickets to another process. |
| **Ticket Inflation** | A mechanism allowing a process to temporarily raise or lower its number of tickets (useful in trusted environments). |
| **Stride Scheduling** | A deterministic fair-share scheduler where each job has a stride (inverse of tickets), and the scheduler picks the process with lowest pass value. |
| **Virtual Runtime (vruntime)** | A metric used by CFS to track how much CPU time a process has effectively used, scaled by its priority. |
| **sched_latency** | A parameter in CFS determining how long one process should run before considering a switch (typically 48 ms). |
| **min_granularity** | A parameter in CFS ensuring time slice isn't too small (typically 6 ms). |
| **Nice Level** | A UNIX mechanism for controlling process priority, ranging from -20 (highest) to +19 (lowest). |

## Important Formulas

1. **Fairness Metric (F)**:
   ```
   F = (time first job completes) / (time second job completes)
   ```

2. **Stride Calculation**:
   ```
   stride = large_number / number_of_tickets
   ```

3. **Time Slice Calculation (CFS)**:
   ```
   timeslice_k = (weight_k / sum_{i=0}^{n-1} weight_i) * sched_latency
   ```

4. **vruntime Update (CFS)**:
   ```
   vruntime_i = vruntime_i + (weight_0 / weight_i) * runtime_i
   ```

## Review Questions

1. Compute the solutions for simulations with 3 jobs and random seeds of 1, 2, and 3.

2. Now run with two specific jobs: each of length 10, but one (job 0) with 1 ticket and the other (job 1) with 100 (e.g., -l 10:1,10:100). What happens when the number of tickets is so imbalanced? Will job 0 ever run before job 1 completes? How often? In general, what does such a ticket imbalance do to the behavior of lottery scheduling?

3. When running with two jobs of length 100 and equal ticket allocations of 100 (-l 100:100,100:100), how unfair is the scheduler? Run with some different random seeds to determine the (probabilistic) answer; let unfairness be determined by how much earlier one job finishes than the other.

4. How does your answer to the previous question change as the quantum size (-q) gets larger?

5. Can you make a version of the graph that is found in the chapter? What else would be worth exploring? How would the graph look with a stride scheduler?

## References

- [AC97] "Extending Proportional-Share Scheduling to a Network of Workstations" by Andrea C. Arpaci-Dusseau and David E. Culler. PDPTA'97, June 1997.
- [B+18] "The Battle of the Schedulers: FreeBSD ULE vs. Linux CFS" by J. Bouron, S. Chevalley, B. Lepers, W. Zwaenepoel, R. Gouicem, J. Lawall, G. Muller, J. Sopena. USENIX ATC'18, July 2018.
- [B72] "Symmetric binary B-Trees: Data Structure And Maintenance Algorithms" by Rudolf Bayer. Acta Informatica, Volume 1, Number 4, December 1972.
- [D82] "Why Numbering Should Start At Zero" by Edsger Dijkstra, August 1982.
- [K+15] "Profiling A Warehouse-scale Computer" by S. Kanev, P. Ranganathan, J.P. Darago, K. Hazelwood, T. Moseley, G. Wei, D. Brooks. ISCA'15, June 2015.
- [J09] "Inside The Linux 2.6 Completely Fair Scheduler" by M. Tim Jones. December 15, 2009.
- [KL88] "A Fair Share Scheduler" by J. Kay and P. Lauder. CACM, Volume 31 Issue 1, January 1988.
- [WW94] "Lottery Scheduling: Flexible Proportional-Share Resource Management" by Carl A. Waldspurger and William E. Weihl. OSDI'94, November 1994.
- [W95] "Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management" by Carl A. Waldspurger. Ph.D. Thesis, MIT, 1995.
- [W02] "Memory Resource Management in VMware ESX Server" by Carl A. Waldspurger. OSDI'02, Boston, Massachusetts.

## Key Insights from the Chapter

### Randomness in Lottery Scheduling
> **TIP: USE RANDOMNESS**  
> One of the most beautiful aspects of lottery scheduling is its use of randomness. When you have to make a decision, using such a randomized approach is often a robust and simple way of doing so.

> **Advantages of Random Approaches**:
> 1. Avoids strange corner-case behaviors (e.g., LRU replacement policy has worst-case performance for some cyclic-sequential workloads)
> 2. Lightweight - requires little state to track alternatives (vs. traditional fair-share scheduling which requires per-process accounting)
> 3. Fast - as long as generating a random number is quick, making the decision is also quick

### Ticket Representation
> **TIP: USE TICKETS TO REPRESENT SHARES**  
> One of the most powerful (and basic) mechanisms in the design of lottery (and stride) scheduling is that of the ticket. The ticket is used to represent a process's share of the CPU in these examples, but can be applied much more broadly.

### Data Structure Choice
> **TIP: USE EFFICIENT DATA STRUCTURES WHEN APPROPRIATE**  
> In many cases, a list will do. In many cases, it will not. Knowing which data structure to use when is a hallmark of good engineering. In the case discussed herein, simple lists found in earlier schedulers simply do not work well on modern systems, particular in the heavily loaded servers found in datacenters. Such systems contain thousands of active processes; searching through a long list to find the next job to run on each core every few milliseconds would waste precious CPU cycles. A better structure was needed, and CFS provided one by adding an excellent implementation of a red-black tree.

### Key Trade-Offs
> **Lottery vs. Stride**:
> - Lottery: Simple, probabilistic, no global state
> - Stride: Deterministic, exact proportions, requires global state

> **CFS Design Philosophy**:
> - Balance between fairness and performance
> - Minimize scheduling overhead (5% of datacenter CPU time)
> - Efficient data structures for scalability (red-black trees)
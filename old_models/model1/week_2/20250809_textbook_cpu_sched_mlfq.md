<!-- 
Generated by LectureQ
Date: 2025-08-09 19:59:25
Model: qwen3:30b
-->

<think>
I need to create comprehensive study notes from the textbook chapter on "CPU Scheduling: MLFQ" (Multi-Level Feedback Queue). I must include EVERY detail without summarizing, preserving all information, examples, and explanations.

Let me go through each slide and extract all the information:

Slide 1:
- Introduces MLFQ (Multi-Level Feedback Queue) as a well-known scheduling approach
- First described by Corbato et al. in 1962 in CTSS (Compatible Time-Sharing System)
- Led to Corbato receiving the Turing Award
- MLFQ addresses two problems:
  1. Optimizing turnaround time (running shorter jobs first), but OS doesn't know job length
  2. Making system feel responsive to interactive users (minimizing response time)
- MLFQ tries to learn about processes as they run to make better scheduling decisions
- The key question: "HOW TO SCHEDULE WITHOUT PERFECT KNOWLEDGE?"

Slide 2:
- MLFQ learns from history to predict future behavior (common in OS, hardware branch predictors, caching)
- Basic rules for MLFQ:
  • Rule 1: If Priority(A) > Priority(B), A runs (B doesn't)
  • Rule 2: If Priority(A) = Priority(B), A & B run in RR
- MLFQ varies priority based on observed behavior:
  - Interactive jobs (relinquish CPU for input) get high priority
  - CPU-intensive jobs get reduced priority
- Figure 8.1 shows a snapshot of queues with jobs at different priorities

Slide 3:
- Introduces the concept of "allotment" - amount of time a job can spend at a given priority level before priority is reduced
- First attempt at priority adjustment algorithm:
  • Rule 3: When a job enters, it's placed at highest priority
  • Rule 4a: If job uses up allotment, priority reduced (moves down one queue)
  • Rule 4b: If job gives up CPU before allotment is up, stays at same priority (allotment reset)

Slide 4:
- Example 1: Long-running job with time slice of 10ms
  - Enters at highest priority (Q2)
  - After 10ms, moves to Q1
  - After another 10ms, moves to Q0 (lowest priority)
- Example 2: Long-running job A and short interactive job B
  - A is running in lowest priority queue
  - B arrives at time T=100, enters highest queue
  - B completes in two time slices (20ms)
  - A resumes running at low priority
- MLFQ approximates SJF (Shortest Job First) by assuming jobs might be short and giving them high priority

Slide 5:
- Example 3: Interactive job with I/O
  - Interactive job B needs CPU for 1ms before I/O, competing with long-running job A
  - MLFQ keeps B at highest priority because it relinquishes CPU before allotment expires
  - This helps interactive jobs run quickly
- Problems with current MLFQ:
  1. Starvation: too many interactive jobs can consume all CPU time, starving long-running jobs
  2. Gaming: smart users can game scheduler by issuing I/O before allotment expires to stay at high priority
  3. Behavior change: a job changing from CPU-bound to interactive won't be treated properly

Slide 6:
- Attempt #2: Priority Boost
  • Rule 5: After time period S, move all jobs to topmost queue
  - Solves starvation: jobs guaranteed not to starve
  - Also helps jobs that transition from CPU-bound to interactive
- Example: Long-running job competing with two short interactive jobs
  - Without priority boost (left), long-running job starved
  - With priority boost every 100ms (right), long-running job gets periodic CPU time
- S is called a "voo-doo constant" - hard to set correctly
  - Too high: long-running jobs could starve
  - Too low: interactive jobs may not get proper CPU share

Slide 7:
- Attempt #3: Better Accounting
  - Problem: Rules 4a and 4b allow gaming by relinquishing CPU before allotment expires
  - Solution: Rewrite Rule 4 to track CPU time used at each level, regardless of I/O behavior
  • Rule 4: Once job uses up its allotment at a given level (regardless of I/O), priority reduced
- Example: Figure 8.5 shows gaming with old rules (left) vs. new rule (right)
  - With old rules, process can dominate CPU by issuing I/O before allotment expires
  - With new rule, process moves down queues regardless of I/O behavior

Slide 8:
- Tuning MLFQ:
  - How many queues? 
  - Time slice per queue?
  - Allotment?
  - Boost frequency (S)?
- Solaris MLFQ (Time-Sharing class):
  - 60 queues
  - Time-slice lengths: 20ms (highest) to hundreds of ms (lowest)
  - Priorities boosted around every 1 second
- Other implementations:
  - FreeBSD uses mathematical formula based on CPU usage
  - Uses decayed usage for priority boost

Slide 9:
- Other features:
  - Some schedulers reserve highest priority levels for OS work
  - User can provide hints (e.g., "nice" command to adjust priority)
- Tip: "USE ADVICE WHERE POSSIBLE" - OS rarely knows best for each process, so user hints can help

Slide 10:
- Summary of refined MLFQ rules:
  • Rule 1: If Priority(A) > Priority(B), A runs
  • Rule 2: If Priority(A) = Priority(B), A & B run in RR
  • Rule 3: New jobs enter at highest priority
  • Rule 4: Once job uses up allotment at a level, priority reduced
  • Rule 5: Periodically boost all jobs to topmost queue

Slide 11:
- References:
  - [A+17] Automatic Database Management System Tuning
  - [AD00] Multilevel Feedback Queue Scheduling in Solaris
  - [B86] The Design of the UNIX Operating System
  - [C+62] An Experimental Time-Sharing System
  - [CS97] Inside Windows NT
  - [E95] An Analysis of Decay-Usage Scheduling
  - [LM+89] The Design and Implementation of the 4.3BSD UNIX Operating System
  - [M06] Solaris Internals
  - [O11] John Ousterhout's Home Page
  - [P+95] Informed Prefetching and Caching
  - [Y+18] Principled Schedulability Analysis for Distributed Storage Systems

Slide 12:
- Homework (Simulation):
  - mlfq.py program to simulate MLFQ
  - Questions about running simulations, reproducing examples, configuring for round-robin, gaming the scheduler, etc.

Now I'll create comprehensive study notes following all the instructions. I'll structure it with proper headers, include all details without summarizing, and make it exhaustive for exam preparation.

I'll start with chapter title, learning objectives, key terms, then go through each concept in detail with all examples, figures, and explanations.
</think>

# Week 2: CPU Scheduling - Multi-Level Feedback Queue (MLFQ)

## Chapter Overview

This chapter covers the **Multi-Level Feedback Queue (MLFQ)** scheduling algorithm, one of the most well-known approaches to scheduling in operating systems. The MLFQ scheduler was first described by Corbato et al. in 1962 in a system known as the Compatible Time-Sharing System (CTSS), and this work led to Corbato receiving the Turing Award.

## Core Problem Addressed by MLFQ

MLFQ tries to address two fundamental scheduling problems simultaneously:

1. **Optimizing turnaround time** - which, as discussed in previous notes, is done by running shorter jobs first. However, the OS generally doesn't know how long a job will run for (the exact knowledge required by algorithms like SJF or STCF).

2. **Making the system feel responsive to interactive users** - minimizing response time. Unfortunately, algorithms like Round Robin reduce response time but are terrible for turnaround time.

**THE CRUX: HOW TO SCHEDULE WITHOUT PERFECT KNOWLEDGE?**

How can we design a scheduler that both minimizes response time for interactive jobs while also minimizing turnaround time without priori knowledge of job length?

## Key Principles of MLFQ

### Learning from History

The multi-level feedback queue is an excellent example of a system that learns from the past to predict the future. Such approaches are common in operating systems (and many other places in Computer Science, including hardware branch predictors and caching algorithms). Such approaches work when jobs have phases of behavior and are thus predictable; however, one must be careful with such techniques, as they can easily be wrong and drive a system to make worse decisions than it would have with no knowledge at all.

## Basic Rules of MLFQ

### Queue Structure

- MLFQ has a number of distinct queues, each assigned a different priority level.
- At any given time, a job that is ready to run is on a single queue.
- MLFQ uses priorities to decide which job should run at a given time: a job with higher priority (i.e., a job on a higher queue) is chosen to run.
- If more than one job is on a given queue (same priority), round-robin scheduling is used among those jobs.

### Fundamental Rules

1. **Rule 1**: If Priority(A) > Priority(B), A runs (B doesn't).
2. **Rule 2**: If Priority(A) = Priority(B), A & B run in Round Robin (RR).

The key to MLFQ scheduling lies in how the scheduler sets priorities. Rather than giving a fixed priority to each job, MLFQ varies the priority of a job based on its observed behavior.

- If a job repeatedly relinquishes the CPU while waiting for input from the keyboard, MLFQ will keep its priority high (this is how an interactive process might behave).
- If a job uses the CPU intensively for long periods of time, MLFQ will reduce its priority.

In this way, MLFQ tries to learn about processes as they run, and thus use the history of the job to predict its future behavior.

### Visual Representation (Figure 8.1)

```
Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8
[Low Priority] [High Priority]
D  C  A  B
```

*Figure 8.1: MLFQ Example*

- Two jobs (A and B) are at the highest priority level.
- Job C is in the middle.
- Job D is at the lowest priority.
- Given current knowledge, the scheduler would alternate between A and B (highest priority), while C and D would never get to run.

## Priority Adjustment Mechanism

### Key Concept: Allotment

The **allotment** is the amount of time a job can spend at a given priority level before the scheduler reduces its priority. For simplicity, initially, we assume the allotment is equal to a single time slice.

### Attempt #1: Basic Priority Adjustment Rules

- **Rule 3**: When a job enters the system, it is placed at the highest priority (the topmost queue).
- **Rule 4a**: If a job uses up its allotment while running, its priority is reduced (i.e., it moves down one queue).
- **Rule 4b**: If a job gives up the CPU (for example, by performing an I/O operation) before the allotment is up, it stays at the same priority level (i.e., its allotment is reset).

### Example 1: A Single Long-Running Job

*Figure 8.2: Long-running Job Over Time*

```
Q2
Q1
Q0
0 50 100 150 200
```

- Time slice = 10 ms
- Job enters at highest priority (Q2)
- After 10 ms (one time slice), scheduler reduces job's priority to Q1
- After another 10 ms, job is lowered to lowest priority (Q0)
- Job remains in Q0

### Example 2: Along Came A Short Job

*Figure 8.3 (left): Long-running job A and short interactive job B*

- Job A (long-running CPU-intensive) has been running in the lowest priority queue (Q0).
- Job B (short-running interactive) arrives at time T=100.
- B is inserted into the highest queue (Q2).
- B runs for 20 ms (two time slices) and completes.
- A resumes running at low priority.

**Why this works**: MLFQ assumes a job might be short (giving it high priority). If it's short, it runs quickly and completes. If it's not short, it moves down the queues, proving itself to be a long-running batch-like process. In this manner, MLFQ approximates SJF (Shortest Job First).

### Example 3: What About I/O?

*Figure 8.3 (right): Interactive job with I/O*

- Interactive job B (gray) needs CPU only for 1 ms before performing I/O.
- Long-running batch job A (black) is competing for CPU.
- MLFQ keeps B at highest priority because B relinquishes the CPU before its allotment expires.
- This achieves MLFQ's goal of running interactive jobs quickly.

## Problems with Current MLFQ

### Problem 1: Starvation

- If there are "too many" interactive jobs in the system, they will combine to consume all CPU time, and thus long-running jobs will never receive any CPU time (they starve).
- We'd like to make some progress on these jobs even in this scenario.

### Problem 2: Gaming the Scheduler

- A smart user could rewrite their program to game the scheduler.
- **Gaming**: Doing something sneaky to trick the scheduler into giving you more than your fair share of the resource.
- **Attack**: Before the allotment is used, issue an I/O operation (e.g., to a file) and thus relinquish the CPU.
- This allows the job to remain in the same queue, gaining a higher percentage of CPU time.
- When done right (e.g., running for 99% of the allotment before relinquishing the CPU), a job could nearly monopolize the CPU.

### Problem 3: Changing Behavior Over Time

- A program may change its behavior over time; what was CPU-bound may transition to a phase of interactivity.
- With current approach, such a job would be out of luck and not be treated like other interactive jobs in the system.

### Security Note on Scheduling

> **TIP: SCHEDULING MUST BE SECURE FROM ATTACK**
> 
> You might think that a scheduling policy, whether inside the OS itself (as discussed herein), or in a broader context (e.g., in a distributed storage system's I/O request handling), is not a security concern, but in increasingly many cases, it is exactly that. Consider the modern data center, in which users from around the world share CPUs, memories, networks, and storage systems; without care in policy design and enforcement, a single user may be able to adversely harm others and gain advantage for itself. Thus, scheduling policy forms an important part of the security of a system, and should be carefully constructed.

## Attempt #2: Priority Boost

### New Rule

- **Rule 5**: After some time period S, move all the jobs in the system to the topmost queue.

This new rule solves two problems at once:
1. Processes are guaranteed not to starve: by sitting in the top queue, a job will share the CPU with other high-priority jobs in a round-robin fashion, and thus eventually receive service.
2. If a CPU-bound job has become interactive, the scheduler treats it properly once it has received the priority boost.

### Example: Long-running Job Competing with Interactive Jobs

*Figure 8.4: Without (Left) and With (Right) Priority Boost*

```
Without Boost (Left):
Q2
Q1
Q0
0 50 100 150 200

With Boost (Right):
Q2
Q1
Q0
0 50 100 150 200
```

- Left: No priority boost → long-running job gets starved once short jobs arrive.
- Right: Priority boost every 100 ms → long-running job gets boosted to highest priority periodically, making some progress.

### The "Voo-Doo Constant" Problem

The addition of time period S leads to the obvious question: what should S be set to?

- John Ousterhout, a well-regarded systems researcher, used to call such values in systems **voo-doo constants**, because they seemed to require some form of black magic to set correctly.
- Unfortunately, S has that flavor:
  - If set too high, long-running jobs could starve.
  - If set too low, interactive jobs may not get a proper share of the CPU.
- As such, it is often left to the system administrator to find the right value – or in the modern world, increasingly, to automatic methods based on machine learning.

## Attempt #3: Better Accounting

### Problem with Current Rules

The real culprit is Rules 4a and 4b, which let a job retain its priority by relinquishing the CPU before its allotment expires.

### Solution: Better Accounting of CPU Time

Instead of forgetting how much of its allotment a process used at a given level when it performs I/O, the scheduler should keep track; once a process has used its allotment, it is demoted to the next priority queue. Whether it uses its allotment in one long burst or many small ones should not matter.

### Revised Rule 4

- **Rule 4**: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).

### Example: Preventing Gaming

*Figure 8.5: Without (Left) and With (Right) Gaming Tolerance*

```
Without Protection (Left):
Q0 Q1 Q2
0 50 100 150 200 250 300

With Better Accounting (Right):
Q0 Q1 Q2
0 50 100 150 200 250 300
```

- Left: Without protection from gaming, a process can issue I/O before its allotment ends, staying at the same priority level and dominating CPU time.
- Right: With better accounting, regardless of I/O behavior, the process slowly moves down queues and cannot gain an unfair share of the CPU.

### Ousterhout's Law

> **TIP: AVOID VOO-DOO CONSTANTS (OUSTERHOUT'S LAW)**
> 
> Avoiding voo-doo constants is a good idea whenever possible. Unfortunately, as in the example above, it is often difficult. One could try to make the system learn a good value, but that too is not straightforward.
> 
> The frequent result: a configuration file filled with default parameter values that a seasoned administrator can tweak when something isn't quite working correctly. As you can imagine, these are often left unmodified, and thus we are left to hope that the defaults work well in the field.
> 
> This tip brought to you by our old OS professor, John Ousterhout, and hence we call it Ousterhout's Law.

## Tuning MLFQ and Other Issues

### Parameterization Questions

- How many queues should there be?
- How big should the time slice be per queue?
- What should the allotment be?
- How often should priority be boosted to avoid starvation and account for changes in behavior?

There are no easy answers to these questions, and thus only some experience with workloads and subsequent tuning of the scheduler will lead to a satisfactory balance.

### Queue Time Slices

Most MLFQ variants allow for varying time-slice length across different queues:

- **High-priority queues** (for interactive jobs) are usually given short time slices (e.g., 10 or fewer milliseconds).
- **Low-priority queues** (for CPU-bound jobs) are given longer time slices (e.g., 100s of milliseconds).

*Figure 8.6: Lower Priority, Longer Quanta*

```
Q2
Q1
Q0
0 50 100 150 200 250 300
```

- Two jobs run for 20 ms at highest queue (with 10-ms time slice).
- 40 ms in middle queue (20-ms time slice).
- 40 ms time slice at lowest queue.

### Solaris MLFQ Implementation

- The Time-Sharing scheduling class (TS) is particularly easy to configure.
- It provides a set of tables that determine:
  - How priority of a process is altered throughout its lifetime
  - How long each time slice is
  - How often to boost priority of a job
- Default values:
  - 60 queues
  - Time-slice lengths: 20 milliseconds (highest priority) to a few hundred milliseconds (lowest)
  - Priorities boosted around every 1 second

### Alternative Implementations

- Some MLFQ schedulers don't use tables or the exact rules described in this chapter; rather they adjust priorities using mathematical formulae.
- **FreeBSD scheduler (version 4.3)** uses a formula to calculate current priority level based on CPU usage, with usage decayed over time.
- See Epema's paper for an excellent overview of such decay-usage algorithms and their properties.

### Additional Features

- Some schedulers reserve the highest priority levels for operating system work; thus typical user jobs can never obtain the highest levels of priority in the system.
- Some systems allow user advice to help set priorities (e.g., using `nice` command-line utility to increase or decrease priority).

### User Advice Tip

> **TIP: USE ADVICE WHERE POSSIBLE**
> 
> As the operating system rarely knows what is best for each and every process of the system, it is often useful to provide interfaces to allow users or administrators to provide some hints to the OS. We often call such hints **advice**, as the OS need not necessarily pay attention to it, but rather might take the advice into account in order to make a better decision.
> 
> Such hints are useful in many parts of the OS, including:
> - Scheduler (e.g., with `nice`)
> - Memory manager (e.g., `madvise`)
> - File system (e.g., informed prefetching and caching)

## MLFQ: Summary

### Refined Rules

The refined set of MLFQ rules, spread throughout the chapter, are:

1. **Rule 1**: If Priority(A) > Priority(B), A runs (B doesn't).
2. **Rule 2**: If Priority(A) = Priority(B), A & B run in round-robin fashion using the time slice (quantum length) of the given queue.
3. **Rule 3**: When a job enters the system, it is placed at the highest priority (the topmost queue).
4. **Rule 4**: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).
5. **Rule 5**: After some time period S, move all the jobs in the system to the topmost queue.

### Why MLFQ is Effective

MLFQ is interesting for the following reason: instead of demanding a priori knowledge of the nature of a job, it observes the execution of a job and prioritizes it accordingly. In this way, it manages to achieve the best of both worlds:

- It can deliver excellent overall performance (similar to SJF/STCF) for short-running interactive jobs.
- It is fair and makes progress for long-running CPU-intensive workloads.

### Systems Using MLFQ

Many systems use a form of MLFQ as their base scheduler, including:
- BSD UNIX derivatives ([LM+89, B86])
- Solaris ([M06])
- Windows NT and subsequent Windows operating systems ([CS97])

## References

- **[A+17]** "Automatic Database Management System Tuning Through Large-scale Machine Learning" by Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, Bohan Zhang. SIGMOD '17.
  - Not about CPU scheduling in OS, but an example of automatically tuning parameters via ML techniques.

- **[AD00]** "Multilevel Feedback Queue Scheduling in Solaris" by Andrea Arpaci-Dusseau.
  - Available: http://www.ostep.org/Citations/notes-solaris.pdf
  - Great short notes on Solaris scheduler details.

- **[B86]** "The Design of the UNIX Operating System" by M.J. Bach. Prentice-Hall, 1986.
  - Classic old book on how a real UNIX operating system is built.

- **[C+62]** "An Experimental Time-Sharing System" by F. J. Corbato, M. M. Daggett, R. C. Daley. IFIPS 1962.
  - Source of many first ideas in multi-level feedback scheduling (later went into Multics).

- **[CS97]** "Inside Windows NT" by Helen Custer and David A. Solomon. Microsoft Press, 1997.
  - The NT book for learning about something other than UNIX.

- **[E95]** "An Analysis of Decay-Usage Scheduling in Multiprocessors" by D.H.J. Epema. SIGMETRICS '95.
  - Nice paper on scheduling in mid-1990s, including overview of decay-usage schedulers.

- **[LM+89]** "The Design and Implementation of the 4.3BSD UNIX Operating System" by S.J. Leffler, M.K. McKusick, M.J. Karels, J.S. Quarterman. Addison-Wesley, 1989.
  - OS classic written by four main people behind BSD.

- **[M06]** "Solaris Internals: Solaris 10 and OpenSolaris Kernel Architecture" by Richard McDougall. Prentice-Hall, 2006.
  - Good book about Solaris and how it works.

- **[O11]** "John Ousterhout's Home Page" by John Ousterhout. www.stanford.edu/~ouster/.
  - Home page of famous Professor Ousterhout.

- **[P+95]** "Informed Prefetching and Caching" by R.H. Patterson, G.A. Gibson, E. Ginting, D. Stodolsky, J. Zelenka. SOSP '95.
  - Paper about cool ideas in file systems, including how applications can give OS advice.

- **[Y+18]** "Principled Schedulability Analysis for Distributed Storage Systems using Thread Architecture Models" by Suli Yang, Jing Liu, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. OSDI '18.
  - Recent work demonstrating difficulty of scheduling I/O requests in distributed storage systems.

## Homework (Simulation)

### Program: mlfq.py

This program allows you to see how the MLFQ scheduler presented in this chapter behaves. See the README for details.

### Questions

1. Run a few randomly-generated problems with just two jobs and two queues; compute the MLFQ execution trace for each. Make your life easier by limiting the length of each job and turning off I/Os.

2. How would you run the scheduler to reproduce each of the examples in the chapter?

3. How would you configure the scheduler parameters to behave just like a round-robin scheduler?

4. Craft a workload with two jobs and scheduler parameters so that one job takes advantage of the older Rules 4a and 4b (turned on with the `-S` flag) to game the scheduler and obtain 99% of the CPU over a particular time interval.

5. Given a system with a quantum length of 10 ms in its highest queue, how often would you have to boost jobs back to the highest priority level (with the `-B` flag) in order to guarantee that a single long-running (and potentially-starving) job gets at least 5% of the CPU?

6. One question that arises in scheduling is which end of a queue to add a job that just finished I/O; the `-I` flag changes this behavior for this scheduling simulator. Play around with some workloads and see if you can see the effect of this flag.

## Key Terms and Definitions

| Term | Definition |
|------|------------|
| **Multi-Level Feedback Queue (MLFQ)** | A scheduling algorithm that uses multiple priority queues and adjusts a job's priority based on its observed behavior. |
| **Allotment** | The amount of time a job can spend at a given priority level before the scheduler reduces its priority. |
| **Starvation** | A situation where a job never gets CPU time because higher-priority jobs continuously preempt it. |
| **Gaming the Scheduler** | Doing something sneaky to trick the scheduler into giving you more than your fair share of CPU time. |
| **Voo-Doo Constants** | Parameters in systems that seem to require black magic to set correctly, like the priority boost period S. |
| **Ousterhout's Law** | The principle that avoiding voo-doo constants is a good idea whenever possible. |
| **Priority Boost** | A mechanism where all jobs are moved to the topmost queue periodically to prevent starvation. |
| **Interactive Job** | A job that frequently relinquishes the CPU (e.g., waiting for user input), typically short-running. |
| **CPU-Bound Job** | A job that uses the CPU intensively for long periods, where response time isn't important. |
| **Turnaround Time** | The total time taken to complete a job from submission to completion. |
| **Response Time** | The time taken from when a request is made until the first response is produced. |
| **Round Robin (RR)** | A scheduling algorithm where each job gets a fixed time slice before being preempted. |

## Important Figures and Diagrams

### Figure 8.1: MLFQ Example
```
Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8
[Low Priority] [High Priority]
D  C  A  B
```
- Shows jobs at different priority levels (A and B highest, D lowest).

### Figure 8.2: Long-running Job Over Time
```
Q2
Q1
Q0
0 50 100 150 200
```
- Illustrates a long-running job moving from highest to lowest priority queue.

### Figure 8.3: Along Came An Interactive Job
```
Left (Without I/O):
Q0 Q1 Q2
0 50 100 150 200

Right (With I/O):
Q0 Q1 Q2
0 50 100 150 200
```
- Left: Long-running job A and short interactive job B.
- Right: Interactive job B with I/O behavior.

### Figure 8.4: Without (Left) and With (Right) Priority Boost
```
Without Boost (Left):
Q2
Q1
Q0
0 50 100 150 200

With Boost (Right):
Q2
Q1
Q0
0 50 100 150 200
```
- Left: Long-running job starved without priority boost.
- Right: Long-running job gets periodic CPU time with priority boost.

### Figure 8.5: Without (Left) and With (Right) Gaming Tolerance
```
Without Protection (Left):
Q0 Q1 Q2
0 50 100 150 200 250 300

With Better Accounting (Right):
Q0 Q1 Q2
0 50 100 150 200 250 300
```
- Left: Process can game scheduler by issuing I/O before allotment expires.
- Right: Process moves down queues regardless of I/O behavior.

### Figure 8.6: Lower Priority, Longer Quanta
```
Q2
Q1
Q0
0 50 100 150 200 250 300
```
- Shows different time slices at different priority levels (shorter at high priority, longer at low priority).
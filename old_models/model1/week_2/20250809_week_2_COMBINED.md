<!-- 
Combined Study Guide
Generated by LectureQ
Date: 2025-08-09 20:21:10
Week/Module: Week 2
Files Combined: 6
Content Type: Mixed
-->

# Week 2 - Complete Study Guide

> **Auto-generated combined study guide**  
> Generated from 6 individual notes files  
> Date: August 09, 2025

## ðŸ“š Table of Contents

1. [Operating Systems: Scheduling (Week 2, Lecture 2)](#operating-systems-scheduling-(week-2-lecture-2))
2. [Week 2: CPU Scheduling - Comprehensive Study Notes](#week-2-cpu-scheduling---comprehensive-study-notes)
3. [Chapter 9: Scheduling - Proportional Share](#chapter-9-scheduling---proportional-share)
4. [Week 2: CPU Scheduling - Multi-Level Feedback Queue (MLFQ)](#week-2-cpu-scheduling---multi-level-feedback-queue-(mlfq))
5. [Chapter Title](#chapter-title)
6. [Week 2: Textbook VM Mechanism - Comprehensive Study Notes](#week-2-textbook-vm-mechanism---comprehensive-study-notes)


---

# Operating Systems: Scheduling (Week 2, Lecture 2)

<think>
I need to create comprehensive lecture notes for a student who missed Week 2, Lecture 2 on Operating Systems Scheduling. I'll follow all the requirements and formatting instructions provided.

Let me break down what I need to include:

1. **Title**: Must include week and lecture information
2. **Overview**: A comprehensive paragraph describing the lecture
3. **Hierarchical headers** organized logically
4. **All content** from both slides and transcript
5. **Professional academic formatting** throughout
6. **Detailed tables, diagrams, and examples**
7. **Summary** with key takeaways
8. **Study questions** (10+)

Let me analyze the content I need to cover:

From the slides:
- Operating Systems Scheduling
- Last week's topic: Virtualisation
- Scheduler Summary: FCFS, SJF, STCF, RR, MLFQ
- Scheduling vs Dispatching
- Scheduling and States (Running, Ready, Blocked)
- Definitions: Workload, Job, Process, Scheduler, Metric
- Good vs Bad Scheduling (with hospital analogy)
- Metrics: Turnaround Time, Response Time, Wait Time, Throughput, Resource Utilisation, Overhead, Fairness
- Gantt Charts and Tables for scheduling examples
- Scheduling Assumptions
- FCFS, SJF, STCF, RR algorithms with examples
- I/O Blocking
- MLFQ (Multi-level Feedback Queue)
- Priority (Fixed vs Dynamic)
- Lottery Scheduling
- Completely Fair Scheduler (CFS)
- Multi-core CPUs

From the transcript:
- Instructor's explanations, analogies, and emphasis points
- Real-world examples (hospital analogy, university assignments)
- Student interactions and questions
- Instructor's clarifications and additional details

Now I'll create the notes following the detailed formatting instructions.

I'll start with the title, then overview, then organize the content into logical sections with appropriate headers.

Let me structure this:


### Overview
[Comprehensive paragraph describing what this lecture covers, main topics, and learning objectives]

---

### Scheduling Fundamentals

#### Last Week's Topic: Virtualisation
- Each process thinks it has its own computer
- Direct execution is faster
- Limited execution at key points to ensure OS control
- Hardware provides a lot of OS functionality
- User vs Kernel mode
- Clock functionality
- Register saving

#### Scheduling Overview
- **Scheduling**: Determining which process to dispatch (Conceptual "When")
- **Dispatcher**: Determines how to switch between processes (Mechanical "How")
  - User mode/kernel mode transitions
  - Saving PCBs (Process Control Blocks)
- **Scheduling vs Dispatching**:
  - Scheduler: Determines which process to dispatch (Conceptual)
  - Dispatcher: Determines how to switch between processes (Mechanical)

#### Scheduling States
Processes exist in three possible states:
- **Running**: The operating system thinks the process is doing work
- **Ready**: The process wants to do work but isn't currently running
- **Blocked**: The process can't do anything (typically waiting for I/O)

*Example*: 
- A process waiting for user input is blocked
- When the input is received, it transitions to ready
- The scheduler may choose to run it or not

#### Key Definitions
- **Workload**: The 'jobs' that need to be done
- **Job**: View as current CPU burst of a process
  - Processes alternate between CPU & I/O
  - Processes move between ready and blocked queues
- **Scheduler**: Logic for the job choice
- **Metric**: Something to determine good from bad

*Instructor Insight*: "If you don't know what metric means at this point in your university careers, I feel very bad for you. It means to determine if something is good or bad."

#### Good Scheduling vs Bad Scheduling
*Activity*: Imagine running a hospital with an operating room (OR) and patients needing surgery.

**Metrics for Good Scheduling**:
- **Turnaround Time**: Time between arrival and finish
  - *Example*: Patient arrives at 9am, surgery finishes at 12pm â†’ Turnaround time = 3 hours
- **Response Time**: Time between arrival and start of treatment
  - *Example*: Patient arrives at 9am, surgery starts at 9:30am â†’ Response time = 30 minutes
- **Wait Time**: Time waiting when could have been treated
  - *Example*: Patient arrives at 9am, surgery starts at 9:30am, but was waiting for 2 hours â†’ Wait time = 2 hours
- **Throughput**: Number of happy patients (non-dead) processed per unit time
- **Resource Utilisation**: Operating room should be busy as much as possible
- **Overhead (Switches)**: Minimise context switches (e.g., switching patients)
- **Fairness**: Even treatment distribution among patients

*Instructor Insight*: "The good is the left where it's kind of even nominally and the right is bad where green is dominating and blues getting not much treatment."

---

### Scheduling Algorithms

#### Gantt Charts
A visual representation of scheduling:
```
Time: 0    5    10    15
Task: Task1 Task2 Task3
```

*Instructor Insight*: "Gantt chart is your life once you have a job, right, or at least that's what your manager will think your life is."

#### Scheduling Assumptions
1. Jobs run for a fixed time
2. All jobs arrive simultaneously
3. All jobs are CPU-only
4. Run-time of each job is known

*Instructor Insight*: "These are some, none of these are true, right, like as in like in a real scheduling system, but when we're building algorithms we can start with like an easy example and then work out what's wrong with it and then expand on it."

#### First Come, First Serve (FCFS)
- Simplest scheduling algorithm
- Processes run in the order they arrive
- *Advantages*: Very simple to implement; has a 'veneer' of fairness
- *Problems*: Dependent on job order; big tasks can 'starve' small tasks (Convoy Effect)

*Example Calculation*:
| Job | Arrival Time | Burst Time | Turnaround Time |
|-----|--------------|------------|-----------------|
| A   | 0            | 5          | 5               |
| B   | 0            | 5          | 10              |
| C   | 0            | 5          | 15              |

Average Turnaround Time = (5 + 10 + 15) / 3 = 10

*Instructor Insight*: "If you've got a tutorial, I'll do it now, and then I'll wait and do the big job later. This is what most people sort of do."

*Convoy Effect*:
```
Big Task
Big Task
The Convoy
```

*Instructor Insight*: "You have this one task that's gonna take forever to do anything, and all these little tasks that are really quick and speedy, who are just sitting and waiting for the long task to complete."

#### Shortest Job First (SJF)
- Schedule the shortest job first
- Minimizes average turnaround time
- *Problem*: If jobs don't arrive simultaneously, it may not be optimal

*Example Calculation*:
| Job | Arrival Time | Burst Time | Turnaround Time |
|-----|--------------|------------|-----------------|
| A   | 0            | 20         | 20              |
| B   | 0            | 4          | 24              |
| C   | 0            | 4          | 28              |

Average Turnaround Time = (20 + 24 + 28) / 3 = 24

*Alternative Example*:
| Job | Arrival Time | Burst Time | Turnaround Time |
|-----|--------------|------------|-----------------|
| A   | 0            | 4          | 4               |
| B   | 0            | 4          | 8               |
| C   | 0            | 20         | 28              |

Average Turnaround Time = (4 + 8 + 28) / 3 = 16.6

*Instructor Insight*: "Shortest job first is the solution to all our problems. Nothing will ever go wrong."

#### Shortest Time to Completion First (STCF)
- Preemptive version of SJF
- Always run the process with the shortest remaining time
- *Metrics*: Turnaround Time = completion_time - arrival_time; Response Time = start_time - arrival_time

*Example Calculation*:
| Job | Arrival Time | Burst Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|---------------|
| A   | 0            | 10         | 10              | 0             |
| B   | 2            | 10         | 18              | 8             |
| C   | 4            | 10         | 26              | 16            |

Average Response Time = (0 + 8 + 16) / 3 = 8

*Instructor Insight*: "Shortest time to completion first is, Earliest deadline first, which is what we call it in real-time systems."

#### Round Robin (RR)
- Each process gets a fixed time slice (quantum)
- After the quantum expires, the process is preempted
- *Metrics*: Turnaround Time = completion_time - arrival_time; Response Time = start_time - arrival_time

*Example Calculation*:
| Job | Arrival Time | Burst Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|---------------|
| A   | 0            | 10         | 26              | 0             |
| B   | 2            | 10         | 26              | 0             |
| C   | 4            | 10         | 26              | 0             |

Average Response Time = (0 + 0 + 0) / 3 = 0

*Instructor Insight*: "The main thing with round robin, I think from a conceptual level is like how long, how often do you switch."

#### I/O Blocking
- Processes can be interrupted for I/O operations
- *Key Insight*: "Don't waste CPU" - if a process is waiting for I/O, run another process

*Example*:
```
Time: 0    2    4    6    8   10
Task: CPU  I/O  CPU  I/O  CPU  I/O
```

*Instructor Insight*: "If a process is interruptible like this, where it has a lot of little interruptions, it generally means that it doesn't do very much."

---

### Advanced Scheduling

#### Multi-Level Feedback Queue (MLFQ)
- **Situation**:
  - Jobs can be of varying length
  - Jobs can arrive at any time
  - We don't know when they will finish
  - Different jobs have different aims:
    - Interactive Jobs: Response Time
    - Batch Jobs: Turnaround Time

#### Priority Scheduling
- **Fixed Priority**:
  - Higher priority jobs run first
  - *Problem*: Long jobs can starve short jobs
  - *Example*:
    ```
    Big Job
    Small Job
    Small Job Big Job Big Job
    ```

- **Dynamic Priority**:
  - If a job takes a lot of CPU (runs for too long), reduce its priority
  - If a job uses only a little CPU, keep its priority high
  - *Instructor Insight*: "If a job can only 'lose priority', won't it eventually get ignored?"

#### MLFQ Rules
1. Run high priority jobs first (round robin for ties)
2. You start high priority
3. If you run for too long, your priority drops
4. Every so often, put jobs back into the top priority

*Instructor Insight*: "If you're a really big job, right, you, you run and then you're like, oh I've used my time, lower priority, I've used my time, uh lower priority, I've used my time. I want time, I want time, and then eventually it'll just reset."

#### MLFQ Design Decisions
- How many queues?
- How long is an allotment?
- Are allotments the same for each queue level?
- How often to refresh priorities?

*Instructor Insight*: "How many cus? Who knows? Who, who knows, it depends on what you're running, right? Like as in like, but we have to make this kind of decision. The way that you make these decisions is you run it and you see, right, and you experiment because these tasks are so like stupidly complicated."

#### Lottery Scheduling
- Processes get tickets based on priority
- Random ticket selection determines which process runs
- *Instructor Insight*: "It turns out it's actually not too bad. So basically the idea is you just, you just have a random number generator and you and you have tickets and you give processors tickets depending on how high a priority they need to be, and then you just like roll the dice..."

*Potential Issues*:
- Ticket trading
- Ticket inflation
- Ticket currencies

#### Completely Fair Scheduler (CFS)
- **Premise**:
  - All tasks have their vruntime measured
  - Smallest vruntime tasks are run with priority
  - Each task runs for a bit over target latency period
  - Each task gets a share depending on its weight (no starvation)
  - Works using a Red/Black tree (used in Linux since 2007)

*Instructor Insight*: "This is where we have to say if you use a full allotment, not your full allotment, just a full allotment, so we need to start counting allotments..."

*Example*:
- Task 1: Video Rendering (Long run-time, CPU intensive, non-interactive)
- Task 2: Word Processor (Interactive, intermittent CPU bursts)

*How CFS Works*:
- Task 2 accumulates little vruntime
- When it wakes (user input), it gets run immediately

*Instructor Insight*: "The 'nice' can be thought of as a 'soft' priority. Changing a program's nice value changes the weighting for CFS."

#### Multi-core CPUs
- Linux: One red-black tree per core
- *Challenge*: Fairly distribute tasks between cores
- *Instructor Insight*: "It's like, oh, I put you, you know, you know, who who's done this at like a shopping centre or like when you're waiting at the airport or something like that, and you're like, there are like 6 queues and you're like, I shall pick the queue. And then there's like someone who has a complaint with the manager in front of you on the line and you're just there going, right, I picked the wrong queue."

---

### Summary / Key Takeaways

1. **Scheduling Fundamentals**: Scheduling determines *when* to run processes (conceptual), while dispatching determines *how* to switch between them (mechanical).
2. **Key Metrics**: Turnaround Time (completion - arrival), Response Time (start - arrival), Wait Time, Throughput, Resource Utilisation, Overhead, Fairness.
3. **FCFS**: Simple but can lead to convoy effect where long jobs starve short jobs.
4. **SJF**: Minimizes average turnaround time but requires knowing job lengths in advance.
5. **STCF**: Preemptive SJF that minimizes response time for interactive processes.
6. **RR**: Fair scheduling with fixed time quantum, but can have high overhead if quantum is too small.
7. **MLFQ**: Advanced scheduling that dynamically adjusts priority based on CPU usage.
8. **CFS**: Completely Fair Scheduler used in Linux since 2007, using vruntime and Red/Black trees.
9. **Priority Scheduling**: Fixed priority can lead to starvation; dynamic priority adjusts based on usage.
10. **I/O Blocking**: Critical for efficient scheduling - don't waste CPU when processes are waiting for I/O.

---

### Study Questions

1. Explain the difference between scheduling and dispatching, including their roles in process management.

2. Describe the convoy effect with an example of how it impacts scheduling performance.

3. Calculate the average turnaround time for a set of processes using FCFS scheduling, given their arrival times and burst times.

4. How does Shortest Time to Completion First (STCF) differ from Shortest Job First (SJF)? Provide an example where STCF outperforms SJF.

5. What is the main disadvantage of Round Robin (RR) scheduling when the time quantum is too small? How does this relate to overhead?

6. Explain how I/O blocking affects process scheduling and why it's important to consider in scheduling algorithms.

7. Describe the rules of Multi-Level Feedback Queue (MLFQ) scheduling, including how priority changes over time.

8. What is the purpose of the "nice" value in Linux scheduling? How does it affect a process's priority?

9. How does the Completely Fair Scheduler (CFS) use vruntime to determine which process should run next?

10. Explain why dynamic priority adjustment is necessary in scheduling systems, and describe how it prevents starvation.

11. Describe the Red-Black tree structure and explain why it's used in the Completely Fair Scheduler.

12. What are the main challenges in scheduling processes on multi-core CPUs, and how might these be addressed?

13. Compare and contrast FCFS, SJF, STCF, and RR scheduling algorithms in terms of their suitability for different types of workloads (interactive vs batch).

14. Explain the concept of "allotment" in MLFQ scheduling and why it's important to track how much of an allotment a process has used.

15. How does the Completely Fair Scheduler ensure that no process starves, even if it's a long-running batch job?

16. Describe the lottery scheduling algorithm and explain how it could be implemented with ticket trading.

17. What is the significance of the "target latency" in the Completely Fair Scheduler?

18. Explain the difference between a preemptive and non-preemptive scheduling algorithm, and provide an example of each.

19. How does the priority of a process change in a dynamic priority scheduling system when it has been running for a long time?

20. Describe the main components of a Process Control Block (PCB) and explain how it's used during context switching.
</think>

# Operating Systems: Scheduling (Week 2, Lecture 2)

### Overview
This lecture covers the fundamental concepts of process scheduling in operating systems, including scheduling algorithms, key metrics for evaluating scheduling performance, and advanced scheduling techniques. The lecture begins with a review of last week's topic on virtualization before diving into scheduling fundamentals. Key topics include the distinction between scheduling and dispatching, process states (running, ready, blocked), scheduling metrics (turnaround time, response time, throughput), and various scheduling algorithms including First Come First Serve (FCFS), Shortest Job First (SJF), Shortest Time to Completion First (STCF), Round Robin (RR), and Multi-Level Feedback Queue (MLFQ). The lecture also explores dynamic priority scheduling, lottery scheduling, and the Completely Fair Scheduler (CFS) used in Linux, with practical examples and real-world analogies to illustrate concepts.

---

### Scheduling Fundamentals

#### Scheduling vs Dispatching
- **Scheduler**: Determines *which process to dispatch* (Conceptual "When")
  - *Instructor Insight*: "Scheduler: Determines which process to dispatch. Conceptual (When)"
  
- **Dispatcher**: Determines *how to switch* between processes (Mechanical "How")
  - *Instructor Insight*: "Dispatcher: Determines how to switch between processes. User mode/kernel mode. Saving PCBs... Mechanical (How)"

*Key Distinction*: 
- **Scheduling** is the *conceptual decision* of which process to run next
- **Dispatching** is the *mechanical implementation* of switching to that process

#### Process States
Processes exist in three possible states:
- **Running**: The operating system thinks the process is doing work
- **Ready**: The process wants to do work but isn't currently running
- **Blocked**: The process can't do anything (typically waiting for I/O)

*Instructor Insight*: "Locked is pretty easy to understand. It basically means that like you want to use IO in some way, so you're like, I would like to use the printer as a process, and now I can't use the printer, so I'm just standing there waiting, waiting."

*State Transitions*:
```
Running â†’ Blocked (I/O request)
Blocked â†’ Ready (I/O completed)
Ready â†’ Running (Scheduler selects process)
```

#### Key Definitions
- **Workload**: The 'jobs' that need to be done
- **Job**: View as current CPU burst of a process
  - *Instructor Insight*: "A job is like I have a job, it's some run processy bit for a bit, and that's kind of what a job is"
  
- **Process**: Alternates between CPU & I/O
  - *Instructor Insight*: "Process sometimes alternates between CPU & I/O. Process moves between ready and blocked queues"

- **Scheduler**: Logic for the job choice
  - *Instructor Insight*: "Scheduler: Logic for the job choice"

- **Metric**: Something to determine good from bad
  - *Instructor Insight*: "If you don't know what metric means at this point in your university careers, I feel very bad for you. It means to determine if something is good or bad."

---

### Scheduling Metrics

#### Hospital Analogy Activity
*Instructor Activity*: "Imagine you are running a hospital and you have an operating room (for surgery). You have a list of patients who need various operations... How do we determine the best way to use the operating room? What things do we want to maximise or minimise?"

#### Key Metrics for Scheduling
| Metric | Description | Formula |
|--------|-------------|---------|
| **Turnaround Time** | Time between arrival and finish | Completion time - Arrival time |
| **Response Time** | Time between arrival and start of treatment | Start time - Arrival time |
| **Wait Time** | Time waiting when could have been treated | (Start time - Arrival time) - (Time spent processing) |
| **Throughput** | Number of happy patients processed per unit time | Total patients processed / Total time |
| **Resource Utilisation** | Operating room should be busy as much as possible | (Time operating room is busy) / (Total time) |
| **Overhead (Switches)** | Cost of switching between processes | Number of context switches |
| **Fairness** | Even treatment distribution among patients | Equal treatment over time |

*Instructor Insight*: "The good is the left where it's kind of even nominally and the right is bad where green is dominating and blues getting not much treatment."

---

### Scheduling Algorithms

#### Gantt Charts
A visual representation of scheduling:
```
Time: 0    5    10    15
Task: Task1 Task2 Task3
```

*Instructor Insight*: "Gantt chart is your life once you have a job, right, or at least that's what your manager will think your life is."

#### Scheduling Assumptions
1. Jobs run for a fixed time
2. All jobs arrive simultaneously
3. All jobs are CPU-only
4. Run-time of each job is known

*Instructor Insight*: "These are some, none of these are true, right, like as in like in a real scheduling system, but when we're building algorithms we can start with like an easy example and then work out what's wrong with it and then expand on it."

---

#### First Come, First Serve (FCFS)
- Simplest scheduling algorithm
- Processes run in the order they arrive
- *Advantages*: Very simple to implement; has a 'veneer' of fairness
- *Problems*: Dependent on job order; big tasks can 'starve' small tasks (Convoy Effect)

*Example Calculation*:
| Job | Arrival Time | Burst Time | Turnaround Time |
|-----|--------------|------------|-----------------|
| A   | 0            | 5          | 5               |
| B   | 0            | 5          | 10              |
| C   | 0            | 5          | 15              |

Average Turnaround Time = (5 + 10 + 15) / 3 = 10

*Alternative Example*:
| Job | Arrival Time | Burst Time | Turnaround Time |
|-----|--------------|------------|-----------------|
| A   | 0            | 20         | 20              |
| B   | 0            | 4          | 24              |
| C   | 0            | 4          | 28              |

Average Turnaround Time = (20 + 24 + 28) / 3 = 24

*Convoy Effect*:
```
Big Task
Big Task
The Convoy
```

*Instructor Insight*: "You have this one task that's gonna take forever to do anything, and all these little tasks that are really quick and speedy, who are just sitting and waiting for the long task to complete."

---

#### Shortest Job First (SJF)
- Schedule the shortest job first
- Minimizes average turnaround time
- *Problem*: If jobs don't arrive simultaneously, it may not be optimal

*Example Calculation*:
| Job | Arrival Time | Burst Time | Turnaround Time |
|-----|--------------|------------|-----------------|
| A   | 0            | 20         | 20              |
| B   | 0            | 4          | 24              |
| C   | 0            | 4          | 28              |

Average Turnaround Time = (20 + 24 + 28) / 3 = 24

*Alternative Example*:
| Job | Arrival Time | Burst Time | Turnaround Time |
|-----|--------------|------------|-----------------|
| A   | 0            | 4          | 4               |
| B   | 0            | 4          | 8               |
| C   | 0            | 20         | 28              |

Average Turnaround Time = (4 + 8 + 28) / 3 = 16.6

*Instructor Insight*: "Shortest job first is the solution to all our problems. Nothing will ever go wrong."

---

#### Shortest Time to Completion First (STCF)
- Preemptive version of SJF
- Always run the process with the shortest remaining time
- *Metrics*: Turnaround Time = completion_time - arrival_time; Response Time = start_time - arrival_time

*Example Calculation*:
| Job | Arrival Time | Burst Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|---------------|
| A   | 0            | 10         | 10              | 0             |
| B   | 2            | 10         | 18              | 8             |
| C   | 4            | 10         | 26              | 16            |

Average Response Time = (0 + 8 + 16) / 3 = 8

*Instructor Insight*: "Shortest time to completion first is, Earliest deadline first, which is what we call it in real-time systems."

---

#### Round Robin (RR)
- Each process gets a fixed time slice (quantum)
- After the quantum expires, the process is preempted
- *Metrics*: Turnaround Time = completion_time - arrival_time; Response Time = start_time - arrival_time

*Example Calculation*:
| Job | Arrival Time | Burst Time | Turnaround Time | Response Time |
|-----|--------------|------------|-----------------|---------------|
| A   | 0            | 10         | 26              | 0             |
| B   | 2            | 10         | 26              | 0             |
| C   | 4            | 10         | 26              | 0             |

Average Response Time = (0 + 0 + 0) / 3 = 0

*Instructor Insight*: "The main thing with round robin, I think from a conceptual level is like how long, how often do you switch, right? Because if you switch a lot from an operating system point of view, you know, you're gonna grab stuff and get into memory and then run across and be able to put it back in memory, right, you don't want to do that too often."

---

#### I/O Blocking
- Processes can be interrupted for I/O operations
- *Key Insight*: "Don't waste CPU" - if a process is waiting for I/O, run another process

*Example*:
```
Time: 0    2    4    6    8   10
Task: CPU  I/O  CPU  I/O  CPU  I/O
```

*Instructor Insight*: "If a process is interruptible like this, where it has a lot of little interruptions, it generally means that it doesn't do very much. So when you press something on the keyboard, it's not a very complicated process, right, you're just getting like a signal."

---

### Advanced Scheduling

#### Multi-Level Feedback Queue (MLFQ)
- **Situation**:
  - Jobs can be of varying length
  - Jobs can arrive at any time
  - We don't know when they will finish
  - Different jobs have different aims:
    - Interactive Jobs: Response Time
    - Batch Jobs: Turnaround Time

*Instructor Insight*: "Interactive job, using the keyboard, you want response time. So when, like when you're using a computer, if you click on something and nothing happens for 3 seconds, do you get pissed off? The answer is yes. But if you run like a big long programme and it runs for 10 hours, you don't care."

---

#### Priority Scheduling
- **Fixed Priority**:
  - Higher priority jobs run first
  - *Problem*: Long jobs can starve short jobs
  - *Example*:
    ```
    Big Job
    Small Job
    Small Job Big Job Big Job
    ```

- **Dynamic Priority**:
  - If a job takes a lot of CPU (runs for too long), reduce its priority
  - If a job uses only a little CPU, keep its priority high
  - *Instructor Insight*: "If a job can only 'lose priority', won't it eventually get ignored?"

*Malicious Strategy Question*: "Under this system, how can we be evil?"

*Instructor Insight*: "How would you design your system if you were a process to make sure that you're always in the high priority queue? ... The correct answer is: you work out how long you're allowed to run on the operating system before it demotes your priority, and you sleep one microsecond before that happens."

---

#### MLFQ Rules
1. Run high priority jobs first (round robin for ties)
2. You start high priority
3. If you run for too long, your priority drops
4. Every so often, put jobs back into the top priority

*Instructor Insight*: "If you're a really big job, right, you, you run and then you're like, oh I've used my time, lower priority, I've used my time, uh lower priority, I've used my time. I want time, I want time, and then eventually it'll just reset."

*Instructor Insight*: "You can't wait forever under this system. It's the glory that is the multi-level feedback queue."

---

#### MLFQ Design Decisions
- How many queues?
- How long is an allotment?
- Are allotments the same for each queue level?
- How often to refresh priorities?

*Instructor Insight*: "How many cus? Who knows? Who, who knows, it depends on what you're running, right? Like as in like, but we have to make this kind of decision. The way that you make these decisions is you run it and you see, right, and you experiment because these tasks are so like stupidly complicated."

---

#### Lottery Scheduling
- Processes get tickets based on priority
- Random ticket selection determines which process runs
- *Instructor Insight*: "It turns out it's actually not too bad. So basically the idea is you just, you just have a random number generator and you and you have tickets and you give processors tickets depending on how high a priority they need to be, and then you just like roll the dice..."

*Potential Issues*:
- Ticket trading
- Ticket inflation
- Ticket currencies

---

#### Completely Fair Scheduler (CFS)
- **Premise**:
  - All tasks have their vruntime measured
  - Smallest vruntime tasks are run with priority
  - Each task runs for a bit over target latency period
  - Each task gets a share depending on its weight (no starvation)
  - Works using a Red/Black tree (used in Linux since 2007)

*Instructor Insight*: "This is where we have to say if you use a full allotment, not your full allotment, just a full allotment, so we need to start counting allotments..."

*Example*:
- Task 1: Video Rendering (Long run-time, CPU intensive, non-interactive)
- Task 2: Word Processor (Interactive, intermittent CPU bursts)

*How CFS Works*:
```
Task 2 accumulates little vruntime. When it wakes (user input) it gets run immediately.
```

*Instructor Insight*: "The 'nice' can be thought of as a 'soft' priority. Changing a program's nice value changes the weighting for CFS."

---

#### Multi-core CPUs
- Linux: One red-black tree per core
- *Challenge*: Fairly distribute tasks between cores
- *Instructor Insight*: "It's like, oh, I put you, you know, you know, who who's done this at like a shopping centre or like when you're waiting at the airport or something like that, and you're like, there are like 6 queues and you're like, I shall pick the queue. And then there's like someone who has a complaint with the manager in front of you on the line and you're just there going, right, I picked the wrong queue."

---

### Summary / Key Takeaways

1. **Scheduling Fundamentals**: Scheduling determines *when* to run processes (conceptual), while dispatching determines *how* to switch between them (mechanical).
2. **Key Metrics**: Turnaround Time (completion - arrival), Response Time (start - arrival), Wait Time, Throughput, Resource Utilisation, Overhead, and Fairness are critical for evaluating scheduling performance.
3. **FCFS**: Simple but can lead to convoy effect where long jobs starve short jobs.
4. **SJF**: Minimizes average turnaround time but requires knowing job lengths in advance.
5. **STCF**: Preemptive SJF that minimizes response time for interactive processes.
6. **RR**: Fair scheduling with fixed time quantum, but can have high overhead if quantum is too small.
7. **MLFQ**: Advanced scheduling that dynamically adjusts priority based on CPU usage to balance interactive and batch workloads.
8. **CFS**: Completely Fair Scheduler used in Linux since 2007, using vruntime and Red/Black trees to ensure fair CPU allocation.
9. **Priority Scheduling**: Fixed priority can lead to starvation; dynamic priority adjusts based on usage to prevent starvation.
10. **I/O Blocking**: Critical for efficient scheduling - don't waste CPU when processes are waiting for I/O.

---

### Study Questions

1. Explain the difference between scheduling and dispatching, including their roles in process management.

2. Describe the convoy effect with an example of how it impacts scheduling performance.

3. Calculate the average turnaround time for a set of processes using FCFS scheduling, given their arrival times and burst times.

4. How does Shortest Time to Completion First (STCF) differ from Shortest Job First (SJF)? Provide an example where STCF outperforms SJF.

5. What is the main disadvantage of Round Robin (RR) scheduling when the time quantum is too small? How does this relate to overhead?

6. Explain how I/O blocking affects process scheduling and why it's important to consider in scheduling algorithms.

7. Describe the rules of Multi-Level Feedback Queue (MLFQ) scheduling, including how priority changes over time.

8. What is the purpose of the "nice" value in Linux scheduling? How does it affect a process's priority?

9. How does the Completely Fair Scheduler (CFS) use vruntime to determine which process should run next?

10. Explain why dynamic priority adjustment is necessary in scheduling systems, and describe how it prevents starvation.

11. Describe the Red-Black tree structure and explain why it's used in the Completely Fair Scheduler.

12. What are the main challenges in scheduling processes on multi-core CPUs, and how might these be addressed?

13. Compare and contrast FCFS, SJF, STCF, and RR scheduling algorithms in terms of their suitability for different types of workloads (interactive vs batch).

14. Explain the concept of "allotment" in MLFQ scheduling and why it's important to track how much of an allotment a process has used.

15. How does the Completely Fair Scheduler ensure that no process starves, even if it's a long-running batch job?

16. Describe the lottery scheduling algorithm and explain how it could be implemented with ticket trading.

17. What is the significance of the "target latency" in the Completely Fair Scheduler?

18. Explain the difference between a preemptive and non-preemptive scheduling algorithm, and provide an example of each.

19. How does the priority of a process change in a dynamic priority scheduling system when it has been running for a long time?

20. Describe the main components of a Process Control Block (PCB) and explain how it's used during context switching.


---

# Week 2: CPU Scheduling - Comprehensive Study Notes

<think>
I need to create comprehensive study notes from the textbook chapter "CPU Scheduling" based on the provided slides. I'll make sure to include EVERY detail without summarizing.

Let me break down the content slide by slide and extract all information:

Slide 1:
- Introduces scheduling as a high-level policy for OS scheduler
- Mentions that scheduling has origins in operations management
- Presents "THE CRUX" question about developing scheduling policies
- Lists key assumptions about workload (which will be discussed)

Slide 2:
- Lists specific workload assumptions:
  1. Each job runs for the same amount of time.
  2. All jobs arrive at the same time.
  3. Once started, each job runs to completion.
  4. All jobs only use the CPU (no I/O).
  5. The run-time of each job is known.
- Explains that assumption 5 (knowing run-time) makes scheduler "omniscient"
- Introduces scheduling metrics, specifically turnaround time
- Defines turnaround time: Tturnaround = Tcompletion - Tarrival
- Notes that with all jobs arriving at same time, Tarrival = 0 and Tturnaround = Tcompletion
- Mentions fairness as another metric (Jain's Fairness Index)
- Introduces FIFO (First In, First Out) scheduling

Slide 3:
- Explains FIFO properties: simple and easy to implement
- Gives example with 3 jobs (A, B, C) each running for 10 seconds
- Shows average turnaround time = 20 seconds
- Relaxes assumption 1 (jobs run for same time) and shows FIFO performs poorly with different job lengths
- Example: A runs 100 seconds, B and C run 10 seconds each
- Shows convoy effect (B+79 reference)
- Explains convoy effect: short jobs get queued behind a long job
- Mentions grocery store analogy for convoy effect

Slide 4:
- Introduces Shortest Job First (SJF) as solution to FIFO problem
- Explains SJF runs shortest job first, then next shortest, etc.
- Gives example with A (100s), B (10s), C (10s)
- Shows average turnaround time improves from 110s to 50s
- Mentions SJF is optimal under certain assumptions
- Notes that SJF is non-preemptive

Slide 5:
- Discusses preemptive vs. non-preemptive schedulers
- Explains that modern schedulers are preemptive
- Introduces Shortest Time-to-Completion First (STCF) or Preemptive Shortest Job First (PSJF)
- Explains STCF runs the job with least time remaining
- Gives example where A arrives at t=0 (100s), B and C arrive at t=10 (10s each)
- Shows STCF improves average turnaround time from 103.33s to 50s
- Notes STCF is provably optimal

Slide 6:
- Introduces new metric: response time
- Defines response time: Tresponse = Tfirstrun - Tarrival
- Gives example with schedule from Slide 5 (A at t=0, B and C at t=10)
- Shows response times: A=0, B=0, C=10 (average=3.33)
- Explains STCF is bad for response time
- Mentions interactive performance as a concern

Slide 7:
- Introduces Round Robin (RR) scheduling
- Explains RR runs a job for a time slice (quantum) then switches to next job
- Defines time slice as multiple of timer-interrupt period
- Gives example with 3 jobs (A, B, C) each running for 5s
- Shows SJF vs. RR with 1s time slice
- Calculates average response time: RR=1, SJF=5
- Explains trade-off with time slice length:
  - Shorter time slice = better response time
  - Too short = context switching cost dominates
- Mentions context switching cost includes cache, TLB, branch predictor flushing

Slide 8:
- Explains that RR is bad for turnaround time (worse than FIFO)
- Gives example: A, B, C each run for 5s, RR with 1s time slice finishes at 13, 14, 15 (avg=14)
- Explains the trade-off between fairness and metrics:
  - Unfair = better turnaround time (run short jobs to completion)
  - Fair = better response time (evenly divide CPU)
- Mentions "You can't have your cake and eat it too" as a saying

Slide 9:
- Discusses incorporating I/O into scheduling
- Explains that when a job issues I/O, it's blocked and scheduler should run another job
- Gives example with two jobs:
  - A: runs 10ms, does I/O (10ms), repeats (5 times)
  - B: runs 50ms without I/O
- Shows poor resource use in Figure 7.8 (A's I/O time wasted)
- Shows better resource use in Figure 7.9 (overlap of CPU and I/O)
- Explains that scheduler treats each CPU burst as a job

Slide 10:
- Discusses the problem of not knowing job lengths (assumption 5)
- Explains that OS usually doesn't know job lengths
- Mentions that we need a scheduler that behaves like SJF/STCF without knowing job lengths
- Mentions incorporating RR ideas for good response time

Slide 11:
- Summarizes two families of schedulers:
  - First family: runs shortest job remaining (optimizes turnaround time)
  - Second family: alternates between all jobs (optimizes response time)
- Notes the inherent trade-off
- Mentions incorporating I/O
- States that the fundamental problem is OS can't see into future
- Introduces multi-level feedback queue as solution for next chapter

Slide 12:
- Lists references with brief descriptions

Slide 13:
- Includes homework/simulation details and questions

Now I'll create comprehensive study notes following all the instructions, making sure to include every detail without summarizing.

I'll structure it with proper headers, bullet points, code blocks for formulas, and tables where appropriate. I'll make sure to include all examples, definitions, metrics, and explanations.
</think>


### Chapter Introduction

Scheduling is a high-level policy employed by an OS scheduler that determines the order in which processes are executed. While low-level mechanisms like context switching should be understood (if not, review previous chapters), this chapter focuses on the high-level policies that govern scheduling.

The origins of scheduling predate computer systems, drawing from operations management and applied to computers. Early approaches were taken from fields like assembly lines, where similar concerns about efficiency exist.

**THE CRUX: HOW TO DEVELOP SCHEDULING POLICY**  
*How should we develop a basic framework for thinking about scheduling policies? What are the key assumptions? What metrics are important? What basic approaches have been used in the earliest computer systems?*

---

### 7.1 Workload Assumptions

Before exploring scheduling policies, we must make simplifying assumptions about the processes (sometimes called jobs) running in the system. These assumptions collectively form the "workload" and are critical for developing policies.

**The workload assumptions we make are:**

1. Each job runs for the same amount of time.
2. All jobs arrive at the same time.
3. Once started, each job runs to completion.
4. All jobs only use the CPU (i.e., they perform no I/O).
5. The run-time of each job is known.

**Important Note:**  
*We said many of these assumptions were unrealistic, but just as some animals are more equal than others in Orwell's Animal Farm [O45], some assumptions are more unrealistic than others in this chapter. In particular, it might bother you that the run-time of each job is known: this would make the scheduler omniscient, which, although it would be great (probably), is not likely to happen anytime soon.*

---

### 7.2 Scheduling Metrics

To compare different scheduling policies, we need a scheduling metricâ€”a measure used to evaluate performance.

**Primary Metric: Turnaround Time**  
The turnaround time of a job is defined as the time at which the job completes minus the time at which the job arrived in the system. More formally:

```
Tturnaround = Tcompletion - Tarrival (7.1)
```

*Because we have assumed that all jobs arrive at the same time, for now Tarrival = 0 and hence Tturnaround = Tcompletion. This fact will change as we relax the aforementioned assumptions.*

**Additional Metric: Fairness**  
Fairness is measured (for example) by Jain's Fairness Index [J91]. Performance and fairness are often at odds in scheduling; a scheduler may optimize performance but at the cost of preventing a few jobs from running, thus decreasing fairness.

*This conundrum shows us that life isn't always perfect.*

---

### 7.3 First In, First Out (FIFO)

The most basic algorithm is **First In, First Out (FIFO)** scheduling, also known as First Come, First Served (FCFS).

**Properties of FIFO:**
- Clearly simple and easy to implement
- Works pretty well under our initial assumptions

**Example with Equal Job Lengths:**  
Imagine three jobs arrive in the system, A, B, and C, at roughly the same time (Tarrival = 0). Assume A arrived just before B, which arrived just before C. Each job runs for 10 seconds.

```
Time: 0 20 40 60 80 100 120
      ABC
```

*Figure 7.1: FIFO Simple Example*

- A finishes at 10
- B finishes at 20
- C finishes at 30

Average turnaround time = (10 + 20 + 30) / 3 = 20

**Relaxing Assumption 1 (Jobs Run for Different Amounts of Time):**  
Now assume A runs for 100 seconds while B and C run for 10 each.

```
Time: 0 20 40 60 80 100 120
      ABC
```

*Figure 7.2: Why FIFO Is Not That Great*

- A finishes at 100
- B finishes at 110
- C finishes at 120

Average turnaround time = (100 + 110 + 120) / 3 = 110 seconds

**Convoy Effect:**  
This problem is generally referred to as the **convoy effect** [B+79], where a number of relatively-short potential consumers of a resource get queued behind a heavyweight resource consumer.

*This scheduling scenario might remind you of a single line at a grocery store and what you feel like when you see the person in front of you with three carts full of provisions and a checkbook out; it's going to be a while.*

**TIP: THE PRINCIPLE OF SJF**  
*Shortest Job First represents a general scheduling principle that can be applied to any system where the perceived turnaround time per customer (or, in our case, a job) matters. Think of any line you have waited in: if the establishment in question cares about customer satisfaction, it is likely they have taken SJF into account. For example, grocery stores commonly have a "ten-items-or-less" line to ensure that shoppers with only a few things to purchase don't get stuck behind the family preparing for some upcoming nuclear winter.*

---

### 7.4 Shortest Job First (SJF)

**Definition:**  
Shortest Job First (SJF) runs the shortest job first, then the next shortest, and so on.

**Example with Different Job Lengths:**  
Three jobs (A, B, C), where A runs for 100 seconds while B and C run for 10 each.

```
Time: 0 20 40 60 80 100 120
      BC A
```

*Figure 7.3: SJF Simple Example*

- B finishes at 10
- C finishes at 20
- A finishes at 120

Average turnaround time = (10 + 20 + 120) / 3 = 50 seconds

*This is more than a factor of two improvement over FIFO (110 seconds).*

**Note:**  
*Given our assumptions about jobs all arriving at the same time, we could prove that SJF is indeed an optimal scheduling algorithm. However, you are in a systems class, not theory or operations research; no proofs are allowed.*

---

### 7.5 Shortest Time-to-Completion First (STCF)

**Definition:**  
STCF (also known as Preemptive Shortest Job First or PSJF [CK68]) adds preemption to SJF.

**How STCF Works:**  
Any time a new job enters the system, the STCF scheduler determines which of the remaining jobs (including the new job) has the least time left, and schedules that one.

**Example with Late Arrivals:**  
Assume A arrives at t=0 and needs to run for 100 seconds, whereas B and C arrive at t=10 and each need to run for 10 seconds.

*With pure SJF (non-preemptive):*

```
Time: 0 20 40 60 80 100 120
      A BC[B,C arrive]
```

*Figure 7.4: SJF With Late Arrivals From B and C*

- A finishes at 100
- B finishes at 110
- C finishes at 120

Average turnaround time = (100 + (110 - 10) + (120 - 10)) / 3 = 103.33 seconds

*With STCF (preemptive):*

```
Time: 0 20 40 60 80 100 120
      ABC A[B,C arrive]
```

*Figure 7.5: STCF Simple Example*

- A finishes at 120
- B finishes at 20
- C finishes at 30

Average turnaround time = ((120 - 0) + (20 - 10) + (30 - 10)) / 3 = 50 seconds

**Note:**  
*Given our new assumptions, STCF is provably optimal; given that SJF is optimal if all jobs arrive at the same time, you should probably be able to see the intuition behind the optimality of STCF.*

---

### 7.6 A New Metric: Response Time

**Definition:**  
Response time is defined as the time from when the job arrives in a system to the first time it is scheduled.

```
Tresponse = Tfirstrun - Tarrival (7.2)
```

*Some define it slightly differently, e.g., to also include the time until the job produces some kind of "response"; our definition is the best-case version of this, essentially assuming that the job produces a response instantaneously.*

**Example with STCF Schedule:**  
With A arriving at t=0, and B and C at t=10:

- Response time for A = 0
- Response time for B = 0
- Response time for C = 10

Average response time = (0 + 0 + 10) / 3 = 3.33 seconds

**Why STCF is Bad for Response Time:**  
*If three jobs arrive at the same time, for example, the third job has to wait for the previous two jobs to run in their entirety before being scheduled just once. While great for turnaround time, this approach is quite bad for response time and interactivity. Indeed, imagine sitting at a terminal, typing, and having to wait 10 seconds to see a response from the system just because some other job got scheduled in front of yours: not too pleasant.*

---

### 7.7 Round Robin

**Definition:**  
Round Robin (RR) scheduling runs a job for a time slice (sometimes called a scheduling quantum) and then switches to the next job in the run queue. It repeatedly does so until the jobs are finished. For this reason, RR is sometimes called time-slicing.

**Time Slice Requirements:**  
The length of a time slice must be a multiple of the timer-interrupt period; thus if the timer interrupts every 10 milliseconds, the time slice could be 10, 20, or any other multiple of 10 ms.

**Example with Equal Job Lengths:**  
Assume three jobs A, B, and C arrive at the same time in the system, and that they each wish to run for 5 seconds.

*With SJF (runs to completion):*

```
Time: 0 5 10 15 20 25 30
      ABC
```

*Figure 7.6: SJF Again (Bad for Response Time)*

- A response time = 0
- B response time = 5
- C response time = 10
- Average response time = (0 + 5 + 10) / 3 = 5

*With RR and 1-second time slice:*

```
Time: 0 5 10 15 20 25 30
      ABCABCABCABCABC
```

*Figure 7.7: Round Robin (Good For Response Time)*

- A response time = 0
- B response time = 1
- C response time = 2
- Average response time = (0 + 1 + 2) / 3 = 1

**Trade-off with Time Slice Length:**  
- Shorter time slice = better performance under response-time metric
- Too short = context switching cost dominates overall performance

**TIP: AMORTIZATION CAN REDUCE COSTS**  
*The general technique of amortization is commonly used in systems when there is a fixed cost to some operation. By incurring that cost less often (i.e., by performing the operation fewer times), the total cost to the system is reduced. For example, if the time slice is set to 10 ms, and the context-switch cost is 1 ms, roughly 10% of time is spent context switching and is thus wasted. If we want to amortize this cost, we can increase the time slice, e.g., to 100 ms. In this case, less than 1% of time is spent context switching, and thus the cost of time-slicing has been amortized.*

**Context Switching Cost:**  
*The cost of context switching does not arise solely from the OS actions of saving and restoring a few registers. When programs run, they build up a great deal of state in CPU caches, TLBs, branch predictors, and other on-chip hardware. Switching to another job causes this state to be flushed and new state relevant to the currently-running job to be brought in, which may exact a noticeable performance cost [MB91].*

**RR Performance on Turnaround Time:**  
*RR, with a reasonable time slice, is thus an excellent scheduler if response time is our only metric. But what about our old friend turnaround time? Let's look at our example above again. A, B, and C, each with running times of 5 seconds, arrive at the same time, and RR is the scheduler with a (long) 1-second time slice. We can see from the picture above that A finishes at 13, B at 14, and C at 15, for an average of 14. Pretty awful!*

*It is not surprising, then, that RR is indeed one of the worst policies if turnaround time is our metric. Intuitively, this should make sense: what RR is doing is stretching out each job as long as it can, by only running each job for a short bit before moving to the next. Because turnaround time only cares about when jobs finish, RR is nearly pessimal, even worse than simple FIFO in many cases.*

**Trade-off Between Fairness and Metrics:**  
*More generally, any policy (such as RR) that is fair, i.e., that evenly divides the CPU among active processes on a small time scale, will perform poorly on metrics such as turnaround time. Indeed, this is an inherent trade-off: if you are willing to be unfair, you can run shorter jobs to completion, but at the cost of response time; if you instead value fairness, response time is lowered, but at the cost of turnaround time.*

**TIP: OVERLAP ENABLES HIGHER UTILIZATION**  
*When possible, overlap operations to maximize the utilization of systems. Overlap is useful in many different domains, including when performing disk I/O or sending messages to remote machines; in either case, starting the operation and then switching to other work is a good idea, and improves the overall utilization and efficiency of the system.*

---

### 7.8 Incorporating I/O

**Relaxing Assumption 4 (Jobs Perform No I/O):**  
*First we will relax assumption 4 â€” of course all programs perform I/O. Imagine a program that didn't take any input: it would produce the same output each time. Imagine one without output: it is the proverbial tree falling in the forest, with no one to see it; it doesn't matter that it ran.*

**Scheduler Decision When Job Initiates I/O:**  
When a job initiates an I/O request, the currently-running job won't be using the CPU during the I/O (it's blocked waiting for I/O completion). If the I/O is sent to a hard disk drive, the process might be blocked for a few milliseconds or longer. Thus, the scheduler should probably schedule another job on the CPU at that time.

**Scheduler Decision When I/O Completes:**  
When I/O completes, an interrupt is raised, and the OS runs and moves the process that issued the I/O from blocked back to the ready state. The OS could even decide to run the job at that point.

**Example with Two Jobs (A and B):**  
- A: runs for 10 ms, then issues I/O (I/O takes 10 ms), repeats (5 times)
- B: uses CPU for 50 ms, performs no I/O

*Poor Resource Use (Figure 7.8):*

```
Time: 0 20 40 60 80 100 120 140
      A A A A A B BBBB
CPU
Disk
```

*Figure 7.8: Poor Use Of Resources*

*Better Resource Use (Figure 7.9):*

```
Time: 0 20 40 60 80 100 120 140
      A A A A A B B B B B
CPU
Disk
```

*Figure 7.9: Overlap Allows Better Use Of Resources*

**How STCF Accounts for I/O:**  
*The scheduler treats each 10-ms sub-job of A as an independent job. Thus, when the system starts, its choice is whether to schedule a 10-ms A or a 50-ms B. With STCF, the choice is clear: choose the shorter one, in this case A. Then, when the first sub-job of A has completed, only B is left, and it begins running. Then a new sub-job of A is submitted, and it preempts B and runs for 10 ms. Doing so allows for overlap, with the CPU being used by one process while waiting for the I/O of another process to complete; the system is thus better utilized.*

**Key Insight:**  
*By treating each CPU burst as a job, the scheduler makes sure processes that are "interactive" get run frequently. While those interactive jobs are performing I/O, other CPU-intensive jobs run, thus better utilizing the processor.*

---

### 7.9 No More Oracle

**Relaxing Assumption 5 (Job Lengths Known):**  
*With a basic approach to I/O in place, we come to our final assumption: that the scheduler knows the length of each job. As we said before, this is likely the worst assumption we could make. In fact, in a general-purpose OS (like the ones we care about), the OS usually knows very little about the length of each job.*

**The Challenge:**  
*How can we build an approach that behaves like SJF/STCF without such a priori knowledge? Further, how can we incorporate some of the ideas we have seen with the RR scheduler so that response time is also quite good?*

---

### 7.10 Summary

*We have introduced the basic ideas behind scheduling and developed two families of approaches. The first type (SJF, STCF) optimizes turnaround time, but is bad for response time. The second type (RR) optimizes response time but is bad for turnaround. Both are bad where the other is good, alas, an inherent trade-off common in systems. We have also seen how we might incorporate I/O into the picture, but have still not solved the problem of the fundamental inability of the OS to see into the future. Shortly, we will see how to overcome this problem, by building a scheduler that uses the recent past to predict the future. This scheduler is known as the multi-level feedback queue, and it is the topic of the next chapter.*

---

### Key Terms and Definitions

| Term | Definition |
|------|------------|
| **Scheduling** | High-level policy determining the order in which processes are executed |
| **Workload** | Simplifying assumptions about processes running in the system |
| **Turnaround Time** | Time at which job completes minus time at which job arrived: Tturnaround = Tcompletion - Tarrival |
| **Response Time** | Time from when job arrives to first time it is scheduled: Tresponse = Tfirstrun - Tarrival |
| **FIFO (First In, First Out)** | Scheduling algorithm where jobs are run in the order they arrive |
| **SJF (Shortest Job First)** | Scheduling algorithm that runs the shortest job first, then next shortest |
| **STCF (Shortest Time-to-Completion First)** | Preemptive version of SJF that runs the job with least remaining time |
| **RR (Round Robin)** | Scheduling algorithm that runs each job for a fixed time slice before switching |
| **Time Slice / Quantum** | Fixed time period a job runs before being preempted in RR scheduling |
| **Convoy Effect** | Situation where short jobs wait behind a long job, causing poor performance |
| **Preemptive Scheduler** | Scheduler that can stop a running process to run another process |
| **Non-preemptive Scheduler** | Scheduler that runs each job to completion before considering new jobs |

---

### Important Figures and Diagrams

*Figure 7.1: FIFO Simple Example*  
- Shows three jobs (A, B, C) each running for 10 seconds
- A finishes at 10, B at 20, C at 30
- Average turnaround time = 20

*Figure 7.2: Why FIFO Is Not That Great*  
- Shows A (100s), B and C (10s each)
- A finishes at 100, B at 110, C at 120
- Average turnaround time = 110

*Figure 7.3: SJF Simple Example*  
- Shows B and C running before A
- B finishes at 10, C at 20, A at 120
- Average turnaround time = 50

*Figure 7.4: SJF With Late Arrivals From B and C*  
- Shows A running to completion before B and C (non-preemptive SJF)
- A finishes at 100, B at 110, C at 120
- Average turnaround time = 103.33

*Figure 7.5: STCF Simple Example*  
- Shows STCF preempting A to run B and C
- A finishes at 120, B at 20, C at 30
- Average turnaround time = 50

*Figure 7.6: SJF Again (Bad for Response Time)*  
- Shows SJF running jobs to completion
- A response time = 0, B = 5, C = 10
- Average response time = 5

*Figure 7.7: Round Robin (Good For Response Time)*  
- Shows RR with 1-second time slice
- A response time = 0, B = 1, C = 2
- Average response time = 1

*Figure 7.8: Poor Use Of Resources*  
- Shows poor resource utilization with A's I/O time wasted
- A runs for 10ms, does I/O (10ms), repeats 5 times
- B runs for 50ms without I/O

*Figure 7.9: Overlap Allows Better Use Of Resources*  
- Shows better resource utilization with overlap
- CPU used while A is doing I/O
- Better overall system utilization

---

### References

| Reference | Description |
|-----------|-------------|
| [B+79] | "The Convoy Phenomenon" by M. Blasgen, J. Gray, M. Mitoma, T. Price. ACM Operating Systems Review, 13:2, April 1979. First reference to convoys in databases and OS. |
| [C54] | "Priority Assignment in Waiting Line Problems" by A. Cobham. Journal of Operations Research, 2:70, pages 70â€“76, 1954. Pioneering paper on using SJF approach in scheduling machine repair. |
| [K64] | "Analysis of a Time-Shared Processor" by Leonard Kleinrock. Naval Research Logistics Quarterly, 11:1, pages 59â€“73, March 1964. First reference to round-robin scheduling algorithm. |
| [CK68] | "Computer Scheduling Methods and their Countermeasures" by Edward G. Coffman and Leonard Kleinrock. AFIPS '68 (Spring), April 1968. Early introduction to basic scheduling disciplines. |
| [J91] | "The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling" by R. Jain. Interscience, New York, April 1991. Standard text on computer systems measurement. |
| [O45] | "Animal Farm" by George Orwell. Secker and Warburg (London), 1945. Allegorical book about power and corruption. |
| [PV56] | "Machine Repair as a Priority Waiting-Line Problem" by Thomas E. Phipps Jr., W. R. Van Voorhis. Operations Research, 4:1, pages 76â€“86, February 1956. Follow-on work generalizing SJF approach to machine repair. |
| [MB91] | "The effect of context switches on cache performance" by Jeffrey C. Mogul, Anita Borg. ASPLOS, 1991. Study on how cache performance is affected by context switching. |
| [W15] | "You can't have your cake and eat it" by Authors: Unknown. Wikipedia (as of December 2015). Discusses similar idioms in other languages. |

---

### Homework (Simulation)

**Program:** `scheduler.py`  
Allows you to see how different schedulers perform under scheduling metrics such as response time, turnaround time, and total wait time.

**Questions:**

1. Compute the response time and turnaround time when running three jobs of length 200 with the SJF and FIFO schedulers.
2. Now do the same but with jobs of different lengths: 100, 200, and 300.
3. Now do the same, but also with the RR scheduler and a time-slice of 1.
4. For what types of workloads does SJF deliver the same turnaround times as FIFO?
5. For what types of workloads and quantum lengths does SJF deliver the same response times as RR?
6. What happens to response time with SJF as job lengths increase? Can you use the simulator to demonstrate the trend?
7. What happens to response time with RR as quantum lengths increase? Can you write an equation that gives the worst-case response time, given N jobs?

---

### Review Questions

1. What are the key assumptions made about the workload in this chapter?
2. Define turnaround time and explain how it's calculated.
3. Explain the convoy effect and provide an example.
4. How does SJF differ from FIFO? What is the advantage of SJF?
5. What is STCF? How does it differ from SJF?
6. Define response time and explain why it's important for interactive systems.
7. Describe how Round Robin scheduling works and explain the trade-off with time slice length.
8. Why is RR bad for turnaround time but good for response time?
9. How should a scheduler handle I/O operations? What is the benefit of overlapping CPU and I/O?
10. Why is it unrealistic to assume the OS knows the length of each job in advance?
11. Explain the trade-off between fairness and performance metrics in scheduling.
12. What is the difference between preemptive and non-preemptive scheduling?


---

# Chapter 9: Scheduling - Proportional Share

<think>
I need to create comprehensive study notes from the textbook chapter on "CPU Scheduling: Lottery" based on the provided slides. The instructions specify that I must include ALL information without summarizing, preserving every detail, example, and explanation.

Let me go through each slide and extract all the information in a comprehensive manner, following the required format.

Starting with Slide 1:

This slide introduces proportional-share scheduling, also known as fair-share scheduling. It explains that instead of optimizing for turnaround or response time, a scheduler might try to guarantee that each job obtains a certain percentage of CPU time.

It mentions lottery scheduling as an example, based on research by Waldspurger and Weihl [WW94], with the basic idea being to hold a lottery to determine which process should run next, with processes getting more chances to win if they should run more often.

The CRUX (core question) is: "HOW TO SHARE THE CPU PROPORTIONALLY?"

Then it moves to Section 9.1: Basic Concept: Tickets Represent Your Share

It explains that tickets are used to represent the share of a resource a process should receive. The percent of tickets a process has represents its share of the system resource.

Example: Two processes, A and B, with A having 75 tickets and B having 25 tickets. They want A to receive 75% of the CPU and B 25%.

Lottery scheduling achieves this probabilistically by holding a lottery every so often (e.g., every time slice). The scheduler knows the total number of tickets (100 in this example) and picks a winning ticket (a number from 0 to 99). If A holds tickets 0-74 and B holds 75-99, the winning ticket determines which process runs.

It provides an example output of winning tickets: "63 85 70 39 76 17 29 41 36 39 10 99 68 83 63 62 43 0 49 12" and the resulting schedule: "A A A A A A A A A A A A A A A A B B B B"

It notes that lottery scheduling leads to probabilistic correctness (not deterministic) - in the example, B got 4 out of 20 time slices (20%) instead of the desired 25%, but with longer competition, they're more likely to achieve the desired percentages.

Slide 2 continues with a tip about using randomness:

It explains three advantages of random approaches:
1. Avoids strange corner-case behaviors that traditional algorithms may have trouble handling
2. Is lightweight, requiring little state to track alternatives (vs. traditional fair-share scheduling which requires per-process accounting)
3. Can be quite fast (as long as generating a random number is quick)

It also provides a tip about using tickets to represent shares, noting that tickets can be applied broadly (e.g., for memory in virtual memory management).

Slide 3 covers Ticket Mechanisms:

It discusses three mechanisms:
1. Ticket currency: Allows a user with a set of tickets to allocate tickets among their own jobs in whatever currency they'd like; the system converts to global value.
   - Example: User A has 100 tickets, gives 500 each to A1 and A2 (in A's currency), which converts to 50 each in global currency. User B has 100 tickets, gives 10 to B1 (in B's currency), which converts to 100 in global currency. Total global tickets = 200.

2. Ticket transfer: A process can temporarily hand off its tickets to another process.
   - Useful in client/server settings: client passes tickets to server to maximize performance, then gets them back.

3. Ticket inflation: A process can temporarily raise or lower its number of tickets.
   - Useful in trusted environments (not competitive), where a process can boost its ticket value to reflect need without communicating with others.

Slide 4 provides implementation details:

It shows code for lottery scheduling decision (Figure 9.1):

```
// counter: used to track if we've found the winner yet
int counter = 0;

// winner: call some random number generator to
// get a value >= 0 and <= (totaltickets - 1)
int winner = getrandom(0, totaltickets);

// current: use this to walk through the list of jobs
node_t*current = head;
while (current) {
    counter = counter + current->tickets;
    if (counter > winner)
        break; // found the winner
    current = current->next;
}
// 'current' is the winner: schedule it...
```

It explains the example with three processes (A: 100 tickets, B: 50 tickets, C: 250 tickets) and total tickets = 400. If the winning ticket is 300:
- Counter starts at 0
- A adds 100 (counter=100 < 300)
- B adds 50 (counter=150 < 300)
- C adds 250 (counter=400 > 300) -> break, C is winner

It notes that organizing the list in sorted order (highest to lowest tickets) can make it more efficient, as it minimizes the number of list iterations.

Slide 5 covers an example with a fairness study:

It defines a fairness metric F = time first job completes / time second job completes. A perfectly fair scheduler would achieve F=1.

Figure 9.2 shows a graph plotting average fairness as job length (R) varies from 1 to 1000 over thirty trials. It notes that when job length is short, average fairness can be low; only as jobs run for significant time slices does the lottery scheduler approach the desired fair outcome.

Slide 6 introduces Stride Scheduling:

It explains that stride scheduling is a deterministic fair-share scheduler invented by Waldspurger [W95], addressing the issue that lottery scheduling doesn't deliver exact proportions over short time scales.

Stride scheduling:
- Each job has a stride, inverse in proportion to the number of tickets it has.
- Stride = large number / number of tickets (e.g., 10,000 / ticket value)
- Example: A (100 tickets) -> stride=100, B (50 tickets) -> stride=200, C (250 tickets) -> stride=40
- Each job has a pass value (initially 0), incremented by stride each time it runs
- Scheduler picks process with lowest pass value to run next

Pseudocode:
```
curr = remove_min(queue); // pick client with min pass
schedule(curr); // run for quantum
curr->pass += curr->stride; // update pass using stride
insert(queue, curr); // return curr to queue
```

It provides a trace (Figure 9.3) showing how the scheduler behaves over time with A, B, C having strides 100, 200, 40.

It notes that stride scheduling gets proportions exactly right at the end of each scheduling cycle, while lottery scheduling achieves proportions probabilistically over time.

It also explains why lottery scheduling is still used despite stride scheduling's precision: lottery has no global state per process, making it easier to incorporate new processes.

Slide 7 covers the Linux Completely Fair Scheduler (CFS):

It explains that CFS implements fair-share scheduling in a highly efficient and scalable manner.

Basic Operation:
- Goal: fairly divide CPU evenly among all competing processes
- Uses virtual runtime (vruntime) - each process accumulates vruntime as it runs
- When scheduling decision occurs, picks process with lowest vruntime to run next

It discusses the tension between fairness and performance:
- Switch too often: increased fairness but decreased performance (too much context switching)
- Switch less often: increased performance but decreased near-term fairness

CFS manages this with two parameters:
1. sched_latency: determines how long one process should run before considering a switch (typically 48 ms)
   - For n processes, time slice = sched_latency / n
   - Example: 4 processes -> 12 ms per process

2. min_granularity: ensures time slice isn't too small (typically 6 ms)
   - Example: 10 processes -> would be 4.8 ms without min_granularity, but set to 6 ms instead

It explains that CFS uses a periodic timer interrupt (e.g., every 1 ms), and tracks vruntime precisely.

Weighting (Nice):
- CFS uses nice level (from -20 to +19) to control priority
- Maps nice value to weight using a table:
```
static const int prio_to_weight[40] = {
/*-20*/ 88761, 71755, 56483, 46273, 36291,
/*-15*/ 29154, 23254, 18705, 14949, 11916,
/*-10*/ 9548, 7620, 6100, 4904, 3906,
/*-5*/ 3121, 2501, 1991, 1586, 1277,
/*0*/ 1024, 820, 655, 526, 423,
/*5*/ 335, 272, 215, 172, 137,
/*10*/ 110, 87, 70, 56, 45,
/*15*/ 36, 29, 23, 18, 15,
};
```

Time slice formula:
timeslice_k = (weight_k / sum_{i=0}^{n-1} weight_i) * sched_latency

Example: 
- A has nice value -5 (weight=3121), B has nice value 0 (weight=1024)
- A's time slice â‰ˆ 3/4 of sched_latency (36 ms), B's â‰ˆ 1/4 (12 ms)

vruntime formula:
vruntime_i = vruntime_i + (weight_0 / weight_i) * runtime_i

It explains that the table preserves CPU proportionality ratios when the difference in nice values is constant.

Slide 8 discusses using red-black trees for efficiency:

- CFS keeps processes in a red-black tree (instead of a simple list)
- Red-black trees maintain low depth, ensuring O(log n) operations instead of O(n) for lists
- Only running (runnable) processes are kept in the tree
- Processes that go to sleep are removed from the tree

It provides an example with vruntime values: 1, 5, 9, 10, 14, 18, 17, 21, 22, 24
- In a list, finding the next job is O(1) but insertion is O(n)
- In a red-black tree, both operations are O(log n)

Slide 9 covers dealing with I/O and sleeping processes:

- Problem: If a process sleeps for a long time, when it wakes up, its vruntime will be behind, causing it to monopolize CPU while catching up
- CFS handles this by setting the vruntime of a waking process to the minimum value in the tree (to avoid starvation)
- However, this means jobs that sleep for short periods frequently don't get their fair share

Slide 10 covers other CFS features and a tip about efficient data structures:

- CFS has many other features (cache performance heuristics, multi-CPU strategies, etc.)
- Tip: "Use efficient data structures when appropriate" - simple lists don't scale for thousands of processes

Slide 11 is a summary of the chapter:

- Introduces proportional-share scheduling with three approaches: lottery, stride, and CFS
- Lottery uses randomness to achieve proportional share; stride does so deterministically
- CFS is a weighted round-robin with dynamic time slices, built to scale and perform well under load
- Problems with fair-share schedulers: don't mesh well with I/O, ticket/priority assignment is hard
- Proportional-share schedulers are effective in domains like virtualized data centers

Slide 12 has references:
- [AC97] Extending Proportional-Share Scheduling to a Network of Workstations
- [B+18] The Battle of the Schedulers: FreeBSD ULE vs. Linux CFS
- [B72] Symmetric binary B-Trees
- [D82] Why Numbering Should Start At Zero
- [K+15] Profiling A Warehouse-scale Computer
- [J09] Inside The Linux 2.6 Completely Fair Scheduler
- [KL88] A Fair Share Scheduler
- [WW94] Lottery Scheduling: Flexible Proportional-Share Resource Management
- [W95] Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management
- [W02] Memory Resource Management in VMware ESX Server

Slide 13 has homework/simulation details:
- A program called lottery.py allows seeing how a lottery scheduler works
- Questions about simulations with different parameters

Now I'll create comprehensive study notes based on all this information, following the required format.
</think>


### Learning Objectives
- Understand proportional-share scheduling concepts
- Learn about lottery scheduling and its implementation
- Explore ticket mechanisms (currency, transfer, inflation)
- Analyze stride scheduling as a deterministic alternative
- Study the Linux Completely Fair Scheduler (CFS) in detail
- Compare different fair-share scheduling approaches

### Core Concept: Proportional-Share Scheduling

**Definition**: Proportional-share scheduling is a type of scheduler that guarantees each job a certain percentage of CPU time, rather than optimizing for turnaround or response time.

**CRUX (Core Question)**: *HOW TO SHARE THE CPU PROPORTIONALLY?*

**Key Mechanism**: Instead of focusing on traditional metrics like turnaround time or response time, proportional-share scheduling ensures that each process receives a specific proportion of the CPU resources based on its assigned share.

**Historical Context**: 
- Early example: Waldspurger and Weihl [WW94] research
- Known as **lottery scheduling**
- The idea is older [KL88]

### 9.1 Basic Concept: Tickets Represent Your Share

**Definition**: Tickets are used to represent the share of a resource (like CPU time) that a process should receive. The percentage of tickets a process has represents its share of the system resource.

**Example**: 
- Two processes, A and B
- A has 75 tickets, B has 25 tickets
- Desired allocation: A gets 75% of CPU, B gets 25%

**Implementation**:
1. Total tickets = 100 (75 + 25)
2. Scheduler picks a winning ticket (random number from 0 to 99)
3. A holds tickets 0-74, B holds tickets 75-99
4. Winning ticket determines which process runs

**Example Output**:
- Winning tickets: `63 85 70 39 76 17 29 41 36 39 10 99 68 83 63 62 43 0 49 12`
- Resulting schedule: `A A A A A A A A A A A A A A A A B B B B`

**Key Insight**: 
- Lottery scheduling achieves proportional sharing **probabilistically**, not deterministically
- In the example, B received 4 out of 20 time slices (20%) instead of the desired 25%
- The longer the jobs compete, the more likely they are to achieve the desired percentages

### 9.2 Ticket Mechanisms

#### Ticket Currency
**Definition**: A mechanism allowing a user with a set of tickets to allocate tickets among their own jobs in whatever currency they prefer; the system automatically converts this to global value.

**Example**:
- User A has 100 tickets
- User A runs two jobs (A1, A2) and gives each 500 tickets (in A's currency)
- User B has 100 tickets
- User B runs one job (B1) and gives it 10 tickets (in B's currency)

**Conversion**:
- A1: 500 (A's currency) â†’ 50 (global currency)
- A2: 500 (A's currency) â†’ 50 (global currency)
- B1: 10 (B's currency) â†’ 100 (global currency)

**Global Ticket Total**: 200 (50 + 50 + 100)

#### Ticket Transfer
**Definition**: A process can temporarily hand off its tickets to another process.

**Use Case**: Client/server setting
- Client process sends message to server asking it to do work on client's behalf
- Client passes tickets to server to maximize server performance while handling request
- Server transfers tickets back to client when finished

#### Ticket Inflation
**Definition**: A process can temporarily raise or lower the number of tickets it owns.

**Use Case**: 
- In trusted environments (not competitive scenarios)
- A process that needs more CPU time can boost its ticket value to reflect that need
- No communication with other processes required

**Important Note**: 
- In competitive scenarios with untrusted processes, inflation makes little sense (a greedy process could take over the machine)
- In trusted environments (e.g., group of processes working together), inflation can be useful

### 9.3 Implementation

**Key Insight**: Lottery scheduling is remarkably simple to implement.

**Required Components**:
1. Good random number generator
2. Data structure to track processes (e.g., list)
3. Total number of tickets

**Example Implementation** (Figure 9.1):

```c
// counter: used to track if we've found the winner yet
int counter = 0;

// winner: call some random number generator to
// get a value >= 0 and <= (totaltickets - 1)
int winner = getrandom(0, totaltickets);

// current: use this to walk through the list of jobs
node_t*current = head;
while (current) {
    counter = counter + current->tickets;
    if (counter > winner)
        break; // found the winner
    current = current->next;
}
// 'current' is the winner: schedule it...
```

**Example with Three Processes**:
- Process A: 100 tickets
- Process B: 50 tickets
- Process C: 250 tickets
- Total tickets: 400

**Example Execution** (winning ticket = 300):
1. counter = 0 + 100 = 100 (A's tickets) â†’ 100 < 300 â†’ continue
2. counter = 100 + 50 = 150 (B's tickets) â†’ 150 < 300 â†’ continue
3. counter = 150 + 250 = 400 (C's tickets) â†’ 400 > 300 â†’ break
4. Current points to C (winner)

**Optimization Tip**: 
- Organize the list in sorted order (highest to lowest tickets)
- This minimizes the number of list iterations, especially when a few processes have most tickets
- Sorting does not affect correctness but improves efficiency

### 9.4 An Example: Fairness Study

**Fairness Metric (F)**:
- Defined as time first job completes divided by time second job completes
- F = 1 indicates perfect fairness (both jobs finish at same time)

**Example Calculation**:
- If R = 10, first job finishes at time 10, second job at time 20
- F = 10/20 = 0.5

**Figure 9.2**: Lottery Fairness Study
- Plots average fairness as job length (R) varies from 1 to 1000 over thirty trials
- When job length is short, average fairness can be low
- As jobs run for significant time slices, lottery scheduler approaches desired fair outcome

**Key Insight**: 
- Lottery scheduling requires longer job execution times to achieve near-perfect fairness
- Short jobs exhibit more variance in completion times

### 9.5 How To Assign Tickets?

**Core Problem**: The system behavior strongly depends on how tickets are allocated.

**Approach Considered (and Rejected)**:
- Assume users know best: each user is handed some number of tickets, can allocate to any jobs they run
- This solution doesn't provide guidance on what to do (non-solution)

**Open Problem**: 
- Given a set of jobs, the "ticket-assignment problem" remains open
- How to assign tickets to achieve desired behavior

### 9.6 Stride Scheduling

**Definition**: A deterministic fair-share scheduler invented by Waldspurger [W95], addressing the issue that lottery scheduling doesn't deliver exact proportions over short time scales.

**Key Concept**:
- Each job has a stride, which is the inverse of the number of tickets it has
- Stride = large number / number of tickets (e.g., 10,000 / ticket value)

**Example Calculation**:
- Process A: 100 tickets â†’ stride = 10,000 / 100 = 100
- Process B: 50 tickets â†’ stride = 10,000 / 50 = 200
- Process C: 250 tickets â†’ stride = 10,000 / 250 = 40

**Scheduler Operation**:
- Each process has a pass value (initially 0)
- When a process runs, its pass value is incremented by its stride
- Scheduler picks process with lowest pass value to run next

**Pseudocode**:
```
curr = remove_min(queue); // pick client with min pass
schedule(curr); // run for quantum
curr->pass += curr->stride; // update pass using stride
insert(queue, curr); // return curr to queue
```

**Example Trace** (Figure 9.3):
| Pass(A) (stride=100) | Pass(B) (stride=200) | Pass(C) (stride=40) | Who Runs? |
|----------------------|----------------------|---------------------|-----------|
| 0                    | 0                    | 0                   | A         |
| 100                  | 0                    | 0                   | B         |
| 100                  | 200                  | 0                   | C         |
| 100                  | 200                  | 40                  | C         |
| 100                  | 200                  | 80                  | C         |
| 100                  | 200                  | 120                 | A         |
| 200                  | 200                  | 120                 | C         |
| 200                  | 200                  | 160                 | C         |
| 200                  | 200                  | 200                 | ...       |

**Key Insight**: 
- C ran 5 times, A twice, B once â†’ exactly proportional to ticket values (250:100:50)
- Lottery scheduling achieves proportions probabilistically over time
- Stride scheduling gets exact proportions at the end of each scheduling cycle

**Why Use Lottery Scheduling Instead of Stride?**
- Lottery scheduling has no global state per process
- When a new job enters, lottery scheduling simply adds it with its tickets, updates total tickets
- With stride scheduling, determining the initial pass value for a new process is problematic (if set to 0, it monopolizes CPU)

### 9.7 The Linux Completely Fair Scheduler (CFS)

**Definition**: A fair-share scheduler implemented in Linux that achieves similar goals to proportional-share scheduling but in an efficient and scalable manner.

**Key Insight**: 
- CFS aims to spend very little time making scheduling decisions
- Recent studies show scheduling uses about 5% of overall datacenter CPU time [K+15]
- Reducing scheduling overhead is a key goal in modern scheduler architecture

#### Basic Operation

**Virtual Runtime (vruntime)**:
- As each process runs, it accumulates vruntime
- In basic case, each process's vruntime increases at the same rate as physical time
- When scheduling decision occurs, CFS picks process with lowest vruntime to run next

**Scheduling Tension**:
- Switch too often: increased fairness but decreased performance (too much context switching)
- Switch less often: increased performance but decreased near-term fairness

**CFS Parameters**:
1. **sched_latency**: Determines how long one process should run before considering a switch (typically 48 ms)
   - For n processes, time slice = sched_latency / n
   - Example: 4 processes â†’ 12 ms per process
2. **min_granularity**: Ensures time slice isn't too small (typically 6 ms)
   - Example: 10 processes would be 4.8 ms without min_granularity, but set to 6 ms

**Example Execution** (Figure 9.4):
- Four jobs (A, B, C, D) each run for two time slices
- C and D complete, leaving two remaining
- Remaining jobs run for 24 ms in round-robin fashion

**How CFS Handles Scheduling Decisions**:
- Uses periodic timer interrupt (e.g., every 1 ms)
- Tracks vruntime precisely
- Over long term, approximates ideal CPU sharing

#### Weighting (Nice)

**Definition**: CFS enables controls over process priority using the classic UNIX "nice" level.

**Nice Value Range**: -20 to +19 (default = 0)
- Positive nice values â†’ lower priority
- Negative nice values â†’ higher priority
- "When you're too nice, you just don't get as much (scheduling) attention"

**Weight Mapping Table**:
```c
static const int prio_to_weight[40] = {
/*-20*/ 88761, 71755, 56483, 46273, 36291,
/*-15*/ 29154, 23254, 18705, 14949, 11916,
/*-10*/ 9548, 7620, 6100, 4904, 3906,
/*-5*/ 3121, 2501, 1991, 1586, 1277,
/*0*/ 1024, 820, 655, 526, 423,
/*5*/ 335, 272, 215, 172, 137,
/*10*/ 110, 87, 70, 56, 45,
/*15*/ 36, 29, 23, 18, 15,
};
```

**Time Slice Formula**:
```
timeslice_k = (weight_k / sum_{i=0}^{n-1} weight_i) * sched_latency
```

**Example Calculation**:
- Process A: nice value -5 â†’ weight = 3121
- Process B: nice value 0 â†’ weight = 1024
- A's time slice â‰ˆ (3121 / (3121 + 1024)) * sched_latency â‰ˆ 3/4 * sched_latency (36 ms)
- B's time slice â‰ˆ (1024 / (3121 + 1024)) * sched_latency â‰ˆ 1/4 * sched_latency (12 ms)

**vruntime Formula**:
```
vruntime_i = vruntime_i + (weight_0 / weight_i) * runtime_i
```

**Key Insight**: 
- The table preserves CPU proportionality ratios when the difference in nice values is constant
- Example: If A had nice value 5 (weight=335) and B had nice value 10 (weight=110), CFS would schedule them in exactly the same manner as before

#### Using Red-Black Trees

**Why Red-Black Trees?**
- Simple lists don't scale for modern systems with thousands of processes
- Searching through a long list every few milliseconds wastes CPU cycles
- Red-black trees maintain low depth, ensuring O(log n) operations instead of O(n)

**CFS Implementation**:
- Keeps only running (runnable) processes in the red-black tree
- Processes that go to sleep (e.g., waiting for I/O) are removed from the tree
- When a process wakes up, it's reinserted into the tree

**Example with vruntime Values**:
- Values: 1, 5, 9, 10, 14, 18, 17, 21, 22, 24
- In a list: finding next job is O(1), but insertion is O(n)
- In red-black tree: both operations are O(log n)

**Figure 9.5**: CFS Red-Black Tree (depicts the tree structure with values ordered by vruntime)

#### Dealing With I/O And Sleeping Processes

**Problem**: 
- If a process sleeps for a long time (e.g., 10 seconds), when it wakes up, its vruntime will be 10 seconds behind others
- Without handling, it would monopolize CPU while catching up, starving other processes

**CFS Solution**:
- When a process wakes up, CFS sets its vruntime to the minimum value found in the tree
- This avoids starvation but has a cost: jobs that sleep for short periods frequently don't get their fair share

**Key Insight**: 
- CFS handles this by altering vruntime of waking process to minimum value in tree
- This is a trade-off between avoiding starvation and ensuring fair sharing

### 9.8 Summary

**Three Approaches to Proportional-Share Scheduling**:
1. **Lottery Scheduling**: Uses randomness to achieve proportional share
   - Simple implementation
   - Probabilistic correctness (not deterministic)
   - No global state per process

2. **Stride Scheduling**: Deterministic fair-share scheduler
   - Achieves exact proportions at end of each scheduling cycle
   - Requires global state (pass values)
   - More complex to handle new processes

3. **Completely Fair Scheduler (CFS)**: Linux implementation
   - Weighted round-robin with dynamic time slices
   - Built to scale and perform well under load
   - Most widely used fair-share scheduler today

**Key Limitations of Fair-Share Schedulers**:
1. **I/O Problems**: Jobs performing I/O occasionally may not get their fair share of CPU [AC97]
2. **Ticket/Priority Assignment**: Hard problem of determining how many tickets or nice values to assign (e.g., how many tickets should a browser be allocated?)

**Effective Use Cases**:
- Virtualized data centers (cloud): Assign specific CPU percentages to VMs
- Example: Assign one-quarter of CPU cycles to Windows VM and rest to base Linux installation
- Can be extended to other resources (e.g., memory in VMWare ESX Server [W02])

### Key Terms and Definitions

| Term | Definition |
|------|------------|
| **Proportional-Share Scheduling** | A scheduling approach that guarantees each job a certain percentage of CPU time, rather than optimizing for turnaround or response time. |
| **Lottery Scheduling** | A proportional-share scheduling algorithm where processes are given tickets representing their share, and a lottery determines which process runs next. |
| **Tickets** | A mechanism to represent a process's share of the CPU (or other resources), where the percentage of tickets a process has represents its share of the resource. |
| **Ticket Currency** | A mechanism allowing a user to allocate tickets among their own jobs in a local currency, which the system converts to global value. |
| **Ticket Transfer** | A mechanism allowing a process to temporarily hand off its tickets to another process. |
| **Ticket Inflation** | A mechanism allowing a process to temporarily raise or lower its number of tickets (useful in trusted environments). |
| **Stride Scheduling** | A deterministic fair-share scheduler where each job has a stride (inverse of tickets), and the scheduler picks the process with lowest pass value. |
| **Virtual Runtime (vruntime)** | A metric used by CFS to track how much CPU time a process has effectively used, scaled by its priority. |
| **sched_latency** | A parameter in CFS determining how long one process should run before considering a switch (typically 48 ms). |
| **min_granularity** | A parameter in CFS ensuring time slice isn't too small (typically 6 ms). |
| **Nice Level** | A UNIX mechanism for controlling process priority, ranging from -20 (highest) to +19 (lowest). |

### Important Formulas

1. **Fairness Metric (F)**:
   ```
   F = (time first job completes) / (time second job completes)
   ```

2. **Stride Calculation**:
   ```
   stride = large_number / number_of_tickets
   ```

3. **Time Slice Calculation (CFS)**:
   ```
   timeslice_k = (weight_k / sum_{i=0}^{n-1} weight_i) * sched_latency
   ```

4. **vruntime Update (CFS)**:
   ```
   vruntime_i = vruntime_i + (weight_0 / weight_i) * runtime_i
   ```

### Review Questions

1. Compute the solutions for simulations with 3 jobs and random seeds of 1, 2, and 3.

2. Now run with two specific jobs: each of length 10, but one (job 0) with 1 ticket and the other (job 1) with 100 (e.g., -l 10:1,10:100). What happens when the number of tickets is so imbalanced? Will job 0 ever run before job 1 completes? How often? In general, what does such a ticket imbalance do to the behavior of lottery scheduling?

3. When running with two jobs of length 100 and equal ticket allocations of 100 (-l 100:100,100:100), how unfair is the scheduler? Run with some different random seeds to determine the (probabilistic) answer; let unfairness be determined by how much earlier one job finishes than the other.

4. How does your answer to the previous question change as the quantum size (-q) gets larger?

5. Can you make a version of the graph that is found in the chapter? What else would be worth exploring? How would the graph look with a stride scheduler?

### References

- [AC97] "Extending Proportional-Share Scheduling to a Network of Workstations" by Andrea C. Arpaci-Dusseau and David E. Culler. PDPTA'97, June 1997.
- [B+18] "The Battle of the Schedulers: FreeBSD ULE vs. Linux CFS" by J. Bouron, S. Chevalley, B. Lepers, W. Zwaenepoel, R. Gouicem, J. Lawall, G. Muller, J. Sopena. USENIX ATC'18, July 2018.
- [B72] "Symmetric binary B-Trees: Data Structure And Maintenance Algorithms" by Rudolf Bayer. Acta Informatica, Volume 1, Number 4, December 1972.
- [D82] "Why Numbering Should Start At Zero" by Edsger Dijkstra, August 1982.
- [K+15] "Profiling A Warehouse-scale Computer" by S. Kanev, P. Ranganathan, J.P. Darago, K. Hazelwood, T. Moseley, G. Wei, D. Brooks. ISCA'15, June 2015.
- [J09] "Inside The Linux 2.6 Completely Fair Scheduler" by M. Tim Jones. December 15, 2009.
- [KL88] "A Fair Share Scheduler" by J. Kay and P. Lauder. CACM, Volume 31 Issue 1, January 1988.
- [WW94] "Lottery Scheduling: Flexible Proportional-Share Resource Management" by Carl A. Waldspurger and William E. Weihl. OSDI'94, November 1994.
- [W95] "Lottery and Stride Scheduling: Flexible Proportional-Share Resource Management" by Carl A. Waldspurger. Ph.D. Thesis, MIT, 1995.
- [W02] "Memory Resource Management in VMware ESX Server" by Carl A. Waldspurger. OSDI'02, Boston, Massachusetts.

### Key Insights from the Chapter

#### Randomness in Lottery Scheduling
> **TIP: USE RANDOMNESS**  
> One of the most beautiful aspects of lottery scheduling is its use of randomness. When you have to make a decision, using such a randomized approach is often a robust and simple way of doing so.

> **Advantages of Random Approaches**:
> 1. Avoids strange corner-case behaviors (e.g., LRU replacement policy has worst-case performance for some cyclic-sequential workloads)
> 2. Lightweight - requires little state to track alternatives (vs. traditional fair-share scheduling which requires per-process accounting)
> 3. Fast - as long as generating a random number is quick, making the decision is also quick

#### Ticket Representation
> **TIP: USE TICKETS TO REPRESENT SHARES**  
> One of the most powerful (and basic) mechanisms in the design of lottery (and stride) scheduling is that of the ticket. The ticket is used to represent a process's share of the CPU in these examples, but can be applied much more broadly.

#### Data Structure Choice
> **TIP: USE EFFICIENT DATA STRUCTURES WHEN APPROPRIATE**  
> In many cases, a list will do. In many cases, it will not. Knowing which data structure to use when is a hallmark of good engineering. In the case discussed herein, simple lists found in earlier schedulers simply do not work well on modern systems, particular in the heavily loaded servers found in datacenters. Such systems contain thousands of active processes; searching through a long list to find the next job to run on each core every few milliseconds would waste precious CPU cycles. A better structure was needed, and CFS provided one by adding an excellent implementation of a red-black tree.

#### Key Trade-Offs
> **Lottery vs. Stride**:
> - Lottery: Simple, probabilistic, no global state
> - Stride: Deterministic, exact proportions, requires global state

> **CFS Design Philosophy**:
> - Balance between fairness and performance
> - Minimize scheduling overhead (5% of datacenter CPU time)
> - Efficient data structures for scalability (red-black trees)


---

# Week 2: CPU Scheduling - Multi-Level Feedback Queue (MLFQ)

<think>
I need to create comprehensive study notes from the textbook chapter on "CPU Scheduling: MLFQ" (Multi-Level Feedback Queue). I must include EVERY detail without summarizing, preserving all information, examples, and explanations.

Let me go through each slide and extract all the information:

Slide 1:
- Introduces MLFQ (Multi-Level Feedback Queue) as a well-known scheduling approach
- First described by Corbato et al. in 1962 in CTSS (Compatible Time-Sharing System)
- Led to Corbato receiving the Turing Award
- MLFQ addresses two problems:
  1. Optimizing turnaround time (running shorter jobs first), but OS doesn't know job length
  2. Making system feel responsive to interactive users (minimizing response time)
- MLFQ tries to learn about processes as they run to make better scheduling decisions
- The key question: "HOW TO SCHEDULE WITHOUT PERFECT KNOWLEDGE?"

Slide 2:
- MLFQ learns from history to predict future behavior (common in OS, hardware branch predictors, caching)
- Basic rules for MLFQ:
  â€¢ Rule 1: If Priority(A) > Priority(B), A runs (B doesn't)
  â€¢ Rule 2: If Priority(A) = Priority(B), A & B run in RR
- MLFQ varies priority based on observed behavior:
  - Interactive jobs (relinquish CPU for input) get high priority
  - CPU-intensive jobs get reduced priority
- Figure 8.1 shows a snapshot of queues with jobs at different priorities

Slide 3:
- Introduces the concept of "allotment" - amount of time a job can spend at a given priority level before priority is reduced
- First attempt at priority adjustment algorithm:
  â€¢ Rule 3: When a job enters, it's placed at highest priority
  â€¢ Rule 4a: If job uses up allotment, priority reduced (moves down one queue)
  â€¢ Rule 4b: If job gives up CPU before allotment is up, stays at same priority (allotment reset)

Slide 4:
- Example 1: Long-running job with time slice of 10ms
  - Enters at highest priority (Q2)
  - After 10ms, moves to Q1
  - After another 10ms, moves to Q0 (lowest priority)
- Example 2: Long-running job A and short interactive job B
  - A is running in lowest priority queue
  - B arrives at time T=100, enters highest queue
  - B completes in two time slices (20ms)
  - A resumes running at low priority
- MLFQ approximates SJF (Shortest Job First) by assuming jobs might be short and giving them high priority

Slide 5:
- Example 3: Interactive job with I/O
  - Interactive job B needs CPU for 1ms before I/O, competing with long-running job A
  - MLFQ keeps B at highest priority because it relinquishes CPU before allotment expires
  - This helps interactive jobs run quickly
- Problems with current MLFQ:
  1. Starvation: too many interactive jobs can consume all CPU time, starving long-running jobs
  2. Gaming: smart users can game scheduler by issuing I/O before allotment expires to stay at high priority
  3. Behavior change: a job changing from CPU-bound to interactive won't be treated properly

Slide 6:
- Attempt #2: Priority Boost
  â€¢ Rule 5: After time period S, move all jobs to topmost queue
  - Solves starvation: jobs guaranteed not to starve
  - Also helps jobs that transition from CPU-bound to interactive
- Example: Long-running job competing with two short interactive jobs
  - Without priority boost (left), long-running job starved
  - With priority boost every 100ms (right), long-running job gets periodic CPU time
- S is called a "voo-doo constant" - hard to set correctly
  - Too high: long-running jobs could starve
  - Too low: interactive jobs may not get proper CPU share

Slide 7:
- Attempt #3: Better Accounting
  - Problem: Rules 4a and 4b allow gaming by relinquishing CPU before allotment expires
  - Solution: Rewrite Rule 4 to track CPU time used at each level, regardless of I/O behavior
  â€¢ Rule 4: Once job uses up its allotment at a given level (regardless of I/O), priority reduced
- Example: Figure 8.5 shows gaming with old rules (left) vs. new rule (right)
  - With old rules, process can dominate CPU by issuing I/O before allotment expires
  - With new rule, process moves down queues regardless of I/O behavior

Slide 8:
- Tuning MLFQ:
  - How many queues? 
  - Time slice per queue?
  - Allotment?
  - Boost frequency (S)?
- Solaris MLFQ (Time-Sharing class):
  - 60 queues
  - Time-slice lengths: 20ms (highest) to hundreds of ms (lowest)
  - Priorities boosted around every 1 second
- Other implementations:
  - FreeBSD uses mathematical formula based on CPU usage
  - Uses decayed usage for priority boost

Slide 9:
- Other features:
  - Some schedulers reserve highest priority levels for OS work
  - User can provide hints (e.g., "nice" command to adjust priority)
- Tip: "USE ADVICE WHERE POSSIBLE" - OS rarely knows best for each process, so user hints can help

Slide 10:
- Summary of refined MLFQ rules:
  â€¢ Rule 1: If Priority(A) > Priority(B), A runs
  â€¢ Rule 2: If Priority(A) = Priority(B), A & B run in RR
  â€¢ Rule 3: New jobs enter at highest priority
  â€¢ Rule 4: Once job uses up allotment at a level, priority reduced
  â€¢ Rule 5: Periodically boost all jobs to topmost queue

Slide 11:
- References:
  - [A+17] Automatic Database Management System Tuning
  - [AD00] Multilevel Feedback Queue Scheduling in Solaris
  - [B86] The Design of the UNIX Operating System
  - [C+62] An Experimental Time-Sharing System
  - [CS97] Inside Windows NT
  - [E95] An Analysis of Decay-Usage Scheduling
  - [LM+89] The Design and Implementation of the 4.3BSD UNIX Operating System
  - [M06] Solaris Internals
  - [O11] John Ousterhout's Home Page
  - [P+95] Informed Prefetching and Caching
  - [Y+18] Principled Schedulability Analysis for Distributed Storage Systems

Slide 12:
- Homework (Simulation):
  - mlfq.py program to simulate MLFQ
  - Questions about running simulations, reproducing examples, configuring for round-robin, gaming the scheduler, etc.

Now I'll create comprehensive study notes following all the instructions. I'll structure it with proper headers, include all details without summarizing, and make it exhaustive for exam preparation.

I'll start with chapter title, learning objectives, key terms, then go through each concept in detail with all examples, figures, and explanations.
</think>


### Chapter Overview

This chapter covers the **Multi-Level Feedback Queue (MLFQ)** scheduling algorithm, one of the most well-known approaches to scheduling in operating systems. The MLFQ scheduler was first described by Corbato et al. in 1962 in a system known as the Compatible Time-Sharing System (CTSS), and this work led to Corbato receiving the Turing Award.

### Core Problem Addressed by MLFQ

MLFQ tries to address two fundamental scheduling problems simultaneously:

1. **Optimizing turnaround time** - which, as discussed in previous notes, is done by running shorter jobs first. However, the OS generally doesn't know how long a job will run for (the exact knowledge required by algorithms like SJF or STCF).

2. **Making the system feel responsive to interactive users** - minimizing response time. Unfortunately, algorithms like Round Robin reduce response time but are terrible for turnaround time.

**THE CRUX: HOW TO SCHEDULE WITHOUT PERFECT KNOWLEDGE?**

How can we design a scheduler that both minimizes response time for interactive jobs while also minimizing turnaround time without priori knowledge of job length?

### Key Principles of MLFQ

#### Learning from History

The multi-level feedback queue is an excellent example of a system that learns from the past to predict the future. Such approaches are common in operating systems (and many other places in Computer Science, including hardware branch predictors and caching algorithms). Such approaches work when jobs have phases of behavior and are thus predictable; however, one must be careful with such techniques, as they can easily be wrong and drive a system to make worse decisions than it would have with no knowledge at all.

### Basic Rules of MLFQ

#### Queue Structure

- MLFQ has a number of distinct queues, each assigned a different priority level.
- At any given time, a job that is ready to run is on a single queue.
- MLFQ uses priorities to decide which job should run at a given time: a job with higher priority (i.e., a job on a higher queue) is chosen to run.
- If more than one job is on a given queue (same priority), round-robin scheduling is used among those jobs.

#### Fundamental Rules

1. **Rule 1**: If Priority(A) > Priority(B), A runs (B doesn't).
2. **Rule 2**: If Priority(A) = Priority(B), A & B run in Round Robin (RR).

The key to MLFQ scheduling lies in how the scheduler sets priorities. Rather than giving a fixed priority to each job, MLFQ varies the priority of a job based on its observed behavior.

- If a job repeatedly relinquishes the CPU while waiting for input from the keyboard, MLFQ will keep its priority high (this is how an interactive process might behave).
- If a job uses the CPU intensively for long periods of time, MLFQ will reduce its priority.

In this way, MLFQ tries to learn about processes as they run, and thus use the history of the job to predict its future behavior.

#### Visual Representation (Figure 8.1)

```
Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8
[Low Priority] [High Priority]
D  C  A  B
```

*Figure 8.1: MLFQ Example*

- Two jobs (A and B) are at the highest priority level.
- Job C is in the middle.
- Job D is at the lowest priority.
- Given current knowledge, the scheduler would alternate between A and B (highest priority), while C and D would never get to run.

### Priority Adjustment Mechanism

#### Key Concept: Allotment

The **allotment** is the amount of time a job can spend at a given priority level before the scheduler reduces its priority. For simplicity, initially, we assume the allotment is equal to a single time slice.

#### Attempt #1: Basic Priority Adjustment Rules

- **Rule 3**: When a job enters the system, it is placed at the highest priority (the topmost queue).
- **Rule 4a**: If a job uses up its allotment while running, its priority is reduced (i.e., it moves down one queue).
- **Rule 4b**: If a job gives up the CPU (for example, by performing an I/O operation) before the allotment is up, it stays at the same priority level (i.e., its allotment is reset).

#### Example 1: A Single Long-Running Job

*Figure 8.2: Long-running Job Over Time*

```
Q2
Q1
Q0
0 50 100 150 200
```

- Time slice = 10 ms
- Job enters at highest priority (Q2)
- After 10 ms (one time slice), scheduler reduces job's priority to Q1
- After another 10 ms, job is lowered to lowest priority (Q0)
- Job remains in Q0

#### Example 2: Along Came A Short Job

*Figure 8.3 (left): Long-running job A and short interactive job B*

- Job A (long-running CPU-intensive) has been running in the lowest priority queue (Q0).
- Job B (short-running interactive) arrives at time T=100.
- B is inserted into the highest queue (Q2).
- B runs for 20 ms (two time slices) and completes.
- A resumes running at low priority.

**Why this works**: MLFQ assumes a job might be short (giving it high priority). If it's short, it runs quickly and completes. If it's not short, it moves down the queues, proving itself to be a long-running batch-like process. In this manner, MLFQ approximates SJF (Shortest Job First).

#### Example 3: What About I/O?

*Figure 8.3 (right): Interactive job with I/O*

- Interactive job B (gray) needs CPU only for 1 ms before performing I/O.
- Long-running batch job A (black) is competing for CPU.
- MLFQ keeps B at highest priority because B relinquishes the CPU before its allotment expires.
- This achieves MLFQ's goal of running interactive jobs quickly.

### Problems with Current MLFQ

#### Problem 1: Starvation

- If there are "too many" interactive jobs in the system, they will combine to consume all CPU time, and thus long-running jobs will never receive any CPU time (they starve).
- We'd like to make some progress on these jobs even in this scenario.

#### Problem 2: Gaming the Scheduler

- A smart user could rewrite their program to game the scheduler.
- **Gaming**: Doing something sneaky to trick the scheduler into giving you more than your fair share of the resource.
- **Attack**: Before the allotment is used, issue an I/O operation (e.g., to a file) and thus relinquish the CPU.
- This allows the job to remain in the same queue, gaining a higher percentage of CPU time.
- When done right (e.g., running for 99% of the allotment before relinquishing the CPU), a job could nearly monopolize the CPU.

#### Problem 3: Changing Behavior Over Time

- A program may change its behavior over time; what was CPU-bound may transition to a phase of interactivity.
- With current approach, such a job would be out of luck and not be treated like other interactive jobs in the system.

#### Security Note on Scheduling

> **TIP: SCHEDULING MUST BE SECURE FROM ATTACK**
> 
> You might think that a scheduling policy, whether inside the OS itself (as discussed herein), or in a broader context (e.g., in a distributed storage system's I/O request handling), is not a security concern, but in increasingly many cases, it is exactly that. Consider the modern data center, in which users from around the world share CPUs, memories, networks, and storage systems; without care in policy design and enforcement, a single user may be able to adversely harm others and gain advantage for itself. Thus, scheduling policy forms an important part of the security of a system, and should be carefully constructed.

### Attempt #2: Priority Boost

#### New Rule

- **Rule 5**: After some time period S, move all the jobs in the system to the topmost queue.

This new rule solves two problems at once:
1. Processes are guaranteed not to starve: by sitting in the top queue, a job will share the CPU with other high-priority jobs in a round-robin fashion, and thus eventually receive service.
2. If a CPU-bound job has become interactive, the scheduler treats it properly once it has received the priority boost.

#### Example: Long-running Job Competing with Interactive Jobs

*Figure 8.4: Without (Left) and With (Right) Priority Boost*

```
Without Boost (Left):
Q2
Q1
Q0
0 50 100 150 200

With Boost (Right):
Q2
Q1
Q0
0 50 100 150 200
```

- Left: No priority boost â†’ long-running job gets starved once short jobs arrive.
- Right: Priority boost every 100 ms â†’ long-running job gets boosted to highest priority periodically, making some progress.

#### The "Voo-Doo Constant" Problem

The addition of time period S leads to the obvious question: what should S be set to?

- John Ousterhout, a well-regarded systems researcher, used to call such values in systems **voo-doo constants**, because they seemed to require some form of black magic to set correctly.
- Unfortunately, S has that flavor:
  - If set too high, long-running jobs could starve.
  - If set too low, interactive jobs may not get a proper share of the CPU.
- As such, it is often left to the system administrator to find the right value â€“ or in the modern world, increasingly, to automatic methods based on machine learning.

### Attempt #3: Better Accounting

#### Problem with Current Rules

The real culprit is Rules 4a and 4b, which let a job retain its priority by relinquishing the CPU before its allotment expires.

#### Solution: Better Accounting of CPU Time

Instead of forgetting how much of its allotment a process used at a given level when it performs I/O, the scheduler should keep track; once a process has used its allotment, it is demoted to the next priority queue. Whether it uses its allotment in one long burst or many small ones should not matter.

#### Revised Rule 4

- **Rule 4**: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).

#### Example: Preventing Gaming

*Figure 8.5: Without (Left) and With (Right) Gaming Tolerance*

```
Without Protection (Left):
Q0 Q1 Q2
0 50 100 150 200 250 300

With Better Accounting (Right):
Q0 Q1 Q2
0 50 100 150 200 250 300
```

- Left: Without protection from gaming, a process can issue I/O before its allotment ends, staying at the same priority level and dominating CPU time.
- Right: With better accounting, regardless of I/O behavior, the process slowly moves down queues and cannot gain an unfair share of the CPU.

#### Ousterhout's Law

> **TIP: AVOID VOO-DOO CONSTANTS (OUSTERHOUT'S LAW)**
> 
> Avoiding voo-doo constants is a good idea whenever possible. Unfortunately, as in the example above, it is often difficult. One could try to make the system learn a good value, but that too is not straightforward.
> 
> The frequent result: a configuration file filled with default parameter values that a seasoned administrator can tweak when something isn't quite working correctly. As you can imagine, these are often left unmodified, and thus we are left to hope that the defaults work well in the field.
> 
> This tip brought to you by our old OS professor, John Ousterhout, and hence we call it Ousterhout's Law.

### Tuning MLFQ and Other Issues

#### Parameterization Questions

- How many queues should there be?
- How big should the time slice be per queue?
- What should the allotment be?
- How often should priority be boosted to avoid starvation and account for changes in behavior?

There are no easy answers to these questions, and thus only some experience with workloads and subsequent tuning of the scheduler will lead to a satisfactory balance.

#### Queue Time Slices

Most MLFQ variants allow for varying time-slice length across different queues:

- **High-priority queues** (for interactive jobs) are usually given short time slices (e.g., 10 or fewer milliseconds).
- **Low-priority queues** (for CPU-bound jobs) are given longer time slices (e.g., 100s of milliseconds).

*Figure 8.6: Lower Priority, Longer Quanta*

```
Q2
Q1
Q0
0 50 100 150 200 250 300
```

- Two jobs run for 20 ms at highest queue (with 10-ms time slice).
- 40 ms in middle queue (20-ms time slice).
- 40 ms time slice at lowest queue.

#### Solaris MLFQ Implementation

- The Time-Sharing scheduling class (TS) is particularly easy to configure.
- It provides a set of tables that determine:
  - How priority of a process is altered throughout its lifetime
  - How long each time slice is
  - How often to boost priority of a job
- Default values:
  - 60 queues
  - Time-slice lengths: 20 milliseconds (highest priority) to a few hundred milliseconds (lowest)
  - Priorities boosted around every 1 second

#### Alternative Implementations

- Some MLFQ schedulers don't use tables or the exact rules described in this chapter; rather they adjust priorities using mathematical formulae.
- **FreeBSD scheduler (version 4.3)** uses a formula to calculate current priority level based on CPU usage, with usage decayed over time.
- See Epema's paper for an excellent overview of such decay-usage algorithms and their properties.

#### Additional Features

- Some schedulers reserve the highest priority levels for operating system work; thus typical user jobs can never obtain the highest levels of priority in the system.
- Some systems allow user advice to help set priorities (e.g., using `nice` command-line utility to increase or decrease priority).

#### User Advice Tip

> **TIP: USE ADVICE WHERE POSSIBLE**
> 
> As the operating system rarely knows what is best for each and every process of the system, it is often useful to provide interfaces to allow users or administrators to provide some hints to the OS. We often call such hints **advice**, as the OS need not necessarily pay attention to it, but rather might take the advice into account in order to make a better decision.
> 
> Such hints are useful in many parts of the OS, including:
> - Scheduler (e.g., with `nice`)
> - Memory manager (e.g., `madvise`)
> - File system (e.g., informed prefetching and caching)

### MLFQ: Summary

#### Refined Rules

The refined set of MLFQ rules, spread throughout the chapter, are:

1. **Rule 1**: If Priority(A) > Priority(B), A runs (B doesn't).
2. **Rule 2**: If Priority(A) = Priority(B), A & B run in round-robin fashion using the time slice (quantum length) of the given queue.
3. **Rule 3**: When a job enters the system, it is placed at the highest priority (the topmost queue).
4. **Rule 4**: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue).
5. **Rule 5**: After some time period S, move all the jobs in the system to the topmost queue.

#### Why MLFQ is Effective

MLFQ is interesting for the following reason: instead of demanding a priori knowledge of the nature of a job, it observes the execution of a job and prioritizes it accordingly. In this way, it manages to achieve the best of both worlds:

- It can deliver excellent overall performance (similar to SJF/STCF) for short-running interactive jobs.
- It is fair and makes progress for long-running CPU-intensive workloads.

#### Systems Using MLFQ

Many systems use a form of MLFQ as their base scheduler, including:
- BSD UNIX derivatives ([LM+89, B86])
- Solaris ([M06])
- Windows NT and subsequent Windows operating systems ([CS97])

### References

- **[A+17]** "Automatic Database Management System Tuning Through Large-scale Machine Learning" by Dana Van Aken, Andrew Pavlo, Geoffrey J. Gordon, Bohan Zhang. SIGMOD '17.
  - Not about CPU scheduling in OS, but an example of automatically tuning parameters via ML techniques.

- **[AD00]** "Multilevel Feedback Queue Scheduling in Solaris" by Andrea Arpaci-Dusseau.
  - Available: http://www.ostep.org/Citations/notes-solaris.pdf
  - Great short notes on Solaris scheduler details.

- **[B86]** "The Design of the UNIX Operating System" by M.J. Bach. Prentice-Hall, 1986.
  - Classic old book on how a real UNIX operating system is built.

- **[C+62]** "An Experimental Time-Sharing System" by F. J. Corbato, M. M. Daggett, R. C. Daley. IFIPS 1962.
  - Source of many first ideas in multi-level feedback scheduling (later went into Multics).

- **[CS97]** "Inside Windows NT" by Helen Custer and David A. Solomon. Microsoft Press, 1997.
  - The NT book for learning about something other than UNIX.

- **[E95]** "An Analysis of Decay-Usage Scheduling in Multiprocessors" by D.H.J. Epema. SIGMETRICS '95.
  - Nice paper on scheduling in mid-1990s, including overview of decay-usage schedulers.

- **[LM+89]** "The Design and Implementation of the 4.3BSD UNIX Operating System" by S.J. Leffler, M.K. McKusick, M.J. Karels, J.S. Quarterman. Addison-Wesley, 1989.
  - OS classic written by four main people behind BSD.

- **[M06]** "Solaris Internals: Solaris 10 and OpenSolaris Kernel Architecture" by Richard McDougall. Prentice-Hall, 2006.
  - Good book about Solaris and how it works.

- **[O11]** "John Ousterhout's Home Page" by John Ousterhout. www.stanford.edu/~ouster/.
  - Home page of famous Professor Ousterhout.

- **[P+95]** "Informed Prefetching and Caching" by R.H. Patterson, G.A. Gibson, E. Ginting, D. Stodolsky, J. Zelenka. SOSP '95.
  - Paper about cool ideas in file systems, including how applications can give OS advice.

- **[Y+18]** "Principled Schedulability Analysis for Distributed Storage Systems using Thread Architecture Models" by Suli Yang, Jing Liu, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau. OSDI '18.
  - Recent work demonstrating difficulty of scheduling I/O requests in distributed storage systems.

### Homework (Simulation)

#### Program: mlfq.py

This program allows you to see how the MLFQ scheduler presented in this chapter behaves. See the README for details.

#### Questions

1. Run a few randomly-generated problems with just two jobs and two queues; compute the MLFQ execution trace for each. Make your life easier by limiting the length of each job and turning off I/Os.

2. How would you run the scheduler to reproduce each of the examples in the chapter?

3. How would you configure the scheduler parameters to behave just like a round-robin scheduler?

4. Craft a workload with two jobs and scheduler parameters so that one job takes advantage of the older Rules 4a and 4b (turned on with the `-S` flag) to game the scheduler and obtain 99% of the CPU over a particular time interval.

5. Given a system with a quantum length of 10 ms in its highest queue, how often would you have to boost jobs back to the highest priority level (with the `-B` flag) in order to guarantee that a single long-running (and potentially-starving) job gets at least 5% of the CPU?

6. One question that arises in scheduling is which end of a queue to add a job that just finished I/O; the `-I` flag changes this behavior for this scheduling simulator. Play around with some workloads and see if you can see the effect of this flag.

### Key Terms and Definitions

| Term | Definition |
|------|------------|
| **Multi-Level Feedback Queue (MLFQ)** | A scheduling algorithm that uses multiple priority queues and adjusts a job's priority based on its observed behavior. |
| **Allotment** | The amount of time a job can spend at a given priority level before the scheduler reduces its priority. |
| **Starvation** | A situation where a job never gets CPU time because higher-priority jobs continuously preempt it. |
| **Gaming the Scheduler** | Doing something sneaky to trick the scheduler into giving you more than your fair share of CPU time. |
| **Voo-Doo Constants** | Parameters in systems that seem to require black magic to set correctly, like the priority boost period S. |
| **Ousterhout's Law** | The principle that avoiding voo-doo constants is a good idea whenever possible. |
| **Priority Boost** | A mechanism where all jobs are moved to the topmost queue periodically to prevent starvation. |
| **Interactive Job** | A job that frequently relinquishes the CPU (e.g., waiting for user input), typically short-running. |
| **CPU-Bound Job** | A job that uses the CPU intensively for long periods, where response time isn't important. |
| **Turnaround Time** | The total time taken to complete a job from submission to completion. |
| **Response Time** | The time taken from when a request is made until the first response is produced. |
| **Round Robin (RR)** | A scheduling algorithm where each job gets a fixed time slice before being preempted. |

### Important Figures and Diagrams

#### Figure 8.1: MLFQ Example
```
Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8
[Low Priority] [High Priority]
D  C  A  B
```
- Shows jobs at different priority levels (A and B highest, D lowest).

#### Figure 8.2: Long-running Job Over Time
```
Q2
Q1
Q0
0 50 100 150 200
```
- Illustrates a long-running job moving from highest to lowest priority queue.

#### Figure 8.3: Along Came An Interactive Job
```
Left (Without I/O):
Q0 Q1 Q2
0 50 100 150 200

Right (With I/O):
Q0 Q1 Q2
0 50 100 150 200
```
- Left: Long-running job A and short interactive job B.
- Right: Interactive job B with I/O behavior.

#### Figure 8.4: Without (Left) and With (Right) Priority Boost
```
Without Boost (Left):
Q2
Q1
Q0
0 50 100 150 200

With Boost (Right):
Q2
Q1
Q0
0 50 100 150 200
```
- Left: Long-running job starved without priority boost.
- Right: Long-running job gets periodic CPU time with priority boost.

#### Figure 8.5: Without (Left) and With (Right) Gaming Tolerance
```
Without Protection (Left):
Q0 Q1 Q2
0 50 100 150 200 250 300

With Better Accounting (Right):
Q0 Q1 Q2
0 50 100 150 200 250 300
```
- Left: Process can game scheduler by issuing I/O before allotment expires.
- Right: Process moves down queues regardless of I/O behavior.

#### Figure 8.6: Lower Priority, Longer Quanta
```
Q2
Q1
Q0
0 50 100 150 200 250 300
```
- Shows different time slices at different priority levels (shorter at high priority, longer at low priority).


---

# Chapter Title

<think>
We are creating exhaustive study notes from the provided textbook chapter slides.
 The chapter is titled "The Abstraction: Address Spaces" (Week 2: Textbook Vm Intro).
 We must include EVERY detail without summarizing.

 Steps:
 1. Start with the chapter title and note that it's from Week 2.
 2. Extract all concepts, definitions, examples, explanations, figures, references, homework, etc.
 3. Organize by the slide structure (Slide 1 to Slide 9) but in a logical flow for study notes.
 4. Use markdown with clear headers, bullet points, code blocks, etc.

 Important: Do not skip any detail. The student must be able to learn everything from these notes.

 Let's break down each slide and extract every piece of information.

 Slide 1:
   - Title: "The Abstraction: Address Spaces"
   - Content: 
        * Early days of computer systems: users didn't expect much, so building systems was easy.
        * The text says: "users with their expectations of 'ease of use', 'high performance', 'reliability', etc., that really have led to all these headaches."
        * Then a humorous note: "Next time you meet one of those computer users, thank them for all the problems they have caused."

   - Section 13.1 Early Systems:
        * From memory perspective, early machines didn't provide much abstraction.
        * Physical memory looked like Figure 13.1 (page 2) [but note: we don't have the figure, so we describe as per text].
        * OS was a set of routines (a library) sitting in memory starting at physical address 0.
        * One running program (process) sat in physical memory starting at physical address 64k.
        * The user didn't expect much from the OS.

 Slide 2:
   - Section 13.2 Multiprogramming and Time Sharing:
        * Multiprogramming: multiple processes ready to run at a given time, OS switches between them (e.g., when one does I/O).
        * Increased CPU utilization.
        * Machines were expensive (hundreds of thousands or millions of dollars).
        * Time sharing: era of time sharing (cited: [S59, L60, M62, M83]).
        * Limitations of batch computing: programmers tired of long program-debug cycles.
        * Interactivity became important: many users concurrently using a machine, each waiting for timely response.
        * Crude time sharing approach: run one process for a short while, give it full access to all memory (Figure 13.1), then stop, save state to disk (including entire memory), load another, run, etc.
        * Problem: saving and restoring entire memory to disk is too slow (especially as memory grows).
        * Better approach: leave processes in memory while switching (Figure 13.2, page 3).
        * Figure 13.2: Three processes (A, B, C) sharing 512KB physical memory (each has a small part).
        * OS runs one process (e.g., A) while others (B, C) wait in ready queue.
        * New demand: protection (prevent one process from reading/writing another's memory).

 Slide 3:
   - Section 13.3 The Address Space:
        * Abstraction: address space (the running program's view of memory).
        * Address space contains all memory state of the running program.
        * Components:
            - Code: instructions (static, placed at top of address space).
            - Stack: for local variables, function call chain, parameters, return values (grows upward).
            - Heap: for dynamically allocated memory (e.g., malloc in C, new in Java/C++), grows downward.
        * Figure 13.3 (page 4): tiny address space (16KB) example.
            - Code at top (starting at 0, packed into first 1K).
            - Heap starts just after code (at 1KB) and grows downward.
            - Stack starts at 16KB and grows upward.
        * Note: placement of stack and heap is a convention; could be arranged differently (e.g., multiple threads).
        * The program isn't in memory at physical addresses 0-16KB; it's loaded at arbitrary physical address(es).
        * Example: processes A, B, C in Figure 13.2 are loaded at different physical addresses.

   - The Crux: How to virtualize memory? (Key question)
        * Problem: How can the OS build a private, potentially large address space for multiple running processes (all sharing memory) on top of a single physical memory?
        * Virtualization: the program thinks it's loaded at a particular address (e.g., 0) and has a large address space (e.g., 32-bit or 64-bit), but reality is different.
        * Example: Process A in Figure 13.2 tries to load at virtual address 0 -> must be mapped to physical address 320KB.

 Slide 4:
   - Section 13.4 Goals of Virtual Memory (VM):
        * Goal 1: Transparency - OS implements VM invisibly to the program. Program behaves as if it has private physical memory.
        * Goal 2: Efficiency - VM should be efficient in time (not slow down programs) and space (not use too much memory for support structures). Hardware support (e.g., TLBs) is critical.
        * Goal 3: Protection - Protect processes from each other and the OS from processes. When a process does load/store, it shouldn't access other process's memory or OS memory.

   - Tip: The Principle of Isolation
        * Isolation: key principle for reliable systems. If two entities are isolated, one can fail without affecting the other.
        * OS isolates processes to prevent harm from faulty/malicious processes.
        * Microkernels (e.g., [BH70, R+89, S+03]) take isolation further by walling off pieces of the OS.

 Slide 5:
   - Section 13.5 Summary:
        * VM system provides illusion of large, sparse, private address space to each program.
        * Each virtual address space contains program's instructions and data (referenced via virtual addresses).
        * OS (with hardware help) turns virtual memory references into physical addresses.
        * OS provides service for many processes, protecting programs from each other and the OS.
        * Requires mechanism (low-level machinery) and policies (e.g., managing free space, which pages to evict).

 Slide 6:
   - Aside: Every Address You See Is Virtual
        * When you print a pointer in C, it's a virtual address.
        * Example program (va.c) that prints:
            - location of main() (code)
            - location of heap (malloc)
            - location of stack (integer variable)
        * Output example (on 64-bit Mac):
            location of code: 0x1095afe50
            location of heap: 0x1096008c0
            location of stack: 0x7fff691aea64
        * Explanation: code comes first, then heap, then stack at the other end.

 Slide 7:
   - References (cited in the text):
        [BH70]: Brinch Hansen, "The Nucleus of a Multiprogramming System", 1970.
        [CV65]: Corbato and Vyssotsky, "Introduction and Overview of the Multics System", 1965.
        [DV66]: Dennis and Van Horn, "Programming Semantics for Multiprogrammed Computations", 1966.
        [L60]: Licklider, "Man-Computer Symbiosis", 1960.
        [M62]: McCarthy, "Time-Sharing Computer Systems", 1962.
        [M+63]: McCarthy et al., "A Time-Sharing Debugging System for a Small Computer", 1963.
        [M83]: McCarthy, "Reminiscences on the History of Time Sharing", 1983.
        [NS07]: Nethercote and Seward, "Valgrind: A Framework for Heavyweight Dynamic Binary Instrumentation", 2007.
        [R+89]: Rashid et al., "Mach: A System Software kernel", 1989.
        [S59]: Strachey, "Time Sharing in Large Fast Computers", 1959.
        [S+03]: Swift et al., "Improving the Reliability of Commodity Operating Systems", 2003.

 Slide 8:
   - Homework (Code):
        Questions:
        1. Learn about `free` tool: `man free`.
        2. Run `free` (with `-m` for megabytes) and note memory totals and free memory.
        3. Create a program `memory-user.c` that takes a command-line argument (MB to use), allocates an array, and touches each entry indefinitely (or for a time).
        4. While running `memory-user`, run `free` in another terminal. Observe changes in memory usage when the program runs and when killed. Try different memory amounts.
        5. Learn about `pmap`: read `man pmap`.
        6. Use `ps auxw` to find a process (e.g., browser) and then run `pmap` on it (with flags like `-X`).
        7. Observe what `pmap` shows: how many entities make up a modern address space (vs. simple code/stack/heap).
        8. Run `pmap` on `memory-user` with different memory amounts and see if output matches expectations.

 Now, we structure the notes accordingly.

 Important: We are to write exhaustive notes, so we'll include every detail from the slides.

 We'll structure the notes as:

 ## Learning Objectives (if any) - but note: the slides don't have explicit learning objectives, so we skip or note that they are not provided.

 Then, we break down by the sections in the slides.

 Since the slides are numbered, we'll follow the slide order but group by section.

 We'll create headers for each section (13.1, 13.2, etc.) and include all the details.

 Let's write the notes.
</think>

# Week 2: The Abstraction: Address Spaces

### Chapter Overview
This chapter introduces the fundamental concept of **address spaces** as a core abstraction in operating systems. It traces the historical evolution of memory management from early systems to modern virtual memory systems, emphasizing the goals of transparency, efficiency, and protection. The chapter provides exhaustive details on the structure of address spaces, the challenges of memory virtualization, and the principles underlying modern virtual memory systems.

---

### 13.1 Early Systems

#### Memory Abstraction in Early Machines
- **Minimal abstraction**: Early machines provided little memory abstraction to users.
- **Physical memory layout** (Figure 13.1):
  - Operating System (OS) resided in memory starting at physical address **0**.
  - A single running program (process) occupied memory starting at physical address **64KB**.
  - The OS was essentially a library of routines (not a full-fledged system).
- **User expectations**: Users had low expectations ("life was easy for OS developers").
- **Humorous note**:  
  > *"Because users didnâ€™t expect much. It is those darned users with their expectations of 'ease of use', 'high performance', 'reliability', etc., that really have led to all these headaches. Next time you meet one of those computer users, thank them for all the problems they have caused."*

---

### 13.2 Multiprogramming and Time Sharing

#### Evolution from Batch Processing
- **Multiprogramming** ([DV66]):
  - Multiple processes ready to run simultaneously.
  - OS switched between processes (e.g., when one performed I/O).
  - **Goal**: Increase CPU utilization (critical when machines cost hundreds of thousands/millions of dollars).
- **Time Sharing** ([S59, L60, M62, M83]):
  - Emerged due to limitations of batch computing.
  - **Key motivation**: Programmers tired of long program-debug cycles in batch environments ([CV65]).
  - **Quote from CV65**:  
    > *"The impetus for time-sharing first arose from professional programmers because of their constant frustration in debugging programs at batch processing installations. Thus, the original goal was to time-share computers to allow simultaneous access by several persons while giving to each of them the illusion of having the whole machine at his disposal."*
  - **Interactivity**: Users demanded timely responses from concurrently executing tasks.

#### Crude Time-Sharing Implementation (Problematic)
- **Approach**: 
  1. Run one process for a short time (full memory access).
  2. Stop process, save entire state to disk (including all physical memory).
  3. Load another processâ€™s state, run it.
- **Critical flaw**: 
  > *"Saving and restoring register-level state (the PC, general-purpose registers, etc.) is relatively fast, but saving the entire contents of memory to disk is brutally non-performant."*
  - **Consequence**: Slow performance as memory size increased.

#### Efficient Time-Sharing Solution (Figure 13.2)
- **Approach**: 
  - Leave processes in physical memory during context switches.
  - OS manages concurrent processes without disk I/O for memory state.
- **Memory layout** (Figure 13.2, page 3):
  - Physical memory: **512KB** total.
  - Three processes (A, B, C) each allocated a segment of memory:
    - Process A: 64KB (physical addresses 0â€“64KB)
    - Process B: 64KB (physical addresses 64KBâ€“128KB)
    - Process C: 64KB (physical addresses 128KBâ€“192KB)
    - OS: 128KB (physical addresses 384KBâ€“512KB)
  - **OS behavior**: 
    - Runs one process (e.g., A) on CPU.
    - Other processes (B, C) wait in ready queue.

#### New Challenge: Protection
- **Requirement**: Prevent processes from accessing each otherâ€™s memory.
  > *"Allowing multiple programs to reside concurrently in memory makes protection an important issue; you donâ€™t want a process to be able to read, or worse, write some other processâ€™s memory."*

---

### 13.3 The Address Space

#### Definition and Purpose
- **Address space**: The running programâ€™s view of memory (abstraction provided by OS).
- **Contains all memory state** of the running program:
  - Code (instructions)
  - Stack (local variables, function calls, parameters, return values)
  - Heap (dynamically allocated memory via `malloc()`/`new`)
  - Statically-initialized variables (e.g., global variables)

#### Address Space Structure (Figure 13.3, page 4)
- **Tiny example**: 16KB address space (simplified for clarity).
- **Layout**:
  | Segment      | Location in Address Space | Growth Direction | Purpose                                                                 |
  |--------------|---------------------------|------------------|-------------------------------------------------------------------------|
  | **Code**     | Top (0â€“1KB)               | Static (no growth) | Instructions (e.g., `main()`, functions)                                |
  | **Heap**     | After code (1KBâ€“2KB)      | Downward         | Dynamically allocated memory (e.g., `malloc(100e6)` in C)               |
  | **Stack**    | Bottom (15KBâ€“16KB)        | Upward           | Local variables, function call chain, parameters, return values          |

- **Growth convention**:
  - Heap grows **downward** (toward lower addresses) when `malloc()` is called.
  - Stack grows **upward** (toward higher addresses) during function calls.
  - **Reason**: Allows both to grow without overlapping (opposite directions).
- **Note**: This layout is a convention; alternative arrangements exist (e.g., for multi-threaded programs).

#### Virtual vs. Physical Memory
- **Programâ€™s illusion**: 
  > *"The program really isnâ€™t in memory at physical addresses 0 through 16KB; rather it is loaded at some arbitrary physical address(es)."*
- **Example** (Figure 13.2):
  - Process A: Loaded at physical address **320KB**.
  - Process B: Loaded at physical address **192KB**.
  - Process C: Loaded at physical address **64KB**.

---

### The Crux: How to Virtualize Memory

#### Core Problem Statement
> *"How can the OS build this abstraction of a private, potentially large address space for multiple running processes (all sharing memory) on top of a single, physical memory?"*

#### Virtualization Mechanism
- **Virtual address**: Address used by the program (e.g., `0` in Process A).
- **Physical address**: Actual location in hardware memory (e.g., `320KB` for Process A).
- **OS/hardware role**: 
  > *"When process A tries to perform a load at address 0 (virtual address), the OS, in tandem with hardware support, must ensure the load doesnâ€™t go to physical address 0 but rather to physical address 320KB."*
- **Key insight**: Virtual memory is the foundation of all modern computer systems.

---

### 13.4 Goals of Virtual Memory (VM)

#### Goal 1: Transparency
- **Definition**: VM implementation must be invisible to the running program.
- **Requirement**: 
  > *"The program shouldnâ€™t be aware that memory is virtualized; rather, the program behaves as if it has its own private physical memory."*
- **OS role**: Multiplex memory among multiple processes without user awareness.

#### Goal 2: Efficiency
- **Two dimensions**:
  1. **Time efficiency**: Programs should not run significantly slower due to VM.
  2. **Space efficiency**: Minimal memory overhead for VM structures.
- **Hardware dependency**: 
  > *"The OS will have to rely on hardware support, including hardware features such as TLBs (which we will learn about in due course)."*

#### Goal 3: Protection
- **Definition**: Prevent processes from accessing each otherâ€™s memory or the OS.
- **Requirement**: 
  > *"When one process performs a load, a store, or an instruction fetch, it should not be able to access or affect in any way the memory contents of any other process or the OS itself."*
- **Consequence**: Enables **isolation** (see Principle of Isolation below).

---

### The Principle of Isolation

#### Definition
> *"If two entities are properly isolated from one another, this implies that one can fail without affecting the other."*

#### OS Implementation
- **Process isolation**: 
  > *"Operating systems strive to isolate processes from each other and in this way prevent one from harming the other."*
- **Memory isolation**: 
  > *"By using memory isolation, the OS further ensures that running programs cannot affect the operation of the underlying OS."*
- **Advanced isolation**: Microkernels ([BH70, R+89, S+03) wall off OS components:
  > *"Some modern OSâ€™s take isolation even further, by walling off pieces of the OS from other pieces of the OS. Such microkernels thus may provide greater reliability than typical monolithic kernel designs."*

---

### 13.5 Summary

#### VM System Responsibilities
- **Primary function**: Provide illusion of a **large, sparse, private address space** to each program.
- **Address space contents**: 
  > *"Each virtual address space contains all of a programâ€™s instructions and data, which can be referenced by the program via virtual addresses."*
- **OS/hardware interaction**: 
  > *"The OS, with some serious hardware help, will take each of these virtual memory references and turn them into physical addresses, which can be presented to the physical memory."*
- **Multi-process handling**: 
  > *"The OS will provide this service for many processes at once, making sure to protect programs from one another, as well as protect the OS."*

#### Core Requirements
- **Mechanism**: Low-level machinery (e.g., page tables, TLBs).
- **Policies**: Critical decisions (e.g., managing free space, evicting pages when memory is full).
- **Learning path**: 
  > *"Weâ€™ll start from the bottom up, describing the critical mechanisms first."*

---

### Aside: Every Address You See Is Virtual

#### Key Insight
> *"Any address you can see as a programmer of a user-level program is a virtual address."*

#### Example Program (`va.c`)
```c
#include <stdio.h>
#include <stdlib.h>
int main(int argc, char *argv[]) {
    printf("location of code : %p\n", main);          // Code segment
    printf("location of heap : %p\n", malloc(100e6));  // Heap segment
    int x = 3;
    printf("location of stack: %p\n", &x);             // Stack segment
    return x;
}
```

#### Sample Output (64-bit Mac)
```
location of code : 0x1095afe50
location of heap : 0x1096008c0
location of stack: 0x7fff691aea64
```

#### Interpretation
- **Code** appears first in the address space (e.g., `0x1095afe50`).
- **Heap** follows code (e.g., `0x1096008c0`).
- **Stack** is at the opposite end of the virtual space (e.g., `0x7fff691aea64`).
- **Critical realization**: 
  > *"All of these addresses are virtual, and will be translated by the OS and hardware in order to fetch values from their true physical locations."*

---

### References

| Citation | Source | Key Contribution |
|----------|--------|------------------|
| [BH70] | Brinch Hansen, *The Nucleus of a Multiprogramming System*, 1970 | First paper proposing minimal kernel for customizable OS |
| [CV65] | Corbato & Vyssotsky, *Introduction to Multics*, 1965 | Early time-sharing motivation: "programmers tired of long program-debug cycles" |
| [DV66] | Dennis & Van Horn, *Programming Semantics for Multiprogrammed Computations*, 1966 | Early paper on multiprogramming |
| [L60] | Licklider, *Man-Computer Symbiosis*, 1960 | Vision of human-computer symbiosis (ahead of its time) |
| [M62] | McCarthy, *Time-Sharing Computer Systems*, 1962 | Earliest recorded paper on time-sharing (McCarthy claims idea since 1957) |
| [M+63] | McCarthy et al., *A Time-Sharing Debugging System*, 1963 | Early system swapping memory to "drum" (disk) |
| [M83] | McCarthy, *Reminiscences on Time Sharing History*, 1983 | Historical note on time-sharing origins |
| [NS07] | Nethercote & Seward, *Valgrind*, 2007 | Dynamic binary instrumentation for memory debugging |
| [R+89] | Rashid et al., *Mach: A System Software Kernel*, 1989 | Influential microkernel project (basis for Mac OS X) |
| [S59] | Strachey, *Time Sharing in Large Fast Computers*, 1959 | One of earliest time-sharing references |
| [S+03] | Swift et al., *Improving OS Reliability*, 2003 | First paper showing microkernel-like thinking improves reliability |

---

### Homework: Virtual Memory Tools (Linux)

#### Task 1: `free` Tool
1. **Command**: `man free` (read entire manual page).
2. **Execution**: 
   - Run `free` (e.g., `free -m` to display in megabytes).
   - **Questions**:
     - How much total memory is in your system?
     - How much is free?
     - Do these numbers match your intuition?

#### Task 2: Memory-User Program (`memory-user.c`)
```c
// memory-user.c
#include <stdio.h>
#include <stdlib.h>
#include <unistd.h>

int main(int argc, char *argv[]) {
    if (argc < 2) {
        fprintf(stderr, "Usage: %s <MB>\n", argv[0]);
        return 1;
    }
    size_t mb = atoi(argv[1]);
    size_t size = mb * 1024 * 1024;
    char *buf = malloc(size);
    if (!buf) {
        perror("malloc");
        return 1;
    }
    // Touch every byte to prevent optimization
    for (size_t i = 0; i < size; i++) {
        buf[i] = 'a';
    }
    // Run indefinitely (or add time limit)
    while (1) {
        sleep(1);
    }
    return 0;
}
```

#### Task 3: Memory Usage Analysis
1. **Run program**: `./memory-user 50` (uses 50MB).
2. **In another terminal**: 
   - Run `free -m` while program is active.
   - **Observe**: 
     - How do memory totals change when the program runs?
     - How do they change when you kill the program (`killall memory-user`)?
     - Do numbers match expectations?
3. **Test different memory amounts** (e.g., 100MB, 500MB).
4. **Extreme test**: Use very large amounts (e.g., 1GB) and observe system behavior.

#### Task 4: `pmap` Tool
1. **Command**: `man pmap` (read entire manual page).
2. **Process identification**:
   - Run `ps auxw` to list processes.
   - Pick a process (e.g., browser, or your `memory-user` program).
3. **Run `pmap`**:
   - `pmap -X <PID>` (shows detailed memory mappings).
   - **Questions**:
     - How many different entities make up a modern address space (vs. simple code/stack/heap)?
     - What do you see in the output?
4. **Test with `memory-user`**:
   - Run `./memory-user 10` (10MB).
   - Get PID via `ps auxw` or add `printf("PID: %d\n", getpid());` to program.
   - Run `pmap -X <PID>`.
   - **Observe**: 
     - Does output match expectations?
     - How does output change with different memory allocations?

---

### Key Terms and Definitions

| Term | Definition |
|------|------------|
| **Address Space** | The running programâ€™s view of memory (abstraction provided by OS). Contains all memory state: code, stack, heap. |
| **Virtual Address** | Address used by a program (e.g., `0` in Process A). Translated to physical address by OS/hardware. |
| **Physical Address** | Actual location in hardware memory (e.g., `320KB` for Process A). |
| **Virtualization** | OS technique making programs believe they have private, large memory space, while reality is shared physical memory. |
| **Transparency** | VM implementation invisible to programs (program behaves as if it has private physical memory). |
| **Protection** | Mechanism preventing processes from accessing each otherâ€™s memory or the OS. |
| **Isolation** | Principle where one entity failing doesnâ€™t affect others (achieved via memory isolation). |
| **Microkernel** | OS design walling off OS components (e.g., Mach kernel), improving reliability through isolation. |

---

### Review Questions

1. **Why did early systems have minimal memory abstraction?**
   - Users had low expectations; OS was a library at physical address 0, with one process at 64KB.

2. **What was the flaw in the crude time-sharing approach?**
   - Saving/restoring entire memory to disk was too slow (especially as memory grew).

3. **How does the address space layout prevent heap/stack collision?**
   - Heap grows downward from code, stack grows upward from bottom â†’ opposite directions.

4. **Why is transparency a key goal of VM?**
   - Programs must behave as if they have private physical memory without awareness of virtualization.

5. **What does `pmap` reveal about modern address spaces?**
   - It shows multiple memory segments beyond code/stack/heap (e.g., shared libraries, memory-mapped files).

6. **Why is protection critical for time-sharing systems?**
   - To prevent processes from reading/writing each otherâ€™s memory (enabling isolation).

7. **What is the "crux" of memory virtualization?**
   - Building a private address space for multiple processes on shared physical memory.

8. **How does the `va.c` example prove all addresses are virtual?**
   - Output shows code, heap, and stack at non-physical locations (e.g., `0x1095afe50`), requiring translation to physical addresses.


---

# Week 2: Textbook VM Mechanism - Comprehensive Study Notes

<think>
I need to create comprehensive study notes from the textbook chapter "Mechanism: Address Translation" based on the slides provided. The instructions specify that I must include ALL information without summarizing, preserving every detail, example, and explanation.

Let me go through each slide carefully and extract all the information in a structured way.

Starting with Slide 1:

This introduces the concept of virtualizing memory using a mechanism similar to limited direct execution (LDE) used for CPU virtualization. The key points are:
- LDE: Let programs run directly on hardware, but interpose at critical points (system calls, timer interrupts) to maintain OS control.
- Goals: Efficiency (minimize OS involvement) and Control (ensure processes only access their own memory).
- The "crux" of the problem: How to efficiently and flexibly virtualize memory while maintaining control over memory access.

Slide 2:
- Introduces "hardware-based address translation" as the mechanism for virtualizing memory.
- Hardware transforms each memory access (instruction fetch, load, store) from virtual to physical address.
- OS must manage memory (track free/used locations) and set up hardware for correct translations.
- The goal is to create an illusion that each program has its own private memory.
- Assumptions for initial approach:
  * User's address space must be placed contiguously in physical memory
  * Address space size < physical memory size
  * All address spaces are the same size

Slide 3:
- Provides an example code sequence (in C and x86 assembly) that loads a value from memory, increments it by 3, and stores it back.
- The C code: 
  void func() {
    int x = 3000; // thanks, Perry.
    x = x + 3; // line of code we are interested in
  }
- The assembly:
  128: movl 0x0(%ebx), %eax ;load 0+ebx into eax
  132: addl $0x03, %eax ;add 3 to eax register
  135: movl %eax, 0x0(%ebx) ;store eax back to mem

- Explains the memory accesses that occur when this code runs:
  * Fetch instruction at address 128
  * Execute (load from address 15 KB)
  * Fetch instruction at address 132
  * Execute (no memory reference)
  * Fetch instruction at address 135
  * Execute (store to address 15 KB)

- Mentions Figure 15.1 (which shows the process's address space layout)

Slide 4:
- Shows Figure 15.1: A Process And Its Address Space
- This figure shows the code section (at address 128) and data section (variable x at address 15 KB)
- The initial value of x is 3000

Slide 5:
- Shows Figure 15.2: Physical Memory with a Single Relocated Process
- This figure shows physical memory layout with the OS using the first slot, and the process relocated to start at physical address 32 KB.
- The free slots are 16 KB-32 KB and 48 KB-64 KB.

- Introduces the concept of "base and bounds" (also called "dynamic relocation") as the first incarnation of hardware-based address translation.

Slide 6:
- Explains base and bounds registers:
  * Base register: Holds the starting physical address of the process's address space.
  * Bounds register (limit register): Holds the size of the address space.
  * Address translation: physical address = virtual address + base
  * Hardware checks if virtual address is within bounds (0 â‰¤ virtual address < bounds)

- Example tracing of instruction execution:
  * For instruction at virtual address 128: physical address = 128 + 32768 = 32896
  * For load from virtual address 15 KB: physical address = 15360 + 32768 = 48128

- Notes that the technique is called "dynamic relocation" because it happens at runtime and allows moving address spaces after a process has started.

Slide 7:
- Explains the bounds register in more detail:
  * It can be defined as either:
    1. The size of the address space (check virtual address against this before adding base)
    2. The physical address of the end of the address space (add base first, then check if within bounds)
  * Both methods are logically equivalent.

- Provides example translations:
  * Virtual Address 0 â†’ Physical Address 16 KB
  * Virtual Address 1 KB â†’ Physical Address 17 KB
  * Virtual Address 3000 â†’ Physical Address 19384
  * Virtual Address 4400 â†’ Fault (out of bounds)

- Mentions that the MMU (Memory Management Unit) is the part of the processor that helps with address translation.

Slide 8:
- Summarizes hardware requirements for dynamic relocation (Figure 15.3):
  * Privileged mode: Needed to prevent user-mode processes from executing privileged operations
  * Base/bounds registers: Need pair per CPU to support address translation and bounds checks
  * Ability to translate virtual addresses and check if within bounds
  * Privileged instructions to update base/bounds
  * Ability to raise exceptions for illegal memory access or privileged instruction execution

- Notes that user processes cannot change base/bounds registers (would cause havoc).

- Mentions the free list data structure for tracking free memory.

Slide 9:
- Summarizes OS requirements for dynamic relocation (Figure 15.4):
  * Memory management: Allocate memory for new processes, reclaim from terminated processes
  * Base/bounds management: Set base/bounds properly upon context switch
  * Exception handling: Code to run when exceptions arise (likely terminate offending process)

Slide 10:
- Explains OS responsibilities in detail:
  * When a process is created: Find space for address space in memory (using free list)
  * When a process is terminated: Reclaim memory and put it back on free list
  * When context switch occurs: Save/restore base/bounds registers
  * Exception handling: Install handlers at boot time for various exceptions

- Mentions that the OS can move a process's address space by:
  1. Descheduling the process
  2. Copying the address space to new location
  3. Updating saved base register in process structure

Slide 11:
- Shows Figures 15.5 and 15.6 which illustrate hardware/OS interaction:
  * Figure 15.5: What OS does at boot time
  * Figure 15.6: What happens when a process runs (with timeline of events)

- Notes that the approach follows "limited direct execution": OS sets up hardware, lets process run directly, only intervenes when process misbehaves.

Slide 12:
- Summarizes the key points about address translation:
  * OS controls each memory access to ensure it stays within bounds
  * Hardware performs translation quickly (virtual to physical)
  * Process is oblivious to translation (wonderful illusion)

- Notes that base-and-bounds is efficient (only needs a base register) and offers protection (ensures no process accesses outside its address space).

- Points out inefficiency: Internal fragmentation (wasted memory between allocated segments).

- Mentions that segmentation will be discussed next as a more sophisticated approach.

Slide 13:
- References to papers and historical context:
  * [M65]: "On Dynamic Program Relocation" by W.C. McGee
  * [P90]: "Relocating loader for MS-DOS .EXE executable files"
  * [SS74]: "The Protection of Information in Computer Systems" by J. Saltzer and M. Schroeder
  * [S04]: History of system call support
  * [WL+93]: "Efficient Software-based Fault Isolation"
  * [W17]: Answer to footnote about "wreak havoc"

Slide 14:
- Homework simulation details:
  * Program: relocation.py
  * Questions about address translations with base and bounds

Now I'll create comprehensive study notes based on all this information, following the instructions to include EVERY detail without summarizing.

I'll structure the notes with proper markdown formatting, hierarchical headers, and include all examples, figures, and technical details.
</think>


### Chapter Title
**Mechanism: Address Translation**

### Learning Objectives
- Understand the concept of hardware-based address translation for virtual memory
- Learn how base-and-bounds (dynamic relocation) works
- Understand the role of hardware support (base/bounds registers) in memory virtualization
- Analyze the OS responsibilities in implementing address translation
- Recognize the limitations of base-and-bounds and the need for more sophisticated techniques

### Key Concepts and Definitions

#### Limited Direct Execution (LDE)
A general mechanism used in virtualization where:
- The program runs directly on hardware for most operations
- The OS interposes at critical points (e.g., system calls, timer interrupts)
- Goal: Balance efficiency (minimize OS involvement) with control (maintain OS oversight)

#### Hardware-Based Address Translation
The specific mechanism used to virtualize memory:
- Hardware transforms each memory access (instruction fetch, load, store) from virtual address to physical address
- On every memory reference, an address translation is performed by the hardware
- The goal is to create the illusion that each program has its own private memory

#### Base and Bounds (Dynamic Relocation)
A simple technique for implementing address translation:
- Uses two hardware registers: base register and bounds (limit) register
- Address translation: physical address = virtual address + base
- Bounds check: ensures virtual address is within 0 â‰¤ virtual address < bounds
- Referred to as "dynamic relocation" because relocation happens at runtime

#### Memory Management Unit (MMU)
The part of the processor that helps with address translation:
- Contains circuitry for performing translations and checking bounds
- Includes the base and bounds registers
- As memory management techniques become more sophisticated, more circuitry is added to the MMU

#### Internal Fragmentation
A type of waste in memory allocation:
- Occurs when allocated memory blocks contain unused space within them
- In base-and-bounds, if the address space is smaller than the allocated physical memory slot, the unused space between the code and data sections is wasted
- Example: Process using memory from 32 KB to 48 KB but only using 32 KB of that space (wasting 16 KB)

#### Privileged Mode (Kernel Mode)
A CPU mode where:
- The OS has access to the entire machine
- Can execute privileged operations (e.g., modify base/bounds registers)
- Required for OS to manage memory virtualization

#### User Mode
A CPU mode where:
- Applications are limited in what they can do
- Cannot execute privileged operations
- Memory references must be translated by hardware

#### Free List
A data structure used by the OS to track free memory:
- A list of ranges of physical memory that are not currently in use
- Simplest data structure for memory allocation tracking

#### Process Control Block (PCB)
A data structure used by the OS to track process state:
- Contains saved values of base and bounds registers when a process is not running
- Used during context switches to restore correct memory mapping

#### Exception Handling
Mechanism for dealing with illegal operations:
- CPU raises an exception when a user program tries to access memory outside its bounds
- CPU stops executing the user program and runs the OS exception handler
- Common reaction: terminate the offending process

### Detailed Mechanism Explanation

#### Address Translation Process
For every memory reference generated by a process:
1. Hardware receives a virtual address from the process
2. Hardware adds the base register value to the virtual address
3. Hardware checks if the resulting physical address is within bounds
4. If within bounds, hardware uses the physical address to access memory
5. If out of bounds, CPU raises an exception (memory access violation)

**Formula for Address Translation:**
```
physical address = virtual address + base
```

#### Bounds Register Implementation
The bounds register can be implemented in two equivalent ways:

1. **Bounds as size of address space:**
   - Hardware checks if virtual address is within 0 â‰¤ virtual address < bounds
   - Then adds base to get physical address
   - Example: Bounds = 16 KB (16384 bytes)

2. **Bounds as physical end address:**
   - Hardware first adds base to virtual address
   - Then checks if resulting physical address is within 0 â‰¤ physical address < (base + bounds)
   - Example: Base = 32 KB, Bounds = 16 KB â†’ physical address must be < 48 KB

Both methods are logically equivalent; the text assumes the first method.

#### Base and Bounds Register Usage Example
Let's trace through an example with a process loaded at physical address 32 KB (32768) with bounds set to 16 KB (16384):

| Virtual Address | Physical Address Calculation | Result |
|----------------|------------------------------|--------|
| 0              | 0 + 32768 = 32768           | 32 KB  |
| 1 KB (1024)    | 1024 + 32768 = 33792        | 33 KB  |
| 3000           | 3000 + 32768 = 35768        | 35 KB  |
| 4400           | 4400 + 32768 = 37168        | Fault (out of bounds) |

*Note: 4400 > 16384 (bounds), so it's out of bounds.*

### Hardware Requirements

#### Table: Hardware Requirements for Dynamic Relocation (Figure 15.3)

| Hardware Requirement | Notes |
|----------------------|-------|
| **Privileged mode** | Needed to prevent user-mode processes from executing privileged operations |
| **Base/bounds registers** | Need pair of registers per CPU to support address translation and bounds checks |
| **Ability to translate virtual addresses and check if within bounds** | Circuitry to do translations and check limits; in this case, quite simple |
| **Privileged instruction(s) to update base/bounds** | OS must be able to set these values before letting a user program run |
| **Privileged instruction(s) to register exception handlers** | OS must be able to tell hardware what code to run if exception occurs |
| **Ability to raise exceptions** | When processes try to access privileged instructions or out-of-bounds memory |

#### Important Notes on Hardware
- Base and bounds registers are hardware structures kept on the chip (one pair per CPU)
- User processes cannot change the base and bounds registers (would cause havoc)
- If a user process tries to modify base/bounds registers, CPU raises an exception and runs "tried to execute privileged operation while in user mode" handler
- CPU must provide a way to inform it of the location of exception handlers

### Operating System Responsibilities

#### Table: OS Responsibilities for Dynamic Relocation (Figure 15.4)

| OS Requirement | Notes |
|----------------|-------|
| **Memory management** | Need to allocate memory for new processes; reclaim memory from terminated processes; generally manage memory via free list |
| **Base/bounds management** | Must set base/bounds properly upon context switch |
| **Exception handling** | Code to run when exceptions arise; likely action is to terminate offending process |

#### Detailed OS Responsibilities

##### 1. Process Creation
- When a process is created, the OS must find space for its address space in memory
- Given assumptions (each address space is smaller than physical memory and same size), this is simple
- OS views physical memory as an array of slots
- Uses a free list to track which slots are free or in use
- When new process is created, OS searches free list for room and marks it used

*Example from Figure 15.2:*
- OS uses first slot (0-16 KB) for itself
- Process relocated into slot starting at 32 KB (32-48 KB)
- Free slots: 16-32 KB and 48-64 KB
- Free list consists of these two entries

##### 2. Process Termination
- When a process is terminated (exits gracefully or killed due to misbehavior), OS must reclaim all its memory
- OS puts the process's memory back on the free list
- Cleans up any associated data structures

##### 3. Context Switching
- Only one base and bounds register pair exists per CPU
- Values differ for each running program (each loaded at different physical address)
- During context switch:
  - When stopping a process, OS saves base/bounds values to memory (in process structure/PCB)
  - When resuming a process, OS sets base/bounds registers to correct values for that process

*Process relocation example:*
- To move a process's address space:
  1. Deschedule the process
  2. Copy the address space from current location to new location
  3. Update saved base register (in process structure) to point to new location
- When process is resumed, its (new) base register is restored

##### 4. Exception Handling
- OS must provide exception handlers for various illegal operations
- At boot time, OS installs these handlers via privileged instructions
- Common exceptions:
  - Out-of-bounds memory access
  - Attempt to execute privileged instruction in user mode
- Typical OS reaction: terminate the offending process
- OS is highly protective of the machine it's running

### Example: Memory Access Sequence

#### C Code Example
```c
void func() {
    int x = 3000; // thanks, Perry.
    x = x + 3; // line of code we are interested in
}
```

#### Assembly Code (x86)
```
128: movl 0x0(%ebx), %eax ;load 0+ebx into eax
132: addl $0x03, %eax ;add 3 to eax register
135: movl %eax, 0x0(%ebx) ;store eax back to mem
```

#### Memory Access Sequence (from process perspective)
1. Fetch instruction at address 128
2. Execute instruction (load from address 15 KB)
3. Fetch instruction at address 132
4. Execute instruction (no memory reference)
5. Fetch instruction at address 135
6. Execute instruction (store to address 15 KB)

*Note: Figure 15.1 shows the process's address space layout with code at 128 and variable x at 15 KB.*

### Timeline of Hardware/OS Interaction

#### Figure 15.5: Limited Direct Execution (Dynamic Relocation) @ Boot
```
OS @ boot Hardware (No Program Yet)
(kernel mode)
initialize trap table
remember addresses of...
system call handler
timer handler
illegal mem-access handler
illegal instruction handler
start interrupt timer
start timer; interrupt after X ms
initialize process table
initialize free list
```

#### Figure 15.6: Limited Direct Execution (Dynamic Relocation) @ Runtime
```
OS @ run Hardware Program
(kernel mode) (user mode)
To start process A:
allocate entry
in process table
alloc memory for process
set base/bound registers
return-from-trap (into A)
restore registers of A
move to user mode
jump to A's (initial) PC
Process A runs
Fetch instruction
translate virtual address
perform fetch
Execute instruction
if explicit load/store:
ensure address is legal
translate virtual address
perform load/store
(A runs...)
Timer interrupt
move to kernel mode
jump to handler
Handle timer
decide: stop A, run B
call switch() routine
save regs(A)
to proc-struct(A)
(including base/bounds)
restore regs(B)
from proc-struct(B)
(including base/bounds)
return-from-trap (into B)
restore registers of B
move to user mode
jump to B's PC
Process B runs
Execute bad load
Load is out-of-bounds;
move to kernel mode
jump to trap handler
Handle the trap
decide to kill process B
deallocate B's memory
free B's entry
in process table
```

### Key Principles and Insights

#### Interposition is Powerful
- Interposition is a generic and powerful technique used in computer systems
- In virtualizing memory, hardware interposes on each memory access, translating virtual addresses to physical addresses
- Benefits: Transparency (interposition done without changing client interface)

#### Why Base and Bounds Works
- Provides both efficiency and control:
  - Efficiency: Only requires a base register addition and bounds check
  - Control: Ensures no process can generate memory references outside its own address space

#### Protection is Critical
- Without protection, processes could overwrite memory (including OS structures)
- Example: Processes could overwrite the trap table and take over the system
- Protection is one of the most important goals of the OS

#### Limitations of Base and Bounds
- **Internal fragmentation**: Wasted memory within allocated blocks
  - Example: Process using memory from 32 KB to 48 KB but only using 32 KB of that space (wasting 16 KB)
  - This happens because the process must be placed in a fixed-size slot
- **Inflexibility**: Address space must be contiguous in physical memory
- **Fixed size**: All address spaces must be the same size

### Historical Context and References

#### Key Papers and References
- [M65] "On Dynamic Program Relocation" by W.C. McGee. IBM Systems Journal, Volume 4:3, 1965, pages 184â€“199.
  - Summary of early work on dynamic relocation and basics on static relocation.

- [P90] "Relocating loader for MS-DOS .EXE executable files" by Kenneth D. A. Pillay. Microprocessors & Microsystems archive, Volume 14:7 (September 1990).
  - Example of a relocating loader for MS-DOS.

- [SS74] "The Protection of Information in Computer Systems" by J. Saltzer and M. Schroeder. CACM, July 1974.
  - Quote: "The concepts of base-and-bound register and hardware-interpreted descriptors appeared, apparently independently, between 1957 and 1959 on three projects with diverse goals. At M.I.T., McCarthy suggested the base-and-bound idea as part of the memory protection system necessary to make time-sharing feasible. IBM independently developed the base-and-bound register as a mechanism to permit reliable multiprogramming of the Stretch (7030) computer system. At Burroughs, R. Barton suggested that hardware-interpreted descriptors would provide direct support for the naming scope rules of higher level languages in the B5000 computer system."

- [S04] "System Call Support" by Mark Smotherman. May 2004. people.cs.clemson.edu/~mark/syscall.html.
  - History of system call support.

- [WL+93] "Efficient Software-based Fault Isolation" by Robert Wahbe, Steven Lucco, Thomas E. Anderson, Susan L. Graham. SOSP '93.
  - Paper about using compiler support to bound memory references without hardware support.

- [W17] Answer to footnote: "Is there anything other than havoc that can be wreaked?" by Waciuma Wanjohi. October 2017.
  - Finding: "It's only since about 1970 that 'wreak havoc' has been more popular than 'wreak vengeance'. In the 1800s, the word wreak was almost always followed by 'his/their vengeance'."

### Homework Simulation Details

#### Program: relocation.py
Allows you to see how address translations are performed in a system with base and bounds registers.

#### Questions to Answer:
1. **Run with seeds 1, 2, and 3, and compute whether each virtual address generated by the process is in or out of bounds. If in bounds, compute the translation.**
   - Example: For seed 1, you'll get a sequence of virtual addresses; for each, determine if it's within bounds (0 â‰¤ virtual address < bounds) and if so, calculate physical address = virtual address + base.

2. **Run with these flags: -s 0 -n 10. What value do you have to set -l (the bounds register) to in order to ensure that all the generated virtual addresses are within bounds?**
   - This asks for the minimum bounds value needed to cover all virtual addresses generated by the simulation.

3. **Run with these flags: -s 1 -n 10 -l 100. What is the maximum value that base can be set to, such that the address space still fits into physical memory in its entirety?**
   - This asks for the maximum base value where (base + bounds) â‰¤ physical memory size.

4. **Run some of the same problems above, but with larger address spaces (-a) and physical memories (-p).**
   - Experiment with different sizes to understand how base and bounds interact with physical memory.

5. **What fraction of randomly-generated virtual addresses are valid, as a function of the value of the bounds register? Make a graph from running with different random seeds, with limit values ranging from 0 up to the maximum size of the address space.**
   - This asks for a statistical analysis of how bounds register value affects valid address rate.

### Comprehensive Summary

#### Address Translation Mechanism
- Hardware-based address translation transforms virtual addresses (generated by processes) into physical addresses (where data actually resides)
- This is done on every memory reference (instruction fetch, load, store)
- The hardware alone cannot virtualize memory; the OS must set up the hardware for correct translations

#### Base and Bounds Implementation
- Uses two hardware registers:
  - **Base register**: Starting physical address of process's address space
  - **Bounds register**: Size of address space (or end address)
- Address translation: physical address = virtual address + base
- Bounds check: ensures virtual address is within 0 â‰¤ virtual address < bounds
- If out of bounds, CPU raises an exception

#### Hardware Support Requirements
1. **Privileged mode**: To prevent user processes from executing privileged operations
2. **Base/bounds registers**: One pair per CPU
3. **Translation circuitry**: To perform address translation and bounds checks
4. **Privileged instructions**: To update base/bounds registers (only available in kernel mode)
5. **Exception handling**: To handle illegal memory access or privileged instruction execution

#### OS Responsibilities
1. **Memory management**: Track free memory using a free list, allocate for new processes, reclaim on termination
2. **Base/bounds management**: Save and restore base/bounds values during context switches
3. **Exception handling**: Install handlers for memory access violations and privileged instruction attempts

#### Advantages of Base and Bounds
- **Efficiency**: Only requires a simple addition and bounds check
- **Protection**: Ensures no process can access memory outside its address space
- **Transparency**: Process is oblivious to address translation (wonderful illusion)

#### Limitations of Base and Bounds
- **Internal fragmentation**: Wasted memory within allocated blocks (e.g., 16 KB wasted in Figure 15.2)
- **Contiguous allocation**: Address space must be placed contiguously in physical memory
- **Fixed size**: All address spaces must be the same size

#### Future Direction
- To overcome limitations of base and bounds, more sophisticated techniques like segmentation will be introduced
- These techniques aim to better utilize physical memory and avoid internal fragmentation

### Review Questions

1. What is the main goal of limited direct execution in virtualization?
2. How does hardware-based address translation work?
3. Explain the formula for address translation using base and bounds.
4. What are the two ways to implement the bounds register, and why are they equivalent?
5. Why can't user processes modify the base and bounds registers?
6. Describe the process of relocating a process's address space in memory.
7. What is internal fragmentation, and how does it occur in base-and-bounds?
8. What are the hardware requirements for implementing dynamic relocation?
9. What are the OS responsibilities when a process is created, terminated, or during context switching?
10. How does the CPU handle an out-of-bounds memory access?

---

## ðŸ“ Study Guide Summary

This combined study guide contains all notes for **Week 2**, automatically generated from 6 individual files.

### Individual Files Included:

1. `20250809_lecture_2.md`
2. `20250809_textbook_cpu_sched.md`
3. `20250809_textbook_cpu_sched_lottery.md`
4. `20250809_textbook_cpu_sched_mlfq.md`
5. `20250809_textbook_vm_intro.md`
6. `20250809_textbook_vm_mechanism.md`

### Usage Tips:
- Use the table of contents above to navigate quickly
- Each section is separated by horizontal rules (`---`)
- Individual files are still available for focused study
- This combined file is perfect for comprehensive review

---
*Generated by [LectureQ](https://github.com/yourusername/lectureq) - Your AI-powered study companion* ðŸ¤–
